{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97226e8",
   "metadata": {},
   "source": [
    "# Phase 3: Downscaled Comprehensive Denoising Method Evaluation\n",
    "\n",
    "## Research Objective\n",
    "Systematically evaluate 3 denoising methods across 4 dimensions to determine optimal approaches for smartphone-based sleep apnea detection under realistic noise conditions using **downscaled sampling for faster execution**.\n",
    "\n",
    "## This Notebook:\n",
    "1. **Downscaled Sampling Focus**: Evaluate **20 randomly sampled files per condition** for rapid prototyping\n",
    "2. **Multi-Method Denoising**: Apply 3 denoising techniques to representative priority conditions\n",
    "3. **Four-Dimensional Evaluation**: Performance recovery, signal quality, computational efficiency, feature preservation\n",
    "4. **Smartphone Suitability Scoring**: Weighted composite metrics for deployment decisions\n",
    "5. **Method Ranking**: Evidence-based recommendations for mobile health applications\n",
    "\n",
    "## Denoising Methods Under Evaluation:\n",
    "- **Spectral Subtraction**: Fast, lightweight, potential musical noise artifacts\n",
    "- **Wiener Filtering**: Balanced statistical approach with moderate complexity\n",
    "- **LogMMSE**: Advanced statistical method with better artifact control\n",
    "\n",
    "## Representative Test Conditions (5dB SNR - Worst Case):\n",
    "- **patient_01_wav_5db_vacuum_cleaner**: Mechanical high-frequency noise\n",
    "- **patient_01_wav_5db_cat**: Animal organic sounds\n",
    "- **patient_01_wav_5db_door_wood_creaks**: Structural low-frequency noise\n",
    "- **patient_01_wav_5db_crying_baby**: Human vocal interference\n",
    "- **patient_01_wav_5db_coughing**: Respiratory interference (most challenging)\n",
    "\n",
    "## Downscaled Optimization:\n",
    "- **Sample Size**: 20 randomly selected files per condition (vs 1,168 full dataset)\n",
    "- **Total Evaluations**: 5 conditions Ã— 3 methods = 15 evaluations (~30 minutes)\n",
    "- **Strategy**: Rapid prototyping and method comparison for proof-of-concept\n",
    "- **Scientific Validity**: Random sampling maintains representativeness for comparative analysis\n",
    "\n",
    "## Expected Outcomes:\n",
    "- Recovery targets: 50% (minimum), 75% (good), 90% (excellent), 100% (perfect)\n",
    "- Computational trade-offs: Traditional signal processing methods comparison\n",
    "- Feature preservation analysis: Which methods maintain breathing biomarkers\n",
    "- Smartphone deployment recommendations: Optimal method per use case\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\n",
      "âœ… Configuration loaded:\n",
      "   ğŸ“ Base data directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\n",
      "   ğŸ¤– Model path: ../models/sleep_apnea_model.pkl\n",
      "   ğŸ”Š Denoising methods: 3 traditional methods\n",
      "   ğŸ“Š Representative conditions: 5\n",
      "   ğŸ“Š Output directories created\n",
      "   âš¡ DOWNSCALED OPTIMIZATION: 20 files per condition\n",
      "   ğŸ¯ Total evaluations: 5 Ã— 3 = 15\n",
      "   ğŸš€ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\n",
      "   ğŸ² Random seed set: 42 (reproducible sampling)\n",
      "âœ… Configuration loaded:\n",
      "   ğŸ“ Base data directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\n",
      "   ğŸ¤– Model path: ../models/sleep_apnea_model.pkl\n",
      "   ğŸ”Š Denoising methods: 3 traditional methods\n",
      "   ğŸ“Š Representative conditions: 5\n",
      "   ğŸ“Š Output directories created\n",
      "   âš¡ DOWNSCALED OPTIMIZATION: 20 files per condition\n",
      "   ğŸ¯ Total evaluations: 5 Ã— 3 = 15\n",
      "   ğŸš€ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\n",
      "   ğŸ² Random seed set: 42 (reproducible sampling)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "print(\"=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_score, recall_score, accuracy_score\n",
    ")\n",
    "import joblib\n",
    "import random\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration paths\n",
    "BASE_DATA_DIR = \"F:/Solo All In One Docs/Scidb Sleep Data/processed\"\n",
    "MODEL_PATH = \"../models/sleep_apnea_model.pkl\"\n",
    "PHASE2_RESULTS_PATH = os.path.join(BASE_DATA_DIR, \"noise_evaluation_results.csv\")\n",
    "PHASE3_CONFIG_PATH = os.path.join(BASE_DATA_DIR, \"phase3_preparation_config.json\")\n",
    "CLEAN_BASELINE_PATH = os.path.join(BASE_DATA_DIR, \"clean_audio_baseline_results.json\")\n",
    "\n",
    "# Denoising methods configuration (3 traditional methods for speed)\n",
    "DENOISING_METHODS = {\n",
    "    'spectral_subtraction': {\n",
    "        'script': '../src/spec_subtraction_same_file.py',\n",
    "        'name': 'Spectral Subtraction',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'moderate'\n",
    "    },\n",
    "    'wiener_filtering': {\n",
    "        'script': '../src/wiener_filtering.py',\n",
    "        'name': 'Wiener Filtering',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'logmmse': {\n",
    "        'script': '../src/log_mmse.py',\n",
    "        'name': 'LogMMSE',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'moderate',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'deepfilternet': {\n",
    "    'script': '../src/denoise_with_deepfilternet.py',\n",
    "    'name': 'DeepFilterNet',\n",
    "    'category': 'deep_learning',\n",
    "    'expected_efficiency': 'low',\n",
    "    'expected_quality': 'excellent'\n",
    "}\n",
    "}\n",
    "\n",
    "# Representative conditions from Phase 2 (5dB worst-case analysis)\n",
    "REPRESENTATIVE_CONDITIONS = [\n",
    "    'patient_01_wav_5db_vacuum_cleaner',    # Mechanical high-frequency noise\n",
    "    'patient_01_wav_5db_cat',               # Animal organic sounds  \n",
    "    'patient_01_wav_5db_door_wood_creaks',  # Structural low-frequency noise\n",
    "    'patient_01_wav_5db_crying_baby',       # Human vocal interference\n",
    "    'patient_01_wav_5db_coughing'           # Respiratory interference\n",
    "]\n",
    "\n",
    "# Audio processing settings (consistent with Phase 1 & 2)\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_DURATION = 30.0\n",
    "\n",
    "# DOWNSCALED SAMPLING CONFIGURATION\n",
    "SAMPLE_SIZE_PER_CONDITION = 20  # Fixed 20 files per condition\n",
    "RANDOM_SEED = 42  # For reproducible sampling\n",
    "\n",
    "# Create output directories\n",
    "DENOISED_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"denoised_audio_downscaled\")\n",
    "RESULTS_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"phase3_downscaled_results\")\n",
    "os.makedirs(DENOISED_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Configuration loaded:\")\n",
    "print(f\"   ğŸ“ Base data directory: {BASE_DATA_DIR}\")\n",
    "print(f\"   ğŸ¤– Model path: {MODEL_PATH}\")\n",
    "print(f\"   ğŸ”Š Denoising methods: {len(DENOISING_METHODS)} traditional methods\")\n",
    "print(f\"   ğŸ“Š Representative conditions: {len(REPRESENTATIVE_CONDITIONS)}\")\n",
    "print(f\"   ğŸ“Š Output directories created\")\n",
    "print(f\"   âš¡ DOWNSCALED OPTIMIZATION: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   ğŸ¯ Total evaluations: {len(REPRESENTATIVE_CONDITIONS)} Ã— {len(DENOISING_METHODS)} = {len(REPRESENTATIVE_CONDITIONS) * len(DENOISING_METHODS)}\")\n",
    "print(f\"   ğŸš€ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\")\n",
    "\n",
    "# Set random seed for reproducible sampling\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ğŸ² Random seed set: {RANDOM_SEED} (reproducible sampling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd1c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature extraction function loaded (27 features matching training pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction function (same 27 features as training pipeline)\n",
    "def extract_comprehensive_features(audio_frame, sample_rate):\n",
    "    \"\"\"Extract the same 27 features used in training pipeline\"\"\"\n",
    "    try:\n",
    "        if len(audio_frame) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Basic acoustic features\n",
    "        rms = float(librosa.feature.rms(y=audio_frame).mean())\n",
    "        zcr = float(librosa.feature.zero_crossing_rate(y=audio_frame).mean())\n",
    "        centroid = float(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n",
    "        bandwidth = float(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n",
    "        rolloff = float(librosa.feature.spectral_rolloff(y=audio_frame, sr=sample_rate).mean())\n",
    "        \n",
    "        # MFCCs (first 8 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=8)\n",
    "        mfcc_means = mfccs.mean(axis=1)\n",
    "        mfcc_stds = mfccs.std(axis=1)\n",
    "        \n",
    "        # Temporal features for breathing patterns (5-second windows)\n",
    "        window_size = int(5 * sample_rate)  # 5 seconds\n",
    "        num_windows = len(audio_frame) // window_size\n",
    "        \n",
    "        if num_windows >= 2:\n",
    "            rms_windows = []\n",
    "            zcr_windows = []\n",
    "            \n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window = audio_frame[start_idx:end_idx]\n",
    "                \n",
    "                rms_windows.append(librosa.feature.rms(y=window).mean())\n",
    "                zcr_windows.append(librosa.feature.zero_crossing_rate(y=window).mean())\n",
    "            \n",
    "            rms_variability = float(np.std(rms_windows))\n",
    "            zcr_variability = float(np.std(zcr_windows))\n",
    "            breathing_regularity = float(1.0 / (1.0 + rms_variability))  # Higher = more regular\n",
    "        else:\n",
    "            rms_variability = 0.0\n",
    "            zcr_variability = 0.0\n",
    "            breathing_regularity = 0.5\n",
    "        \n",
    "        # Silence detection\n",
    "        silence_threshold = np.percentile(np.abs(audio_frame), 20)  # Bottom 20% as silence\n",
    "        silence_mask = np.abs(audio_frame) < silence_threshold\n",
    "        silence_ratio = float(np.mean(silence_mask))\n",
    "        \n",
    "        # Breathing pause detection (continuous silence periods)\n",
    "        silence_changes = np.diff(silence_mask.astype(int))\n",
    "        pause_starts = np.where(silence_changes == 1)[0]\n",
    "        pause_ends = np.where(silence_changes == -1)[0]\n",
    "        \n",
    "        if len(pause_starts) > 0 and len(pause_ends) > 0:\n",
    "            if len(pause_ends) < len(pause_starts):\n",
    "                pause_ends = np.append(pause_ends, len(audio_frame))\n",
    "            pause_durations = (pause_ends[:len(pause_starts)] - pause_starts) / sample_rate\n",
    "            avg_pause_duration = float(np.mean(pause_durations))\n",
    "            max_pause_duration = float(np.max(pause_durations))\n",
    "        else:\n",
    "            avg_pause_duration = 0.0\n",
    "            max_pause_duration = 0.0\n",
    "        \n",
    "        # Combine all features (same structure as training)\n",
    "        features = {\n",
    "            'clean_rms': rms,\n",
    "            'clean_zcr': zcr,\n",
    "            'clean_centroid': centroid,\n",
    "            'clean_bandwidth': bandwidth,\n",
    "            'clean_rolloff': rolloff,\n",
    "            'clean_rms_variability': rms_variability,\n",
    "            'clean_zcr_variability': zcr_variability,\n",
    "            'clean_breathing_regularity': breathing_regularity,\n",
    "            'clean_silence_ratio': silence_ratio,\n",
    "            'clean_avg_pause_duration': avg_pause_duration,\n",
    "            'clean_max_pause_duration': max_pause_duration\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs\n",
    "        for i, (mean_val, std_val) in enumerate(zip(mfcc_means, mfcc_stds), 1):\n",
    "            features[f'clean_mfcc_{i}_mean'] = float(mean_val)\n",
    "            features[f'clean_mfcc_{i}_std'] = float(std_val)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Feature extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Feature extraction function loaded (27 features matching training pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b6063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\n",
      "======================================================================\n",
      "âœ… Phase 2 results loaded: 5 noise conditions evaluated\n",
      "\n",
      "ğŸ“ˆ Phase 2 Performance Summary:\n",
      "   F1-Score Range: 0.000 - 0.218\n",
      "   Average F1-Score: 0.051 (Â±0.095)\n",
      "   Average Degradation: 93.3% (Â±12.5%)\n",
      "\n",
      "âœ… Clean baseline loaded: F1=0.758\n",
      "\n",
      "âœ… Downscaled sampling functions loaded\n",
      "   ğŸ“ Sample size: 20 files per condition\n",
      "   ğŸ² Random seed: 42 (reproducible results)\n",
      "   ğŸ“ Temporary directories will be created for each method-condition combination\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Phase 2 Results and Create Downscaled Sampling Function\n",
    "print(\"ğŸ“Š LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load Phase 2 evaluation results\n",
    "try:\n",
    "    phase2_results = pd.read_csv(PHASE2_RESULTS_PATH)\n",
    "    print(f\"âœ… Phase 2 results loaded: {len(phase2_results)} noise conditions evaluated\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\nğŸ“ˆ Phase 2 Performance Summary:\")\n",
    "    print(f\"   F1-Score Range: {phase2_results['f1_score'].min():.3f} - {phase2_results['f1_score'].max():.3f}\")\n",
    "    print(f\"   Average F1-Score: {phase2_results['f1_score'].mean():.3f} (Â±{phase2_results['f1_score'].std():.3f})\")\n",
    "    print(f\"   Average Degradation: {phase2_results['f1_degradation_pct'].mean():.1f}% (Â±{phase2_results['f1_degradation_pct'].std():.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not load Phase 2 results: {e}\")\n",
    "    print(f\"   Will proceed with fallback configuration\")\n",
    "    phase2_results = None\n",
    "\n",
    "# Load clean baseline for reference\n",
    "try:\n",
    "    with open(CLEAN_BASELINE_PATH, 'r') as f:\n",
    "        clean_baseline = json.load(f)\n",
    "    print(f\"\\nâœ… Clean baseline loaded: F1={clean_baseline['clean_f1_score']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load clean baseline: {e}\")\n",
    "    clean_baseline = {'clean_f1_score': 0.672}  # Fallback value from research plan\n",
    "\n",
    "# Downscaled sampling function for 20 files per condition\n",
    "def sample_files_downscaled(input_dir, sample_size=20):\n",
    "    \"\"\"\n",
    "    Randomly sample a fixed number of files from condition directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing audio files\n",
    "        sample_size: Number of files to sample (default 20)\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled filenames (sorted for reproducibility)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(input_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        if len(all_files) == 0:\n",
    "            print(f\"      âŒ No WAV files found in {input_dir}\")\n",
    "            return []\n",
    "        \n",
    "        if len(all_files) <= sample_size:\n",
    "            print(f\"      ğŸ“Š Using all {len(all_files)} files (less than sample size)\")\n",
    "            return sorted(all_files)\n",
    "        \n",
    "        # Random sampling with fixed seed for reproducibility\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "        \n",
    "        percentage_actual = (sample_size / len(all_files)) * 100\n",
    "        print(f\"      ğŸ“Š Sampled {sample_size} files ({percentage_actual:.1f}%) from {len(all_files)} total files\")\n",
    "        \n",
    "        return sorted(sampled_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Sampling failed for {input_dir}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_temp_sample_directory(source_dir, sampled_files, temp_suffix):\n",
    "    \"\"\"\n",
    "    Create temporary directory with only sampled files for denoising\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Source directory containing all files\n",
    "        sampled_files: List of filenames to copy\n",
    "        temp_suffix: Unique suffix for temp directory\n",
    "    \n",
    "    Returns:\n",
    "        Path to temporary directory\n",
    "    \"\"\"\n",
    "    temp_dir = f\"{source_dir}_temp_downscaled_{temp_suffix}\"\n",
    "    \n",
    "    # Clean up any existing temp directory\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy sampled files to temp directory\n",
    "    copied_count = 0\n",
    "    for filename in sampled_files:\n",
    "        src = os.path.join(source_dir, filename)\n",
    "        dst = os.path.join(temp_dir, filename)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied_count += 1\n",
    "    \n",
    "    print(f\"      ğŸ“ Created temp sample directory: {copied_count} files copied\")\n",
    "    return temp_dir\n",
    "\n",
    "print(f\"\\nâœ… Downscaled sampling functions loaded\")\n",
    "print(f\"   ğŸ“ Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   ğŸ² Random seed: {RANDOM_SEED} (reproducible results)\")\n",
    "print(f\"   ğŸ“ Temporary directories will be created for each method-condition combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\n",
      "ğŸ“‰ Representative Conditions (5dB worst-case per noise category):\n",
      "   patient_01_wav_5db_vacuum_cleaner: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_cat: F1=0.036 (-95.2%)\n",
      "   patient_01_wav_5db_door_wood_creaks: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_crying_baby: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_coughing: F1=0.218 (-71.2%)\n",
      "\n",
      "ğŸ“ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\n",
      "   âœ… patient_01_wav_5db_vacuum_cleaner: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_cat: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_door_wood_creaks: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_crying_baby: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_coughing: 1168 files available, will sample 20\n",
      "\n",
      "ğŸ’¾ Verified priority conditions saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_priority_conditions.csv\n",
      "\n",
      "âœ… Priority condition verification complete:\n",
      "   ğŸ“Š Verified conditions: 5\n",
      "   ğŸ¯ Sample size per condition: 20 files\n",
      "   ğŸ“ˆ Total files to process: 300\n",
      "   â±ï¸  Estimated execution time: ~30 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Select Priority Conditions and Prepare for Downscaled Evaluation\n",
    "print(\"ğŸ¯ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Select priority conditions for Phase 3 evaluation\n",
    "priority_conditions = None\n",
    "\n",
    "if phase2_results is not None:\n",
    "    # Strategy: Select representative conditions across noise types at 5dB (worst-case)\n",
    "    representative_conditions = []\n",
    "    \n",
    "    # Filter for 5dB conditions and get worst-performing per noise category\n",
    "    conditions_5db = phase2_results[phase2_results['condition_name'].str.contains('_5db_')]\n",
    "    \n",
    "    if not conditions_5db.empty:\n",
    "        for condition_name in REPRESENTATIVE_CONDITIONS:\n",
    "            condition_match = conditions_5db[conditions_5db['condition_name'] == condition_name]\n",
    "            if not condition_match.empty:\n",
    "                representative_conditions.append(condition_match.iloc[0])\n",
    "        \n",
    "        if representative_conditions:\n",
    "            priority_conditions = pd.DataFrame(representative_conditions)\n",
    "            print(f\"\\nğŸ¯ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\")\n",
    "            print(f\"ğŸ“‰ Representative Conditions (5dB worst-case per noise category):\")\n",
    "            for idx, row in priority_conditions.iterrows():\n",
    "                print(f\"   {row['condition_name']}: F1={row['f1_score']:.3f} (-{row['f1_degradation_pct']:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  No matching representative conditions found in Phase 2 results\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  No 5dB conditions found in Phase 2 results\")\n",
    "\n",
    "if priority_conditions is None:\n",
    "    print(f\"âš ï¸  Using fallback representative conditions from configuration\")\n",
    "    # Create fallback priority conditions DataFrame\n",
    "    priority_conditions = pd.DataFrame({\n",
    "        'condition_name': REPRESENTATIVE_CONDITIONS,\n",
    "        'f1_score': [0.400] * len(REPRESENTATIVE_CONDITIONS),  # Estimated based on 5dB degradation\n",
    "        'f1_degradation_pct': [47.2] * len(REPRESENTATIVE_CONDITIONS)  # Estimated degradation\n",
    "    })\n",
    "    print(f\"   Using {len(REPRESENTATIVE_CONDITIONS)} representative conditions as fallback\")\n",
    "\n",
    "# Verify condition directories exist and count available files\n",
    "print(f\"\\nğŸ“ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\")\n",
    "verified_conditions = []\n",
    "\n",
    "for _, condition_row in priority_conditions.iterrows():\n",
    "    condition_name = condition_row['condition_name']\n",
    "    condition_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "    \n",
    "    if os.path.exists(condition_dir):\n",
    "        wav_files = [f for f in os.listdir(condition_dir) if f.endswith('.wav')]\n",
    "        file_count = len(wav_files)\n",
    "        \n",
    "        if file_count > 0:\n",
    "            verified_conditions.append(condition_row)\n",
    "            sample_count = min(SAMPLE_SIZE_PER_CONDITION, file_count)\n",
    "            print(f\"   âœ… {condition_name}: {file_count} files available, will sample {sample_count}\")\n",
    "        else:\n",
    "            print(f\"   âŒ {condition_name}: No WAV files found\")\n",
    "    else:\n",
    "        print(f\"   âŒ {condition_name}: Directory not found at {condition_dir}\")\n",
    "\n",
    "if verified_conditions:\n",
    "    priority_conditions = pd.DataFrame(verified_conditions)\n",
    "    \n",
    "    # Save priority conditions for reference\n",
    "    priority_conditions_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_priority_conditions.csv\")\n",
    "    priority_conditions.to_csv(priority_conditions_path, index=False)\n",
    "    print(f\"\\nğŸ’¾ Verified priority conditions saved: {priority_conditions_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Priority condition verification complete:\")\n",
    "    print(f\"   ğŸ“Š Verified conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   ğŸ¯ Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"   ğŸ“ˆ Total files to process: {len(priority_conditions) * SAMPLE_SIZE_PER_CONDITION * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   â±ï¸  Estimated execution time: ~30 minutes\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ No verified conditions available for evaluation\")\n",
    "    print(f\"   Please check that Phase 2 noise injection has been completed\")\n",
    "    priority_conditions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c008f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\n",
      "============================================================\n",
      "âœ… Model loaded (direct): ../models/sleep_apnea_model.pkl\n",
      "ğŸ“Š Model type: RandomForestClassifier\n",
      "âœ… Audio metadata loaded: 10972 records (whitespace cleaned)\n",
      "\n",
      "âœ… Evaluation framework ready for downscaled processing\n",
      "âœ… Model loaded (direct): ../models/sleep_apnea_model.pkl\n",
      "ğŸ“Š Model type: RandomForestClassifier\n",
      "âœ… Audio metadata loaded: 10972 records (whitespace cleaned)\n",
      "\n",
      "âœ… Evaluation framework ready for downscaled processing\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model and Prepare Evaluation Framework\n",
    "print(\"ğŸ¤– LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    model_data = joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if isinstance(model_data, dict):\n",
    "        model = model_data['model']\n",
    "        feature_columns = model_data.get('feature_columns', None)\n",
    "        print(f\"âœ… Model loaded from: {MODEL_PATH}\")\n",
    "        print(f\"ğŸ“Š Model type: {type(model).__name__}\")\n",
    "        if feature_columns:\n",
    "            print(f\"ğŸ¯ Expected features: {len(feature_columns)}\")\n",
    "    else:\n",
    "        # Fallback if model is saved directly\n",
    "        model = model_data\n",
    "        feature_columns = None\n",
    "        print(f\"âœ… Model loaded (direct): {MODEL_PATH}\")\n",
    "        print(f\"ğŸ“Š Model type: {type(model).__name__}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load model: {e}\")\n",
    "    model = None\n",
    "    feature_columns = None\n",
    "\n",
    "# Performance evaluation function optimized for downscaled processing\n",
    "def evaluate_denoised_audio_downscaled(denoised_audio_dir, condition_name, method_name, model, feature_columns, audio_metadata):\n",
    "    \"\"\"Evaluate model performance on denoised audio with downscaled sampling\"\"\"\n",
    "    \n",
    "    print(f\"   ğŸ“Š Evaluating: {method_name} on {condition_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get WAV files in the denoised directory\n",
    "        if not os.path.exists(denoised_audio_dir):\n",
    "            print(f\"      âŒ Directory not found: {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        wav_files = [f for f in os.listdir(denoised_audio_dir) if f.lower().endswith('.wav')]\n",
    "        if not wav_files:\n",
    "            print(f\"      âŒ No WAV files found in {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"      ğŸµ Processing {len(wav_files)} denoised audio files...\")\n",
    "        \n",
    "        # Apply whitespace fix to metadata if available\n",
    "        if audio_metadata is not None and 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "        \n",
    "        # Extract features and get labels with progress monitoring\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        mismatch_count = 0\n",
    "        \n",
    "        for i, wav_file in enumerate(wav_files):\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                wav_path = os.path.join(denoised_audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features is None:\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get corresponding label from metadata\n",
    "                original_filename = wav_file.replace('mixed_', '').replace('denoised_', '').strip()\n",
    "                \n",
    "                if audio_metadata is not None:\n",
    "                    # Find matching metadata record\n",
    "                    metadata_match = audio_metadata[audio_metadata['wav_file'] == original_filename]\n",
    "                    if not metadata_match.empty:\n",
    "                        label = metadata_match.iloc[0]['apnea_label']\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(label)\n",
    "                        processed_count += 1\n",
    "                    else:\n",
    "                        mismatch_count += 1\n",
    "                        if mismatch_count <= 2:  # Show first 2 mismatches\n",
    "                            print(f\"      âš ï¸  No metadata match for: '{original_filename}'\")\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                if failed_count <= 2:  # Show first 2 errors\n",
    "                    print(f\"      âš ï¸  Error processing {wav_file}: {e}\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"      ğŸ“Š Processed: {processed_count}, Failed: {failed_count}, Mismatches: {mismatch_count}\")\n",
    "        \n",
    "        if processed_count == 0:\n",
    "            print(f\"      âŒ No files processed successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to DataFrame and make predictions\n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        labels = np.array(labels_list)\n",
    "        \n",
    "        # Ensure feature order matches training\n",
    "        if feature_columns:\n",
    "            features_df = features_df.reindex(columns=feature_columns, fill_value=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features_df)\n",
    "        prediction_probas = model.predict_proba(features_df)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)  # Sensitivity\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        # Confusion matrix for specificity\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'num_samples': processed_count,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall_sensitivity': recall,\n",
    "            'specificity': specificity,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn),\n",
    "            'sample_size_note': 'downscaled_20_files'\n",
    "        }\n",
    "        \n",
    "        print(f\"      âœ… {method_name}: F1={f1:.3f}, Sens={recall:.3f}, Spec={specificity:.3f} [n={processed_count}]\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Computational efficiency measurement function for downscaled processing\n",
    "def measure_denoising_efficiency_downscaled(input_dir, output_dir, method_script, method_name):\n",
    "    \"\"\"Measure computational efficiency of denoising method on downscaled sample\"\"\"\n",
    "    \n",
    "    print(f\"   â±ï¸  Measuring efficiency for {method_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get input file count\n",
    "        input_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.wav')]\n",
    "        print(f\"      ğŸ“ Input files to process: {len(input_files)}\")\n",
    "        \n",
    "        # Get system resources before\n",
    "        process = psutil.Process()\n",
    "        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Measure processing time\n",
    "        start_time = time.time()\n",
    "        print(f\"      ğŸš€ Starting denoising at {time.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # Run denoising method\n",
    "        cmd = [sys.executable, method_script, '--input', input_dir, '--output', output_dir]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # 5 minute timeout\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"      ğŸ Processing completed in {processing_time:.1f}s\")\n",
    "        \n",
    "        # Get system resources after\n",
    "        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        if os.path.exists(output_dir):\n",
    "            output_files = [f for f in os.listdir(output_dir) if f.lower().endswith('.wav')]\n",
    "            num_files_processed = len(output_files)\n",
    "            \n",
    "            # Estimate total audio duration (assuming 30-second files)\n",
    "            total_audio_duration = num_files_processed * 30.0  # seconds\n",
    "            real_time_factor = total_audio_duration / processing_time if processing_time > 0 else 0\n",
    "            \n",
    "            efficiency_metrics = {\n",
    "                'method_name': method_name,\n",
    "                'processing_time_sec': processing_time,\n",
    "                'files_processed': num_files_processed,\n",
    "                'total_audio_duration_sec': total_audio_duration,\n",
    "                'real_time_factor': real_time_factor,\n",
    "                'memory_usage_mb': memory_after - memory_before,\n",
    "                'peak_memory_mb': memory_after,\n",
    "                'processing_speed_files_per_sec': num_files_processed / processing_time if processing_time > 0 else 0,\n",
    "                'success': result.returncode == 0,\n",
    "                'sample_size_note': 'downscaled_20_files'\n",
    "            }\n",
    "            \n",
    "            print(f\"      âš¡ {method_name}: {processing_time:.1f}s, {real_time_factor:.2f}x RT, {num_files_processed} files\")\n",
    "            return efficiency_metrics\n",
    "            \n",
    "        else:\n",
    "            print(f\"      âŒ {method_name}: Output directory not created\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"      âŒ {method_name}: Processing timeout (>5 minutes)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Efficiency measurement failed for {method_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load audio metadata from Phase 1 for label matching\n",
    "try:\n",
    "    metadata_path = os.path.join(BASE_DATA_DIR, \"audio_metadata.csv\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        audio_metadata = pd.read_csv(metadata_path)\n",
    "        # Apply whitespace stripping immediately upon loading\n",
    "        if 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "            print(f\"âœ… Audio metadata loaded: {len(audio_metadata)} records (whitespace cleaned)\")\n",
    "        else:\n",
    "            print(f\"âœ… Audio metadata loaded: {len(audio_metadata)} records\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Audio metadata not found at {metadata_path}\")\n",
    "        audio_metadata = None\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load audio metadata: {e}\")\n",
    "    audio_metadata = None\n",
    "\n",
    "print(f\"\\nâœ… Evaluation framework ready for downscaled processing\")\n",
    "if model is None:\n",
    "    print(f\"âš ï¸  Model loading failed - evaluation will be limited\")\n",
    "if audio_metadata is None:\n",
    "    print(f\"âš ï¸  Audio metadata missing - label matching may fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04fb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Š APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\n",
      "Time started: 2025-07-30 21:55:07\n",
      "================================================================================\n",
      "ğŸš€ DOWNSCALED PROCESSING CONFIGURATION:\n",
      "   ğŸ¯ Conditions: 5\n",
      "   ğŸ”§ Methods: 3\n",
      "   ğŸ“Š Sample size: 20 files per condition\n",
      "   ğŸ“ˆ Total evaluations: 15\n",
      "   â±ï¸  Expected time: ~30 minutes\n",
      "\n",
      "ğŸ“ Condition 1/5: patient_01_wav_5db_vacuum_cleaner\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:07\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:07\n",
      "      ğŸ Processing completed in 7.3s\n",
      "      âš¡ Spectral Subtraction: 7.3s, 82.17x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 7.3s\n",
      "      âš¡ Spectral Subtraction: 7.3s, 82.17x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:28\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:28\n",
      "      ğŸ Processing completed in 5.6s\n",
      "      âš¡ Wiener Filtering: 5.6s, 106.82x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.6s\n",
      "      âš¡ Wiener Filtering: 5.6s, 106.82x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:43\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:55:43\n",
      "      ğŸ Processing completed in 17.1s\n",
      "      âš¡ LogMMSE: 17.1s, 35.01x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 17.1s\n",
      "      âš¡ LogMMSE: 17.1s, 35.01x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ“ Condition 2/5: patient_01_wav_5db_cat\n",
      "   ğŸ“‰ Original performance: F1=0.036 (-95.2%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ“ Condition 2/5: patient_01_wav_5db_cat\n",
      "   ğŸ“‰ Original performance: F1=0.036 (-95.2%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:11\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:11\n",
      "      ğŸ Processing completed in 5.6s\n",
      "      âš¡ Spectral Subtraction: 5.6s, 108.02x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.6s\n",
      "      âš¡ Spectral Subtraction: 5.6s, 108.02x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:26\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:26\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Wiener Filtering: 5.7s, 105.69x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Wiener Filtering: 5.7s, 105.69x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:41\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:56:41\n",
      "      ğŸ Processing completed in 12.8s\n",
      "      âš¡ LogMMSE: 12.8s, 46.89x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 12.8s\n",
      "      âš¡ LogMMSE: 12.8s, 46.89x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 2/5, ETA: 2.9 minutes\n",
      "\n",
      "ğŸ“ Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 2/5, ETA: 2.9 minutes\n",
      "\n",
      "ğŸ“ Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:05\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:05\n",
      "      ğŸ Processing completed in 6.6s\n",
      "      âš¡ Spectral Subtraction: 6.6s, 90.65x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 6.6s\n",
      "      âš¡ Spectral Subtraction: 6.6s, 90.65x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:22\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:22\n",
      "      ğŸ Processing completed in 5.8s\n",
      "      âš¡ Wiener Filtering: 5.8s, 103.93x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.8s\n",
      "      âš¡ Wiener Filtering: 5.8s, 103.93x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:38\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:57:38\n",
      "      ğŸ Processing completed in 14.5s\n",
      "      âš¡ LogMMSE: 14.5s, 41.27x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 14.5s\n",
      "      âš¡ LogMMSE: 14.5s, 41.27x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 3/5, ETA: 1.9 minutes\n",
      "\n",
      "ğŸ“ Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 3/5, ETA: 1.9 minutes\n",
      "\n",
      "ğŸ“ Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:01\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:01\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Spectral Subtraction: 5.7s, 105.96x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Spectral Subtraction: 5.7s, 105.96x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:16\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:16\n",
      "      ğŸ Processing completed in 5.9s\n",
      "      âš¡ Wiener Filtering: 5.9s, 101.67x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.9s\n",
      "      âš¡ Wiener Filtering: 5.9s, 101.67x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:31\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:31\n",
      "      ğŸ Processing completed in 13.0s\n",
      "      âš¡ LogMMSE: 13.0s, 46.11x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 13.0s\n",
      "      âš¡ LogMMSE: 13.0s, 46.11x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 4/5, ETA: 0.9 minutes\n",
      "\n",
      "ğŸ“ Condition 5/5: patient_01_wav_5db_coughing\n",
      "   ğŸ“‰ Original performance: F1=0.218 (-71.2%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 4/5, ETA: 0.9 minutes\n",
      "\n",
      "ğŸ“ Condition 5/5: patient_01_wav_5db_coughing\n",
      "   ğŸ“‰ Original performance: F1=0.218 (-71.2%)\n",
      "   ğŸ”§ Method 1/3: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:53\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Spectral Subtraction\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:58:53\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Spectral Subtraction: 5.7s, 105.25x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.7s\n",
      "      âš¡ Spectral Subtraction: 5.7s, 105.25x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/3: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:59:08\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for Wiener Filtering\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:59:08\n",
      "      ğŸ Processing completed in 5.8s\n",
      "      âš¡ Wiener Filtering: 5.8s, 102.75x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 5.8s\n",
      "      âš¡ Wiener Filtering: 5.8s, 102.75x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/3: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:59:23\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for LogMMSE\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 21:59:23\n",
      "      ğŸ Processing completed in 12.2s\n",
      "      âš¡ LogMMSE: 12.2s, 49.07x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 12.2s\n",
      "      âš¡ LogMMSE: 12.2s, 49.07x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ’¾ Saving downscaled processing results...\n",
      "ğŸ’¾ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "ğŸ’¾ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "â±ï¸  Total time: 276.0 seconds (4.6 minutes)\n",
      "ğŸ“Š Performance evaluations: 15\n",
      "âš¡ Efficiency measurements: 15\n",
      "ğŸ“ Sample size per condition: 20 files\n",
      "ğŸ¯ Total combinations processed: 5 conditions Ã— 3 methods\n",
      "\n",
      "ğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. Spectral Subtraction: 14.6% average recovery\n",
      "   2. LogMMSE: -9.1% average recovery\n",
      "   3. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 21:59:43\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 3 methods successful\n",
      "   â±ï¸  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ’¾ Saving downscaled processing results...\n",
      "ğŸ’¾ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "ğŸ’¾ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "â±ï¸  Total time: 276.0 seconds (4.6 minutes)\n",
      "ğŸ“Š Performance evaluations: 15\n",
      "âš¡ Efficiency measurements: 15\n",
      "ğŸ“ Sample size per condition: 20 files\n",
      "ğŸ¯ Total combinations processed: 5 conditions Ã— 3 methods\n",
      "\n",
      "ğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. Spectral Subtraction: 14.6% average recovery\n",
      "   2. LogMMSE: -9.1% average recovery\n",
      "   3. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 21:59:43\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Apply Denoising Methods with Downscaled Processing\n",
    "print(\"ğŸ”Š APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\")\n",
    "print(f\"Time started: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def apply_and_evaluate_single_method_downscaled(condition_name, condition_row, method_key, method_config, \n",
    "                                              model, feature_columns, audio_metadata, clean_baseline):\n",
    "    \"\"\"Apply a single denoising method and evaluate performance with downscaled sampling\"\"\"\n",
    "    \n",
    "    method_name = method_config['name']\n",
    "    method_script = method_config['script']\n",
    "    \n",
    "    print(f\"   ğŸ”§ Applying {method_name} to {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Find corresponding noisy audio directory\n",
    "        noisy_audio_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "        \n",
    "        if not os.path.exists(noisy_audio_dir):\n",
    "            print(f\"      âŒ Noisy audio directory not found: {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Apply downscaled sampling (20 files)\n",
    "        sampled_files = sample_files_downscaled(noisy_audio_dir, SAMPLE_SIZE_PER_CONDITION)\n",
    "        all_files_count = len([f for f in os.listdir(noisy_audio_dir) if f.lower().endswith('.wav')])\n",
    "        \n",
    "        if not sampled_files:\n",
    "            print(f\"      âŒ No files to sample from {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"      ğŸ“ Processing {len(sampled_files)} sampled files (from {all_files_count} total)\")\n",
    "        \n",
    "        # Create temporary directory with only sampled files\n",
    "        temp_input_dir = create_temp_sample_directory(\n",
    "            source_dir=noisy_audio_dir,\n",
    "            sampled_files=sampled_files,\n",
    "            temp_suffix=f\"{method_key}_{int(time.time())}\"\n",
    "        )\n",
    "        \n",
    "        # Create output directory for this method-condition combination\n",
    "        denoised_output_path = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "        os.makedirs(denoised_output_path, exist_ok=True)\n",
    "        \n",
    "        # Check if denoising already completed (based on sampled files)\n",
    "        efficiency_metrics = None\n",
    "        existing_files = []\n",
    "        \n",
    "        if os.path.exists(denoised_output_path):\n",
    "            existing_files = [f for f in os.listdir(denoised_output_path) if f.lower().endswith('.wav')]\n",
    "            expected_output = [f for f in sampled_files if f.lower().endswith('.wav')]\n",
    "            \n",
    "            if len(existing_files) >= len(expected_output) * 0.8:  # 80% completion threshold\n",
    "                print(f\"      âœ… {method_name}: Already completed ({len(existing_files)} files)\")\n",
    "                efficiency_metrics = {\n",
    "                    'method_name': method_name,\n",
    "                    'condition_name': condition_name,\n",
    "                    'processing_time_sec': None,  # Already completed\n",
    "                    'files_processed': len(existing_files),\n",
    "                    'success': True,\n",
    "                    'note': 'Previously completed (downscaled)',\n",
    "                    'total_files_available': all_files_count,\n",
    "                    'sampled_files_count': len(sampled_files),\n",
    "                    'sample_size_note': 'downscaled_20_files'\n",
    "                }\n",
    "            else:\n",
    "                print(f\"      ğŸ”„ {method_name}: Incomplete ({len(existing_files)}/{len(expected_output)}) - reprocessing\")\n",
    "        \n",
    "        # Apply denoising method if needed\n",
    "        if efficiency_metrics is None:\n",
    "            efficiency_metrics = measure_denoising_efficiency_downscaled(\n",
    "                input_dir=temp_input_dir,\n",
    "                output_dir=denoised_output_path,\n",
    "                method_script=method_script,\n",
    "                method_name=method_name\n",
    "            )\n",
    "            \n",
    "            # Add sampling metadata to efficiency metrics\n",
    "            if efficiency_metrics:\n",
    "                efficiency_metrics['condition_name'] = condition_name\n",
    "                efficiency_metrics['total_files_available'] = all_files_count\n",
    "                efficiency_metrics['sampled_files_count'] = len(sampled_files)\n",
    "                efficiency_metrics['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "        \n",
    "        # Clean up temporary directory\n",
    "        try:\n",
    "            if os.path.exists(temp_input_dir):\n",
    "                shutil.rmtree(temp_input_dir)\n",
    "                print(f\"      ğŸ§¹ Cleaned up temp directory\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"      âš ï¸  Temp cleanup warning: {cleanup_error}\")\n",
    "        \n",
    "        # Evaluate denoised audio performance\n",
    "        performance_results = None\n",
    "        if os.path.exists(denoised_output_path):\n",
    "            performance_results = evaluate_denoised_audio_downscaled(\n",
    "                denoised_audio_dir=denoised_output_path,\n",
    "                condition_name=condition_name,\n",
    "                method_name=method_name,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata\n",
    "            )\n",
    "            \n",
    "            if performance_results:\n",
    "                # Add original noisy performance for comparison\n",
    "                performance_results['original_f1'] = condition_row['f1_score']\n",
    "                performance_results['original_degradation_pct'] = condition_row['f1_degradation_pct']\n",
    "                \n",
    "                # Calculate recovery metrics\n",
    "                if clean_baseline:\n",
    "                    clean_f1 = clean_baseline['clean_f1_score']\n",
    "                    noisy_f1 = condition_row['f1_score']\n",
    "                    denoised_f1 = performance_results['f1_score']\n",
    "                    \n",
    "                    # Recovery percentage: (denoised - noisy) / (clean - noisy) * 100\n",
    "                    if clean_f1 > noisy_f1:\n",
    "                        recovery_pct = (denoised_f1 - noisy_f1) / (clean_f1 - noisy_f1) * 100\n",
    "                    else:\n",
    "                        recovery_pct = 0\n",
    "                    \n",
    "                    performance_results['f1_recovery_pct'] = recovery_pct\n",
    "                    performance_results['clean_baseline_f1'] = clean_f1\n",
    "                \n",
    "                # Add sampling metadata to performance results\n",
    "                performance_results['total_files_available'] = all_files_count\n",
    "                performance_results['sampled_files_count'] = len(sampled_files)\n",
    "                performance_results['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "                \n",
    "                print(f\"      âœ… {method_name}: F1={performance_results['f1_score']:.3f}, Recovery={recovery_pct:.1f}% [Sample: {len(sampled_files)}/{all_files_count}]\")\n",
    "        \n",
    "        return performance_results, efficiency_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ {method_name} failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main downscaled processing execution\n",
    "if priority_conditions is not None and model is not None:\n",
    "    \n",
    "    print(f\"ğŸš€ DOWNSCALED PROCESSING CONFIGURATION:\")\n",
    "    print(f\"   ğŸ¯ Conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   ğŸ”§ Methods: {len(DENOISING_METHODS)}\")\n",
    "    print(f\"   ğŸ“Š Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    print(f\"   ğŸ“ˆ Total evaluations: {len(priority_conditions) * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   â±ï¸  Expected time: ~30 minutes\")\n",
    "    \n",
    "    all_denoising_results = []\n",
    "    all_efficiency_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each condition sequentially (methods applied sequentially within each condition)\n",
    "    for condition_idx, (_, condition_row) in enumerate(priority_conditions.iterrows()):\n",
    "        condition_name = condition_row['condition_name']\n",
    "        \n",
    "        print(f\"\\nğŸ“ Condition {condition_idx + 1}/{len(priority_conditions)}: {condition_name}\")\n",
    "        print(f\"   ğŸ“‰ Original performance: F1={condition_row['f1_score']:.3f} (-{condition_row['f1_degradation_pct']:.1f}%)\")\n",
    "        \n",
    "        condition_results = []\n",
    "        condition_efficiency = []\n",
    "        \n",
    "        # Apply each denoising method to this condition\n",
    "        for method_idx, (method_key, method_config) in enumerate(DENOISING_METHODS.items()):\n",
    "            method_name = method_config['name']\n",
    "            print(f\"   ğŸ”§ Method {method_idx + 1}/{len(DENOISING_METHODS)}: {method_name}\")\n",
    "            \n",
    "            performance_result, efficiency_result = apply_and_evaluate_single_method_downscaled(\n",
    "                condition_name=condition_name,\n",
    "                condition_row=condition_row,\n",
    "                method_key=method_key,\n",
    "                method_config=method_config,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata,\n",
    "                clean_baseline=clean_baseline\n",
    "            )\n",
    "            \n",
    "            if performance_result:\n",
    "                condition_results.append(performance_result)\n",
    "                all_denoising_results.append(performance_result)\n",
    "            if efficiency_result:\n",
    "                condition_efficiency.append(efficiency_result)\n",
    "                all_efficiency_results.append(efficiency_result)\n",
    "        \n",
    "        # Progress and timing\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_conditions = len(priority_conditions) - (condition_idx + 1)\n",
    "        eta = (elapsed / (condition_idx + 1)) * remaining_conditions if condition_idx > 0 else 0\n",
    "        \n",
    "        print(f\"   âœ… Condition completed: {len(condition_results)} methods successful\")\n",
    "        print(f\"   â±ï¸  Progress: {condition_idx + 1}/{len(priority_conditions)}, ETA: {eta/60:.1f} minutes\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nğŸ’¾ Saving downscaled processing results...\")\n",
    "    \n",
    "    if all_denoising_results:\n",
    "        denoising_df = pd.DataFrame(all_denoising_results)\n",
    "        denoising_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_performance_results.csv\")\n",
    "        denoising_df.to_csv(denoising_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Denoising performance results saved: {denoising_results_path}\")\n",
    "    \n",
    "    if all_efficiency_results:\n",
    "        efficiency_df = pd.DataFrame(all_efficiency_results)\n",
    "        efficiency_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_efficiency_results.csv\")\n",
    "        efficiency_df.to_csv(efficiency_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Denoising efficiency results saved: {efficiency_results_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\")\n",
    "    print(f\"â±ï¸  Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Performance evaluations: {len(all_denoising_results)}\")\n",
    "    print(f\"âš¡ Efficiency measurements: {len(all_efficiency_results)}\")\n",
    "    print(f\"ğŸ“ Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"ğŸ¯ Total combinations processed: {len(priority_conditions)} conditions Ã— {len(DENOISING_METHODS)} methods\")\n",
    "    \n",
    "    # Method ranking preview\n",
    "    if all_denoising_results:\n",
    "        results_df = pd.DataFrame(all_denoising_results)\n",
    "        if 'f1_recovery_pct' in results_df.columns:\n",
    "            method_rankings = results_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            print(f\"\\nğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\")\n",
    "            for rank, (method, recovery) in enumerate(method_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% average recovery\")\n",
    "    \n",
    "    # Set global variables for subsequent cells\n",
    "    denoising_results = all_denoising_results\n",
    "    efficiency_results = all_efficiency_results\n",
    "    \n",
    "else:\n",
    "    if priority_conditions is None:\n",
    "        print(f\"âš ï¸  Priority conditions not available - check Phase 2 results\")\n",
    "    if model is None:\n",
    "        print(f\"âš ï¸  Model not loaded - cannot evaluate performance\")\n",
    "    \n",
    "    denoising_results = []\n",
    "    efficiency_results = []\n",
    "\n",
    "print(f\"\\nTime finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eada7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š SIGNAL QUALITY ASSESSMENT - DOWNSCALED\n",
      "==================================================\n",
      "ğŸ” Performing downscaled signal quality assessment on 15 denoising results...\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.52 dB (Â±0.99) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.52 dB (Â±0.99) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 4.36 dB (Â±1.42) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 4.36 dB (Â±1.42) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.77 dB (Â±0.51) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 6.77 dB (Â±0.51) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.24 dB (Â±0.64) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.24 dB (Â±0.64) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 4.49 dB (Â±0.55) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 4.49 dB (Â±0.55) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.39 dB (Â±0.73) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.39 dB (Â±0.73) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.68 dB (Â±0.26) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.68 dB (Â±0.26) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 4.32 dB (Â±0.45) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 4.32 dB (Â±0.45) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.80 dB (Â±0.38) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.80 dB (Â±0.38) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 4.33 dB (Â±1.70) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 4.33 dB (Â±1.70) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.42 dB (Â±1.25) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.42 dB (Â±1.25) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.22 dB (Â±1.08) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 5.22 dB (Â±1.08) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 8.45 dB (Â±0.59) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 8.45 dB (Â±0.59) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 5.61 dB (Â±0.81) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 5.61 dB (Â±0.81) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 6.19 dB (Â±0.47) [n=10]\n",
      "\n",
      "ğŸ’¾ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "ğŸ“Š Signal Quality Summary (Downscaled):\n",
      "   ğŸ“ˆ Average SNR improvement: 5.59 dB\n",
      "   ğŸ“‰ Average spectral distortion: 2.4072\n",
      "   ğŸ† Best SNR improvement: Spectral Subtraction (8.45 dB)\n",
      "   ğŸ’¥ Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "âœ… Signal quality assessment complete (downscaled)\n",
      "      âœ… SNR improvement: 6.19 dB (Â±0.47) [n=10]\n",
      "\n",
      "ğŸ’¾ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "ğŸ“Š Signal Quality Summary (Downscaled):\n",
      "   ğŸ“ˆ Average SNR improvement: 5.59 dB\n",
      "   ğŸ“‰ Average spectral distortion: 2.4072\n",
      "   ğŸ† Best SNR improvement: Spectral Subtraction (8.45 dB)\n",
      "   ğŸ’¥ Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "âœ… Signal quality assessment complete (downscaled)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Signal Quality Assessment (Downscaled)\n",
    "print(\"ğŸ“Š SIGNAL QUALITY ASSESSMENT - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def calculate_snr(clean_audio, noisy_audio):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio in dB\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(noisy_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        noisy_audio = noisy_audio[:min_len]\n",
    "        \n",
    "        # Calculate signal and noise power\n",
    "        signal_power = np.mean(clean_audio ** 2)\n",
    "        noise_power = np.mean((noisy_audio - clean_audio) ** 2)\n",
    "        \n",
    "        if noise_power > 0:\n",
    "            snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "        else:\n",
    "            snr_db = float('inf')\n",
    "        \n",
    "        return snr_db\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_spectral_distortion(clean_audio, processed_audio, sr=16000):\n",
    "    \"\"\"Calculate spectral distortion between clean and processed audio\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(processed_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        processed_audio = processed_audio[:min_len]\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        clean_spec = np.abs(librosa.stft(clean_audio))\n",
    "        processed_spec = np.abs(librosa.stft(processed_audio))\n",
    "        \n",
    "        # Calculate L2 distance\n",
    "        min_time = min(clean_spec.shape[1], processed_spec.shape[1])\n",
    "        clean_spec = clean_spec[:, :min_time]\n",
    "        processed_spec = processed_spec[:, :min_time]\n",
    "        \n",
    "        spectral_distance = np.sqrt(np.mean((clean_spec - processed_spec) ** 2))\n",
    "        \n",
    "        return spectral_distance\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def assess_signal_quality_downscaled(condition_name, method_name, clean_dir, noisy_dir, denoised_dir, sample_size=10):\n",
    "    \"\"\"Assess signal quality improvement for downscaled method-condition combination\"\"\"\n",
    "    \n",
    "    print(f\"   ğŸ” Assessing {method_name} on {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample files for analysis (use min between sample_size and available files)\n",
    "        denoised_files = [f for f in os.listdir(denoised_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = denoised_files[:min(sample_size, len(denoised_files))]\n",
    "        \n",
    "        snr_improvements = []\n",
    "        spectral_distortions = []\n",
    "        processed_files = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                denoised_path = os.path.join(denoised_dir, wav_file)\n",
    "                denoised_audio, sr = librosa.load(denoised_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Find corresponding noisy and clean files\n",
    "                noisy_path = os.path.join(noisy_dir, wav_file.replace('denoised_', '').replace('mixed_', 'mixed_'))\n",
    "                \n",
    "                # Try to find clean file (remove patient prefix from condition name)\n",
    "                condition_parts = condition_name.split('_')\n",
    "                patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "                clean_filename = wav_file.replace('mixed_', '').replace('denoised_', '')\n",
    "                clean_path = os.path.join(clean_dir, f\"{patient_id}_wav\", clean_filename)\n",
    "                \n",
    "                if os.path.exists(noisy_path):\n",
    "                    noisy_audio, _ = librosa.load(noisy_path, sr=TARGET_SAMPLE_RATE)\n",
    "                    \n",
    "                    if os.path.exists(clean_path):\n",
    "                        clean_audio, _ = librosa.load(clean_path, sr=TARGET_SAMPLE_RATE)\n",
    "                        \n",
    "                        # Calculate SNR improvement\n",
    "                        noisy_snr = calculate_snr(clean_audio, noisy_audio)\n",
    "                        denoised_snr = calculate_snr(clean_audio, denoised_audio)\n",
    "                        \n",
    "                        if noisy_snr is not None and denoised_snr is not None:\n",
    "                            snr_improvement = denoised_snr - noisy_snr\n",
    "                            snr_improvements.append(snr_improvement)\n",
    "                        \n",
    "                        # Calculate spectral distortion\n",
    "                        spectral_dist = calculate_spectral_distortion(clean_audio, denoised_audio)\n",
    "                        if spectral_dist is not None:\n",
    "                            spectral_distortions.append(spectral_dist)\n",
    "                    \n",
    "                    processed_files += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if processed_files < 2:  # Show first 2 errors\n",
    "                    print(f\"      âš ï¸  Error processing {wav_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        quality_metrics = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'files_analyzed': processed_files,\n",
    "            'snr_improvement_db': np.mean(snr_improvements) if snr_improvements else None,\n",
    "            'snr_improvement_std': np.std(snr_improvements) if snr_improvements else None,\n",
    "            'spectral_distortion': np.mean(spectral_distortions) if spectral_distortions else None,\n",
    "            'spectral_distortion_std': np.std(spectral_distortions) if spectral_distortions else None,\n",
    "            'sample_size_note': 'downscaled_analysis'\n",
    "        }\n",
    "        \n",
    "        if snr_improvements:\n",
    "            print(f\"      âœ… SNR improvement: {np.mean(snr_improvements):.2f} dB (Â±{np.std(snr_improvements):.2f}) [n={processed_files}]\")\n",
    "        \n",
    "        return quality_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Quality assessment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Perform signal quality assessment if denoising results are available\n",
    "signal_quality_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ” Performing downscaled signal quality assessment on {len(denoising_results)} denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    for result in denoising_results:\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            clean_dir = BASE_DATA_DIR\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(denoised_dir) and os.path.exists(noisy_dir):\n",
    "                quality_result = assess_signal_quality_downscaled(\n",
    "                    condition_name=condition_name,\n",
    "                    method_name=method_name,\n",
    "                    clean_dir=clean_dir,\n",
    "                    noisy_dir=noisy_dir,\n",
    "                    denoised_dir=denoised_dir,\n",
    "                    sample_size=10  # Analyze 10 files per condition (downscaled)\n",
    "                )\n",
    "                \n",
    "                if quality_result:\n",
    "                    signal_quality_results.append(quality_result)\n",
    "    \n",
    "    # Save signal quality results\n",
    "    if signal_quality_results:\n",
    "        quality_df = pd.DataFrame(signal_quality_results)\n",
    "        quality_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_signal_quality_results.csv\")\n",
    "        quality_df.to_csv(quality_results_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Signal quality results saved: {quality_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nğŸ“Š Signal Quality Summary (Downscaled):\")\n",
    "        valid_snr = quality_df[quality_df['snr_improvement_db'].notna()]\n",
    "        if not valid_snr.empty:\n",
    "            print(f\"   ğŸ“ˆ Average SNR improvement: {valid_snr['snr_improvement_db'].mean():.2f} dB\")\n",
    "            print(f\"   ğŸ“‰ Average spectral distortion: {quality_df['spectral_distortion'].mean():.4f}\")\n",
    "            \n",
    "            # Best and worst methods for signal quality\n",
    "            best_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmax()]\n",
    "            worst_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmin()]\n",
    "            \n",
    "            print(f\"   ğŸ† Best SNR improvement: {best_snr['method_name']} ({best_snr['snr_improvement_db']:.2f} dB)\")\n",
    "            print(f\"   ğŸ’¥ Worst SNR improvement: {worst_snr['method_name']} ({worst_snr['snr_improvement_db']:.2f} dB)\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  No valid SNR measurements available\")\n",
    "    \n",
    "    print(f\"\\nâœ… Signal quality assessment complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Signal quality assessment skipped - no denoising results available\")\n",
    "    signal_quality_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e0d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\n",
      "==================================================\n",
      "ğŸ§ª Analyzing feature preservation for downscaled denoising results...\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.309\n",
      "      âœ… Variance preservation: 2.137\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.309\n",
      "      âœ… Variance preservation: 2.137\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.227\n",
      "      âœ… Variance preservation: 9.564\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.227\n",
      "      âœ… Variance preservation: 9.564\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.022\n",
      "      âœ… Variance preservation: 5.407\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.022\n",
      "      âœ… Variance preservation: 5.407\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.062\n",
      "      âœ… Variance preservation: 9.254\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.062\n",
      "      âœ… Variance preservation: 9.254\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.588\n",
      "      âœ… Variance preservation: 2.053\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.588\n",
      "      âœ… Variance preservation: 2.053\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.006\n",
      "      âœ… Variance preservation: 40.163\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.006\n",
      "      âœ… Variance preservation: 40.163\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.180\n",
      "      âœ… Variance preservation: 7.205\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.180\n",
      "      âœ… Variance preservation: 7.205\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.088\n",
      "      âœ… Variance preservation: 9.119\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.088\n",
      "      âœ… Variance preservation: 9.119\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.130\n",
      "      âœ… Variance preservation: 4.620\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.130\n",
      "      âœ… Variance preservation: 4.620\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.920\n",
      "      âœ… Variance preservation: 10.945\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.920\n",
      "      âœ… Variance preservation: 10.945\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.153\n",
      "      âœ… Variance preservation: 19.279\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.153\n",
      "      âœ… Variance preservation: 19.279\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m clean_features, clean_count = extract_features_from_audio_dir_downscaled(clean_dir, sample_size=\u001b[32m15\u001b[39m)\n\u001b[32m    143\u001b[39m noisy_features, noisy_count = extract_features_from_audio_dir_downscaled(noisy_dir, sample_size=\u001b[32m15\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m denoised_features, denoised_count = \u001b[43mextract_features_from_audio_dir_downscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoised_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      ğŸ“ˆ Features extracted: Clean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Noisy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoisy_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Denoised=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdenoised_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_features \u001b[38;5;129;01mand\u001b[39;00m noisy_features \u001b[38;5;129;01mand\u001b[39;00m denoised_features:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mextract_features_from_audio_dir_downscaled\u001b[39m\u001b[34m(audio_dir, sample_size)\u001b[39m\n\u001b[32m     94\u001b[39m wav_path = os.path.join(audio_dir, wav_file)\n\u001b[32m     95\u001b[39m audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m features = \u001b[43mextract_comprehensive_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features:\n\u001b[32m     99\u001b[39m     features_list.append(features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mextract_comprehensive_features\u001b[39m\u001b[34m(audio_frame, sample_rate)\u001b[39m\n\u001b[32m     11\u001b[39m centroid = \u001b[38;5;28mfloat\u001b[39m(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n\u001b[32m     12\u001b[39m bandwidth = \u001b[38;5;28mfloat\u001b[39m(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m rolloff = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectral_rolloff\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m.mean())\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# MFCCs (first 8 coefficients)\u001b[39;00m\n\u001b[32m     16\u001b[39m mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=\u001b[32m8\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\feature\\spectral.py:638\u001b[39m, in \u001b[36mspectral_rolloff\u001b[39m\u001b[34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, freq, roll_percent)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0.0\u001b[39m < roll_percent < \u001b[32m1.0\u001b[39m:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\u001b[33m\"\u001b[39m\u001b[33mroll_percent must lie in the range (0, 1)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m S, n_fft = \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[43m=\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isrealobj(S):\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpectral rolloff is only defined \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mwith real-valued input\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:2945\u001b[39m, in \u001b[36m_spectrogram\u001b[39m\u001b[34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[39m\n\u001b[32m   2939\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2940\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[32m   2941\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2942\u001b[39m         )\n\u001b[32m   2943\u001b[39m     S = (\n\u001b[32m   2944\u001b[39m         np.abs(\n\u001b[32m-> \u001b[39m\u001b[32m2945\u001b[39m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2949\u001b[39m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2950\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2951\u001b[39m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2952\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2954\u001b[39m         )\n\u001b[32m   2955\u001b[39m         ** power\n\u001b[32m   2956\u001b[39m     )\n\u001b[32m   2958\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:387\u001b[39m, in \u001b[36mstft\u001b[39m\u001b[34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, y_frames.shape[-\u001b[32m1\u001b[39m], n_columns):\n\u001b[32m    385\u001b[39m     bl_t = \u001b[38;5;28mmin\u001b[39m(bl_s + n_columns, y_frames.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     stft_matrix[..., bl_s + off_start : bl_t + off_start] = \u001b[43mfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:91\u001b[39m, in \u001b[36mrfft\u001b[39m\u001b[34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrfft\u001b[39m(x, n=\u001b[38;5;28;01mNone\u001b[39;00m, axis=-\u001b[32m1\u001b[39m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     90\u001b[39m          overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrfft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[39m, in \u001b[36m_execute_1D\u001b[39m\u001b[34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     31\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:61\u001b[39m, in \u001b[36mr2c\u001b[39m\u001b[34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp.shape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: Feature Preservation Analysis (Downscaled)\n",
    "print(\"ğŸ§¬ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def analyze_feature_preservation_downscaled(clean_features, noisy_features, denoised_features):\n",
    "    \"\"\"Analyze how well denoising preserves important breathing features (downscaled)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to DataFrames if needed\n",
    "        if isinstance(clean_features, list):\n",
    "            clean_df = pd.DataFrame(clean_features)\n",
    "        else:\n",
    "            clean_df = clean_features\n",
    "            \n",
    "        if isinstance(noisy_features, list):\n",
    "            noisy_df = pd.DataFrame(noisy_features)\n",
    "        else:\n",
    "            noisy_df = noisy_features\n",
    "            \n",
    "        if isinstance(denoised_features, list):\n",
    "            denoised_df = pd.DataFrame(denoised_features)\n",
    "        else:\n",
    "            denoised_df = denoised_features\n",
    "        \n",
    "        # Ensure same features and sample size\n",
    "        common_features = list(set(clean_df.columns) & set(denoised_df.columns) & set(noisy_df.columns))\n",
    "        min_samples = min(len(clean_df), len(noisy_df), len(denoised_df))\n",
    "        \n",
    "        clean_df = clean_df[common_features].iloc[:min_samples]\n",
    "        noisy_df = noisy_df[common_features].iloc[:min_samples]\n",
    "        denoised_df = denoised_df[common_features].iloc[:min_samples]\n",
    "        \n",
    "        preservation_metrics = {}\n",
    "        \n",
    "        for feature in common_features:\n",
    "            # Correlation preservation\n",
    "            clean_values = clean_df[feature].values\n",
    "            noisy_values = noisy_df[feature].values\n",
    "            denoised_values = denoised_df[feature].values\n",
    "            \n",
    "            # Calculate correlations with original clean values\n",
    "            try:\n",
    "                clean_noisy_corr = np.corrcoef(clean_values, noisy_values)[0, 1]\n",
    "                clean_denoised_corr = np.corrcoef(clean_values, denoised_values)[0, 1]\n",
    "                \n",
    "                # Correlation recovery: how much of the original correlation is restored\n",
    "                if not np.isnan(clean_noisy_corr) and not np.isnan(clean_denoised_corr):\n",
    "                    correlation_recovery = clean_denoised_corr / clean_noisy_corr if clean_noisy_corr != 0 else 1\n",
    "                else:\n",
    "                    correlation_recovery = 0\n",
    "            except:\n",
    "                clean_noisy_corr = 0\n",
    "                clean_denoised_corr = 0\n",
    "                correlation_recovery = 0\n",
    "            \n",
    "            # Variance preservation\n",
    "            clean_var = np.var(clean_values)\n",
    "            noisy_var = np.var(noisy_values)\n",
    "            denoised_var = np.var(denoised_values)\n",
    "            \n",
    "            variance_ratio = denoised_var / clean_var if clean_var > 0 else 0\n",
    "            \n",
    "            # Mean preservation\n",
    "            clean_mean = np.mean(clean_values)\n",
    "            denoised_mean = np.mean(denoised_values)\n",
    "            mean_error = abs(clean_mean - denoised_mean) / abs(clean_mean) if clean_mean != 0 else 0\n",
    "            \n",
    "            preservation_metrics[feature] = {\n",
    "                'clean_noisy_correlation': clean_noisy_corr,\n",
    "                'clean_denoised_correlation': clean_denoised_corr,\n",
    "                'correlation_recovery': correlation_recovery,\n",
    "                'variance_ratio': variance_ratio,\n",
    "                'mean_relative_error': mean_error\n",
    "            }\n",
    "        \n",
    "        return preservation_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Feature preservation analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features_from_audio_dir_downscaled(audio_dir, sample_size=15):\n",
    "    \"\"\"Extract features from a directory of audio files (downscaled sample)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        wav_files = [f for f in os.listdir(audio_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = wav_files[:min(sample_size, len(wav_files))]\n",
    "        \n",
    "        features_list = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                wav_path = os.path.join(audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features:\n",
    "                    features_list.append(features)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return features_list, processed_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Feature extraction from directory failed: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Perform feature preservation analysis\n",
    "feature_preservation_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ§ª Analyzing feature preservation for downscaled denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    method_condition_pairs = list(set([(r['condition_name'], r['method_name']) for r in denoising_results]))\n",
    "    \n",
    "    for condition_name, method_name in method_condition_pairs:\n",
    "        print(f\"\\n   ğŸ”¬ Analyzing {method_name} on {condition_name}...\")\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            condition_parts = condition_name.split('_')\n",
    "            patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "            clean_dir = os.path.join(BASE_DATA_DIR, f\"{patient_id}_wav\")\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(clean_dir) and os.path.exists(noisy_dir) and os.path.exists(denoised_dir):\n",
    "                # Extract features from each audio type (downscaled samples)\n",
    "                print(f\"      ğŸ“Š Extracting features for comparison (downscaled)...\")\n",
    "                \n",
    "                clean_features, clean_count = extract_features_from_audio_dir_downscaled(clean_dir, sample_size=15)\n",
    "                noisy_features, noisy_count = extract_features_from_audio_dir_downscaled(noisy_dir, sample_size=15)\n",
    "                denoised_features, denoised_count = extract_features_from_audio_dir_downscaled(denoised_dir, sample_size=15)\n",
    "                \n",
    "                print(f\"      ğŸ“ˆ Features extracted: Clean={clean_count}, Noisy={noisy_count}, Denoised={denoised_count}\")\n",
    "                \n",
    "                if clean_features and noisy_features and denoised_features:\n",
    "                    preservation_metrics = analyze_feature_preservation_downscaled(\n",
    "                        clean_features=clean_features,\n",
    "                        noisy_features=noisy_features,\n",
    "                        denoised_features=denoised_features\n",
    "                    )\n",
    "                    \n",
    "                    if preservation_metrics:\n",
    "                        # Calculate aggregate preservation scores\n",
    "                        correlation_recoveries = [m['correlation_recovery'] for m in preservation_metrics.values() if not np.isnan(m['correlation_recovery']) and not np.isinf(m['correlation_recovery'])]\n",
    "                        variance_ratios = [m['variance_ratio'] for m in preservation_metrics.values() if not np.isnan(m['variance_ratio']) and not np.isinf(m['variance_ratio'])]\n",
    "                        mean_errors = [m['mean_relative_error'] for m in preservation_metrics.values() if not np.isnan(m['mean_relative_error']) and not np.isinf(m['mean_relative_error'])]\n",
    "                        \n",
    "                        aggregate_result = {\n",
    "                            'condition_name': condition_name,\n",
    "                            'method_name': method_name,\n",
    "                            'avg_correlation_recovery': np.mean(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'std_correlation_recovery': np.std(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'avg_variance_ratio': np.mean(variance_ratios) if variance_ratios else 0,\n",
    "                            'std_variance_ratio': np.std(variance_ratios) if variance_ratios else 0,\n",
    "                            'avg_mean_error': np.mean(mean_errors) if mean_errors else 0,\n",
    "                            'features_analyzed': len(preservation_metrics),\n",
    "                            'sample_size_note': 'downscaled_15_files',\n",
    "                            'detailed_metrics': preservation_metrics\n",
    "                        }\n",
    "                        \n",
    "                        feature_preservation_results.append(aggregate_result)\n",
    "                        \n",
    "                        print(f\"      âœ… Correlation recovery: {aggregate_result['avg_correlation_recovery']:.3f}\")\n",
    "                        print(f\"      âœ… Variance preservation: {aggregate_result['avg_variance_ratio']:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                missing_dirs = []\n",
    "                if not os.path.exists(clean_dir): missing_dirs.append(f\"clean ({clean_dir})\")\n",
    "                if not os.path.exists(noisy_dir): missing_dirs.append(f\"noisy ({noisy_dir})\")\n",
    "                if not os.path.exists(denoised_dir): missing_dirs.append(f\"denoised ({denoised_dir})\")\n",
    "                print(f\"      âš ï¸  Missing directories: {', '.join(missing_dirs)}\")\n",
    "    \n",
    "    # Save feature preservation results\n",
    "    if feature_preservation_results:\n",
    "        # Save aggregate results\n",
    "        preservation_summary = [{\n",
    "            'condition_name': r['condition_name'],\n",
    "            'method_name': r['method_name'],\n",
    "            'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "            'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "            'avg_mean_error': r['avg_mean_error'],\n",
    "            'features_analyzed': r['features_analyzed'],\n",
    "            'sample_size_note': r['sample_size_note']\n",
    "        } for r in feature_preservation_results]\n",
    "        \n",
    "        preservation_df = pd.DataFrame(preservation_summary)\n",
    "        preservation_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_feature_preservation_results.csv\")\n",
    "        preservation_df.to_csv(preservation_results_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Feature preservation results saved: {preservation_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nğŸ§¬ Feature Preservation Summary (Downscaled):\")\n",
    "        print(f\"   ğŸ“Š Average correlation recovery: {preservation_df['avg_correlation_recovery'].mean():.3f}\")\n",
    "        print(f\"   ğŸ“Š Average variance preservation: {preservation_df['avg_variance_ratio'].mean():.3f}\")\n",
    "        print(f\"   ğŸ“Š Average mean error: {preservation_df['avg_mean_error'].mean():.3f}\")\n",
    "        \n",
    "        # Best and worst methods for feature preservation\n",
    "        if len(preservation_df) > 1:\n",
    "            best_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmax()]\n",
    "            worst_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmin()]\n",
    "            \n",
    "            print(f\"   ğŸ† Best preservation: {best_preservation['method_name']} ({best_preservation['avg_correlation_recovery']:.3f})\")\n",
    "            print(f\"   ğŸ’¥ Worst preservation: {worst_preservation['method_name']} ({worst_preservation['avg_correlation_recovery']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nâœ… Feature preservation analysis complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Feature preservation analysis skipped - no denoising results available\")\n",
    "    feature_preservation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compile Comprehensive Results and Multi-Criteria Analysis (Downscaled)\n",
    "print(\"ğŸ” COMPILING COMPREHENSIVE RESULTS AND MULTI-CRITERIA ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Compile all results into comprehensive dataset\n",
    "comprehensive_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ“Š Compiling results from {len(denoising_results)} downscaled denoising evaluations...\")\n",
    "    \n",
    "    # Convert results lists to DataFrames for easier merging\n",
    "    denoising_df = pd.DataFrame(denoising_results) if denoising_results else pd.DataFrame()\n",
    "    efficiency_df = pd.DataFrame(efficiency_results) if 'efficiency_results' in locals() and efficiency_results else pd.DataFrame()\n",
    "    quality_df = pd.DataFrame(signal_quality_results) if 'signal_quality_results' in locals() and signal_quality_results else pd.DataFrame()\n",
    "    preservation_df = pd.DataFrame([{\n",
    "        'condition_name': r['condition_name'],\n",
    "        'method_name': r['method_name'],\n",
    "        'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "        'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "        'avg_mean_error': r['avg_mean_error']\n",
    "    } for r in feature_preservation_results]) if 'feature_preservation_results' in locals() and feature_preservation_results else pd.DataFrame()\n",
    "    \n",
    "    # Merge all results on condition_name and method_name\n",
    "    for _, result in denoising_df.iterrows():\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Start with denoising performance results\n",
    "        comprehensive_result = result.to_dict()\n",
    "        \n",
    "        # Add efficiency metrics\n",
    "        efficiency_match = efficiency_df[\n",
    "            (efficiency_df['condition_name'] == condition_name) & \n",
    "            (efficiency_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not efficiency_match.empty:\n",
    "            eff_result = efficiency_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'processing_time_sec': eff_result.get('processing_time_sec'),\n",
    "                'real_time_factor': eff_result.get('real_time_factor'),\n",
    "                'memory_usage_mb': eff_result.get('memory_usage_mb'),\n",
    "                'processing_speed_files_per_sec': eff_result.get('processing_speed_files_per_sec')\n",
    "            })\n",
    "        \n",
    "        # Add signal quality metrics\n",
    "        quality_match = quality_df[\n",
    "            (quality_df['condition_name'] == condition_name) & \n",
    "            (quality_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not quality_match.empty:\n",
    "            qual_result = quality_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'snr_improvement_db': qual_result.get('snr_improvement_db'),\n",
    "                'spectral_distortion': qual_result.get('spectral_distortion')\n",
    "            })\n",
    "        \n",
    "        # Add feature preservation metrics\n",
    "        preservation_match = preservation_df[\n",
    "            (preservation_df['condition_name'] == condition_name) & \n",
    "            (preservation_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not preservation_match.empty:\n",
    "            pres_result = preservation_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'avg_correlation_recovery': pres_result.get('avg_correlation_recovery'),\n",
    "                'avg_variance_ratio': pres_result.get('avg_variance_ratio'),\n",
    "                'avg_mean_error': pres_result.get('avg_mean_error')\n",
    "            })\n",
    "        \n",
    "        comprehensive_results.append(comprehensive_result)\n",
    "    \n",
    "    print(f\"âœ… Compiled {len(comprehensive_results)} comprehensive result records\")\n",
    "    \n",
    "    # Calculate normalized scores for multi-criteria analysis\n",
    "    if comprehensive_results:\n",
    "        comp_df = pd.DataFrame(comprehensive_results)\n",
    "        \n",
    "        # Normalize scores (0-1 scale) for smartphone suitability calculation\n",
    "        def normalize_score(values, higher_is_better=True):\n",
    "            values = pd.Series(values).fillna(0)  # Handle NaN values\n",
    "            if values.std() == 0:  # All values are the same\n",
    "                return pd.Series([0.5] * len(values))\n",
    "            if higher_is_better:\n",
    "                return (values - values.min()) / (values.max() - values.min())\n",
    "            else:\n",
    "                return (values.max() - values) / (values.max() - values.min())\n",
    "        \n",
    "        # Calculate individual dimension scores (handle NaN values gracefully)\n",
    "        comp_df['f1_recovery_score'] = normalize_score(comp_df.get('f1_recovery_pct', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['efficiency_score'] = normalize_score(comp_df.get('real_time_factor', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['signal_quality_score'] = normalize_score(comp_df.get('snr_improvement_db', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['feature_preservation_score'] = normalize_score(comp_df.get('avg_correlation_recovery', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        \n",
    "        # Calculate smartphone suitability composite score\n",
    "        SMARTPHONE_WEIGHTS = {\n",
    "            'f1_recovery': 0.40,        # Detection performance recovery (most critical)\n",
    "            'efficiency': 0.25,         # Processing speed + memory usage\n",
    "            'signal_quality': 0.20,     # SNR improvement + artifact control\n",
    "            'feature_preservation': 0.15 # Biomarker stability\n",
    "        }\n",
    "        \n",
    "        comp_df['smartphone_suitability_score'] = (\n",
    "            comp_df['f1_recovery_score'] * SMARTPHONE_WEIGHTS['f1_recovery'] +\n",
    "            comp_df['efficiency_score'] * SMARTPHONE_WEIGHTS['efficiency'] +\n",
    "            comp_df['signal_quality_score'] * SMARTPHONE_WEIGHTS['signal_quality'] +\n",
    "            comp_df['feature_preservation_score'] * SMARTPHONE_WEIGHTS['feature_preservation']\n",
    "        )\n",
    "        \n",
    "        # Update comprehensive_results with calculated scores\n",
    "        comprehensive_results = comp_df.to_dict('records')\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        comprehensive_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_comprehensive_results.csv\")\n",
    "        comp_df.to_csv(comprehensive_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Comprehensive results saved: {comprehensive_results_path}\")\n",
    "        \n",
    "        # Display ranking summary\n",
    "        print(f\"\\nğŸ† SMARTPHONE SUITABILITY RANKING (DOWNSCALED):\")\n",
    "        method_rankings = comp_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=False)\n",
    "        for rank, (method, score) in enumerate(method_rankings.items(), 1):\n",
    "            print(f\"   {rank}. {method}: {score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š PERFORMANCE RECOVERY RANKING (DOWNSCALED):\")\n",
    "        if 'f1_recovery_pct' in comp_df.columns:\n",
    "            performance_rankings = comp_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            for rank, (method, recovery) in enumerate(performance_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% F1 recovery\")\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸  No denoising results available for comprehensive analysis\")\n",
    "    print(f\"   Please ensure Cell 5 (denoising application) has been executed successfully\")\n",
    "    comprehensive_results = []\n",
    "\n",
    "if comprehensive_results:\n",
    "    # Generate final summary report for downscaled analysis\n",
    "    print(f\"\\nğŸ“‹ DOWNSCALED PHASE 3 FINAL SUMMARY REPORT:\")\n",
    "    print(f\"\\nğŸ¯ Research Objective Achievement:\")\n",
    "    print(f\"   âœ… Evaluated {len(DENOISING_METHODS)} denoising methods\")\n",
    "    print(f\"   âœ… Tested on {len(REPRESENTATIVE_CONDITIONS)} representative worst-case conditions (5dB SNR)\")\n",
    "    print(f\"   âœ… Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\")\n",
    "    print(f\"   âœ… Generated smartphone deployment recommendations\")\n",
    "    print(f\"   âœ… Downscaled scope: {SAMPLE_SIZE_PER_CONDITION} files per condition for rapid evaluation\")\n",
    "    \n",
    "    # Key findings\n",
    "    comp_df = pd.DataFrame(comprehensive_results)\n",
    "    best_overall = comp_df.loc[comp_df['smartphone_suitability_score'].idxmax()]\n",
    "    \n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        best_performance = comp_df.loc[comp_df['f1_recovery_pct'].idxmax()]\n",
    "    else:\n",
    "        best_performance = None\n",
    "    \n",
    "    valid_efficiency = comp_df[comp_df['real_time_factor'].notna()] if 'real_time_factor' in comp_df.columns else pd.DataFrame()\n",
    "    best_efficiency = valid_efficiency.loc[valid_efficiency['real_time_factor'].idxmax()] if not valid_efficiency.empty else None\n",
    "    \n",
    "    print(f\"\\nğŸ† Key Findings from Downscaled Sampling:\")\n",
    "    print(f\"   ğŸ¥‡ Best Overall Method: {best_overall['method_name']} (Score: {best_overall['smartphone_suitability_score']:.3f})\")\n",
    "    if best_performance is not None:\n",
    "        print(f\"   ğŸ¯ Best Performance Recovery: {best_performance['method_name']} ({best_performance['f1_recovery_pct']:.1f}% F1 recovery)\")\n",
    "    if best_efficiency is not None:\n",
    "        print(f\"   âš¡ Most Efficient: {best_efficiency['method_name']} ({best_efficiency['real_time_factor']:.2f}x real-time)\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        avg_recovery = comp_df['f1_recovery_pct'].mean()\n",
    "        recovery_std = comp_df['f1_recovery_pct'].std()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Downscaled Performance Statistics:\")\n",
    "        print(f\"   ğŸ“ˆ Average F1 Recovery: {avg_recovery:.1f}% (Â±{recovery_std:.1f}%)\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >50% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 50])} / {len(comp_df)}\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >75% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 75])} / {len(comp_df)}\")\n",
    "        print(f\"   ğŸ¯ Conditions tested: {len(REPRESENTATIVE_CONDITIONS)} worst-case (5dB) per noise category\")\n",
    "        print(f\"   ğŸ“ Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    # Save final summary\n",
    "    final_summary = {\n",
    "        'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'phase3_status': 'completed_downscaled',\n",
    "        'downscaled_optimization': {\n",
    "            'sample_size_per_condition': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'total_files_per_evaluation': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'execution_approach': 'rapid_prototyping'\n",
    "        },\n",
    "        'representative_conditions': REPRESENTATIVE_CONDITIONS,\n",
    "        'methods_evaluated': len(DENOISING_METHODS),\n",
    "        'conditions_tested': len(REPRESENTATIVE_CONDITIONS),\n",
    "        'total_evaluations': len(comp_df),\n",
    "        'best_overall_method': best_overall['method_name'],\n",
    "        'best_overall_score': float(best_overall['smartphone_suitability_score']),\n",
    "        'research_contributions': [\n",
    "            'Downscaled systematic multi-dimensional evaluation of denoising for sleep apnea detection',\n",
    "            'Rapid prototyping methodology for efficient method comparison',\n",
    "            'Smartphone deployment feasibility assessment with reduced computational requirements',\n",
    "            'Evidence-based method recommendations for mobile health applications',\n",
    "            'Performance-efficiency trade-off quantification for worst-case scenarios'\n",
    "        ],\n",
    "        'files_generated': [\n",
    "            'downscaled_comprehensive_results.csv',\n",
    "            'downscaled_denoising_performance_results.csv',\n",
    "            'downscaled_denoising_efficiency_results.csv',\n",
    "            'downscaled_signal_quality_results.csv',\n",
    "            'downscaled_feature_preservation_results.csv'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if best_performance is not None:\n",
    "        final_summary['best_performance_method'] = best_performance['method_name']\n",
    "        final_summary['best_performance_recovery'] = float(best_performance['f1_recovery_pct'])\n",
    "        final_summary['average_f1_recovery'] = float(avg_recovery)\n",
    "    \n",
    "    summary_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_phase3_final_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    print(f\"\\nğŸ’¾ Final summary saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\")\n",
    "    print(f\"\\nğŸ“‹ Research Contributions Achieved:\")\n",
    "    print(f\"   âœ… Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\")\n",
    "    print(f\"   âœ… Downscaled sampling methodology for efficient evaluation\")\n",
    "    print(f\"   âœ… Smartphone deployment feasibility assessment\")\n",
    "    print(f\"   âœ… Evidence-based method recommendations for mobile health applications\")\n",
    "    print(f\"   âœ… Performance-efficiency trade-off quantification for worst-case scenarios\")\n",
    "    print(f\"   âœ… Proof-of-concept validation with {SAMPLE_SIZE_PER_CONDITION}-file sampling approach\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Comprehensive analysis skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive results compilation complete (downscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Comprehensive Visualization and Final Results (Downscaled)\n",
    "print(\"ğŸ“ˆ COMPREHENSIVE VISUALIZATION AND FINAL RESULTS - DOWNSCALED\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if 'comprehensive_results' in locals() and comprehensive_results:\n",
    "    # Set up plotting\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    comprehensive_df = pd.DataFrame(comprehensive_results)\n",
    "    \n",
    "    # 1. Smartphone Suitability Score Comparison\n",
    "    plt.subplot(3, 4, 1)\n",
    "    if 'smartphone_suitability_score' in comprehensive_df.columns:\n",
    "        method_scores = comprehensive_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=True)\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(method_scores)))\n",
    "        bars = plt.barh(range(len(method_scores)), method_scores.values, color=colors)\n",
    "        plt.yticks(range(len(method_scores)), method_scores.index)\n",
    "        plt.xlabel('Smartphone Suitability Score')\n",
    "        plt.title('Overall Smartphone Suitability Ranking\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, method_scores.values)):\n",
    "            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{value:.3f}', ha='left', va='center', fontsize=9)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Smartphone Suitability\\nScores Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Smartphone Suitability Ranking\\n(Downscaled)')\n",
    "    \n",
    "    # 2. F1 Recovery Performance\n",
    "    plt.subplot(3, 4, 2)\n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns:\n",
    "        method_f1_recovery = comprehensive_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_f1_recovery)), method_f1_recovery.values, \n",
    "                color=['green' if x >= 75 else 'orange' if x >= 50 else 'red' for x in method_f1_recovery.values])\n",
    "        plt.xticks(range(len(method_f1_recovery)), method_f1_recovery.index, rotation=45, ha='right')\n",
    "        plt.ylabel('F1 Recovery (%)')\n",
    "        plt.title('Detection Performance Recovery\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add horizontal lines for recovery targets\n",
    "        plt.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Min Acceptable')\n",
    "        plt.axhline(y=75, color='orange', linestyle='--', alpha=0.7, label='Good')\n",
    "        plt.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='Excellent')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'F1 Recovery\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Detection Performance Recovery\\n(Downscaled)')\n",
    "    \n",
    "    # 3. Computational Efficiency\n",
    "    plt.subplot(3, 4, 3)\n",
    "    valid_efficiency = comprehensive_df[comprehensive_df['real_time_factor'].notna()] if 'real_time_factor' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_efficiency.empty:\n",
    "        method_efficiency = valid_efficiency.groupby('method_name')['real_time_factor'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_efficiency)), method_efficiency.values,\n",
    "                color=['green' if x >= 1.0 else 'orange' if x >= 0.5 else 'red' for x in method_efficiency.values])\n",
    "        plt.xticks(range(len(method_efficiency)), method_efficiency.index, rotation=45, ha='right')\n",
    "        plt.ylabel('Real-Time Factor')\n",
    "        plt.title('Computational Efficiency\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Real-Time Threshold')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Efficiency\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Computational Efficiency\\n(Downscaled)')\n",
    "    \n",
    "    # 4. Signal Quality Improvement\n",
    "    plt.subplot(3, 4, 4)\n",
    "    valid_quality = comprehensive_df[comprehensive_df['snr_improvement_db'].notna()] if 'snr_improvement_db' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_quality.empty:\n",
    "        method_quality = valid_quality.groupby('method_name')['snr_improvement_db'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_quality)), method_quality.values,\n",
    "                color=['green' if x >= 5 else 'orange' if x >= 0 else 'red' for x in method_quality.values])\n",
    "        plt.xticks(range(len(method_quality)), method_quality.index, rotation=45, ha='right')\n",
    "        plt.ylabel('SNR Improvement (dB)')\n",
    "        plt.title('Signal Quality Enhancement\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.7, label='No Improvement')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Signal Quality\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Signal Quality Enhancement\\n(Downscaled)')\n",
    "    \n",
    "    # 5. Feature Preservation\n",
    "    plt.subplot(3, 4, 5)\n",
    "    valid_preservation = comprehensive_df[comprehensive_df['avg_correlation_recovery'].notna()] if 'avg_correlation_recovery' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_preservation.empty:\n",
    "        method_preservation = valid_preservation.groupby('method_name')['avg_correlation_recovery'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_preservation)), method_preservation.values,\n",
    "                color=['green' if x >= 0.8 else 'orange' if x >= 0.5 else 'red' for x in method_preservation.values])\n",
    "        plt.xticks(range(len(method_preservation)), method_preservation.index, rotation=45, ha='right')\n",
    "        plt.ylabel('Correlation Recovery')\n",
    "        plt.title('Feature Preservation\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature Preservation\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Feature Preservation\\n(Downscaled)')\n",
    "    \n",
    "    # 6. Performance vs Efficiency Scatter Plot\n",
    "    plt.subplot(3, 4, 6)\n",
    "    valid_scatter = comprehensive_df[(comprehensive_df['f1_recovery_pct'].notna()) & \n",
    "                                   (comprehensive_df['real_time_factor'].notna())] if all(col in comprehensive_df.columns for col in ['f1_recovery_pct', 'real_time_factor']) else pd.DataFrame()\n",
    "    if not valid_scatter.empty:\n",
    "        methods = valid_scatter['method_name'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_scatter[valid_scatter['method_name'] == method]\n",
    "            plt.scatter(method_data['real_time_factor'], method_data['f1_recovery_pct'], \n",
    "                       label=method, alpha=0.7, s=60, color=colors[i])\n",
    "        \n",
    "        plt.xlabel('Real-Time Factor')\n",
    "        plt.ylabel('F1 Recovery (%)')\n",
    "        plt.title('Performance vs Efficiency Trade-off\\n(Downscaled)')\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Performance vs Efficiency\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Performance vs Efficiency Trade-off\\n(Downscaled)')\n",
    "    \n",
    "    # 7. Multi-Dimensional Comparison\n",
    "    plt.subplot(3, 4, 7)\n",
    "    radar_metrics = ['f1_recovery_score', 'efficiency_score', 'signal_quality_score', 'feature_preservation_score']\n",
    "    radar_metrics_available = [col for col in radar_metrics if col in comprehensive_df.columns]\n",
    "    radar_labels = ['F1 Recovery', 'Efficiency', 'Signal Quality', 'Feature Preservation'][:len(radar_metrics_available)]\n",
    "    \n",
    "    if radar_metrics_available:\n",
    "        method_radar_scores = comprehensive_df.groupby('method_name')[radar_metrics_available].mean()\n",
    "        \n",
    "        if not method_radar_scores.empty:\n",
    "            method_names = method_radar_scores.index\n",
    "            x_pos = np.arange(len(radar_labels))\n",
    "            width = 0.8 / len(method_names) if len(method_names) > 0 else 0.8\n",
    "            \n",
    "            for i, method in enumerate(method_names):\n",
    "                scores = method_radar_scores.loc[method, radar_metrics_available].values\n",
    "                plt.bar(x_pos + i*width, scores, width, label=method, alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Dimensions')\n",
    "            plt.ylabel('Normalized Scores')\n",
    "            plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "            plt.xticks(x_pos + width*(len(method_names)-1)/2, radar_labels, rotation=45, ha='right')\n",
    "            plt.legend(fontsize=8)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Multi-Dimensional\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Multi-Dimensional\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "    \n",
    "    # 8. Recovery vs Original Performance\n",
    "    plt.subplot(3, 4, 8)\n",
    "    valid_recovery = comprehensive_df[(comprehensive_df['original_f1'].notna()) & \n",
    "                                    (comprehensive_df['f1_score'].notna())] if all(col in comprehensive_df.columns for col in ['original_f1', 'f1_score']) else pd.DataFrame()\n",
    "    if not valid_recovery.empty:\n",
    "        methods = valid_recovery['method_name'].unique()\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_recovery[valid_recovery['method_name'] == method]\n",
    "            plt.scatter(method_data['original_f1'], method_data['f1_score'], \n",
    "                       label=method, alpha=0.7, s=60, color=colors[i])\n",
    "        \n",
    "        # Add diagonal line for reference (no improvement)\n",
    "        min_val = min(valid_recovery['original_f1'].min(), valid_recovery['f1_score'].min())\n",
    "        max_val = max(valid_recovery['original_f1'].max(), valid_recovery['f1_score'].max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='No Improvement')\n",
    "        \n",
    "        plt.xlabel('Original Noisy F1-Score')\n",
    "        plt.ylabel('Denoised F1-Score')\n",
    "        plt.title('Recovery Effectiveness\\n(Downscaled)')\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Recovery Effectiveness\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Recovery Effectiveness\\n(Downscaled)')\n",
    "    \n",
    "    # 9-12: Method Performance Heatmaps (simplified for downscaled data)\n",
    "    heatmap_metrics = [\n",
    "        ('f1_recovery_pct', 'F1 Recovery (%) by Method-Condition\\n(Downscaled)'),\n",
    "        ('real_time_factor', 'Real-Time Factor by Method-Condition\\n(Downscaled)'),\n",
    "        ('snr_improvement_db', 'SNR Improvement (dB) by Method-Condition\\n(Downscaled)'),\n",
    "        ('avg_correlation_recovery', 'Feature Preservation by Method-Condition\\n(Downscaled)')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(heatmap_metrics, 9):\n",
    "        plt.subplot(3, 4, idx)\n",
    "        if metric in comprehensive_df.columns:\n",
    "            valid_data = comprehensive_df[comprehensive_df[metric].notna()]\n",
    "            if not valid_data.empty and len(valid_data['method_name'].unique()) > 1:\n",
    "                try:\n",
    "                    pivot_data = valid_data.pivot_table(index='method_name', columns='condition_name', values=metric, aggfunc='mean')\n",
    "                    if not pivot_data.empty:\n",
    "                        # Adjust figure size for readability\n",
    "                        sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "                                  cbar_kws={'label': metric}, square=False)\n",
    "                        plt.title(title, fontsize=10)\n",
    "                        plt.xlabel('Condition', fontsize=9)\n",
    "                        plt.ylabel('Method', fontsize=9)\n",
    "                        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "                        plt.yticks(rotation=0, fontsize=8)\n",
    "                    else:\n",
    "                        plt.text(0.5, 0.5, f'{title}\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                        plt.title(title, fontsize=10)\n",
    "                except Exception as e:\n",
    "                    plt.text(0.5, 0.5, f'{title}\\nError: {str(e)[:30]}...', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                    plt.title(title, fontsize=10)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, f'{title}\\nInsufficient Data', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title(title, fontsize=10)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'{title}\\nColumn Not Found', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title(title, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comprehensive visualization\n",
    "    viz_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_phase3_comprehensive_analysis.png\")\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ’¾ Comprehensive visualization saved: {viz_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate final summary report  \n",
    "    print(f\"\\nğŸ“‹ FINAL DOWNSCALED PHASE 3 SUMMARY REPORT:\")\n",
    "    print(f\"\\nğŸ¯ Research Objective Achievement:\")\n",
    "    print(f\"   âœ… Evaluated {len(DENOISING_METHODS)} denoising methods\")\n",
    "    print(f\"   âœ… Tested on {len(comprehensive_df['condition_name'].unique())} priority noise conditions\")\n",
    "    print(f\"   âœ… Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\")\n",
    "    print(f\"   âœ… Generated smartphone deployment recommendations\")\n",
    "    print(f\"   âœ… Downscaled approach: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    # Key findings (with safe access to data)\n",
    "    if 'smartphone_suitability_score' in comprehensive_df.columns and not comprehensive_df['smartphone_suitability_score'].isna().all():\n",
    "        best_overall = comprehensive_df.loc[comprehensive_df['smartphone_suitability_score'].idxmax()]\n",
    "        print(f\"   ğŸ¥‡ Best Overall Method: {best_overall['method_name']} (Score: {best_overall['smartphone_suitability_score']:.3f})\")\n",
    "    \n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns and not comprehensive_df['f1_recovery_pct'].isna().all():\n",
    "        best_performance = comprehensive_df.loc[comprehensive_df['f1_recovery_pct'].idxmax()]\n",
    "        print(f\"   ğŸ¯ Best Performance Recovery: {best_performance['method_name']} ({best_performance['f1_recovery_pct']:.1f}% F1 recovery)\")\n",
    "    \n",
    "    if 'real_time_factor' in comprehensive_df.columns:\n",
    "        best_efficiency_df = comprehensive_df[comprehensive_df['real_time_factor'].notna()]\n",
    "        if not best_efficiency_df.empty:\n",
    "            best_efficiency = best_efficiency_df.loc[best_efficiency_df['real_time_factor'].idxmax()]\n",
    "            print(f\"   âš¡ Most Efficient: {best_efficiency['method_name']} ({best_efficiency['real_time_factor']:.2f}x real-time)\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns:\n",
    "        avg_recovery = comprehensive_df['f1_recovery_pct'].mean()\n",
    "        recovery_std = comprehensive_df['f1_recovery_pct'].std()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Downscaled Performance Statistics:\")\n",
    "        print(f\"   ğŸ“ˆ Average F1 Recovery: {avg_recovery:.1f}% (Â±{recovery_std:.1f}%)\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >50% recovery: {len(comprehensive_df[comprehensive_df['f1_recovery_pct'] >= 50])} / {len(comprehensive_df)}\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >75% recovery: {len(comprehensive_df[comprehensive_df['f1_recovery_pct'] >= 75])} / {len(comprehensive_df)}\")\n",
    "        print(f\"   ğŸ“ Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\")\n",
    "    print(f\"\\nğŸ“‹ Research Contributions Achieved:\")\n",
    "    print(f\"   âœ… Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\")\n",
    "    print(f\"   âœ… Downscaled sampling methodology for efficient method comparison\")\n",
    "    print(f\"   âœ… Smartphone deployment feasibility assessment\")\n",
    "    print(f\"   âœ… Evidence-based method recommendations for mobile health applications\")\n",
    "    print(f\"   âœ… Performance-efficiency trade-off quantification\")\n",
    "    print(f\"   âœ… Feature preservation analysis for breathing biomarkers\")\n",
    "    print(f\"   âœ… Proof-of-concept validation with {SAMPLE_SIZE_PER_CONDITION}-file sampling\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Comprehensive visualization skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "    print(f\"   Expected results from:\")\n",
    "    print(f\"     - Cell 5: Denoising application and performance evaluation\")\n",
    "    print(f\"     - Cell 6: Signal quality assessment\")\n",
    "    print(f\"     - Cell 7: Feature preservation analysis\")\n",
    "    print(f\"     - Cell 8: Comprehensive results compilation\")\n",
    "\n",
    "print(f\"\\nğŸ Downscaled Phase 3 notebook execution complete!\")\n",
    "print(f\"Time finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
