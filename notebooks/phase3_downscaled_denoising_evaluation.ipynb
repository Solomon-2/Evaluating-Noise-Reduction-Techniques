{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97226e8",
   "metadata": {},
   "source": [
    "# Phase 3: Downscaled Comprehensive Denoising Method Evaluation\n",
    "\n",
    "## Research Objective\n",
    "Systematically evaluate 3 denoising methods across 4 dimensions to determine optimal approaches for smartphone-based sleep apnea detection under realistic noise conditions using **downscaled sampling for faster execution**.\n",
    "\n",
    "## This Notebook:\n",
    "1. **Downscaled Sampling Focus**: Evaluate **20 randomly sampled files per condition** for rapid prototyping\n",
    "2. **Multi-Method Denoising**: Apply 3 denoising techniques to representative priority conditions\n",
    "3. **Four-Dimensional Evaluation**: Performance recovery, signal quality, computational efficiency, feature preservation\n",
    "4. **Smartphone Suitability Scoring**: Weighted composite metrics for deployment decisions\n",
    "5. **Method Ranking**: Evidence-based recommendations for mobile health applications\n",
    "\n",
    "## Denoising Methods Under Evaluation:\n",
    "- **Spectral Subtraction**: Fast, lightweight, potential musical noise artifacts\n",
    "- **Wiener Filtering**: Balanced statistical approach with moderate complexity\n",
    "- **LogMMSE**: Advanced statistical method with better artifact control\n",
    "\n",
    "## Representative Test Conditions (5dB SNR - Worst Case):\n",
    "- **patient_01_wav_5db_vacuum_cleaner**: Mechanical high-frequency noise\n",
    "- **patient_01_wav_5db_cat**: Animal organic sounds\n",
    "- **patient_01_wav_5db_door_wood_creaks**: Structural low-frequency noise\n",
    "- **patient_01_wav_5db_crying_baby**: Human vocal interference\n",
    "- **patient_01_wav_5db_coughing**: Respiratory interference (most challenging)\n",
    "\n",
    "## Downscaled Optimization:\n",
    "- **Sample Size**: 20 randomly selected files per condition (vs 1,168 full dataset)\n",
    "- **Total Evaluations**: 5 conditions Ã— 3 methods = 15 evaluations (~30 minutes)\n",
    "- **Strategy**: Rapid prototyping and method comparison for proof-of-concept\n",
    "- **Scientific Validity**: Random sampling maintains representativeness for comparative analysis\n",
    "\n",
    "## Expected Outcomes:\n",
    "- Recovery targets: 50% (minimum), 75% (good), 90% (excellent), 100% (perfect)\n",
    "- Computational trade-offs: Traditional signal processing methods comparison\n",
    "- Feature preservation analysis: Which methods maintain breathing biomarkers\n",
    "- Smartphone deployment recommendations: Optimal method per use case\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1910909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\n",
      "âœ… Configuration loaded:\n",
      "   ğŸ“ Base data directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\n",
      "   ğŸ¤– Model path: ../models/sleep_apnea_model.pkl\n",
      "   ğŸ”Š Denoising methods: 4 traditional methods\n",
      "   ğŸ“Š Representative conditions: 5\n",
      "   ğŸ“Š Output directories created\n",
      "   âš¡ DOWNSCALED OPTIMIZATION: 20 files per condition\n",
      "   ğŸ¯ Total evaluations: 5 Ã— 4 = 20\n",
      "   ğŸš€ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\n",
      "   ğŸ² Random seed set: 42 (reproducible sampling)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "print(\"=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_score, recall_score, accuracy_score\n",
    ")\n",
    "import joblib\n",
    "import random\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration paths\n",
    "BASE_DATA_DIR = \"F:/Solo All In One Docs/Scidb Sleep Data/processed\"\n",
    "MODEL_PATH = \"../models/sleep_apnea_model.pkl\"\n",
    "PHASE2_RESULTS_PATH = os.path.join(BASE_DATA_DIR, \"noise_evaluation_results.csv\")\n",
    "PHASE3_CONFIG_PATH = os.path.join(BASE_DATA_DIR, \"phase3_preparation_config.json\")\n",
    "CLEAN_BASELINE_PATH = os.path.join(BASE_DATA_DIR, \"clean_audio_baseline_results.json\")\n",
    "\n",
    "# Denoising methods configuration (3 traditional methods for speed)\n",
    "DENOISING_METHODS = {\n",
    "    'spectral_subtraction': {\n",
    "        'script': '../src/spec_subtraction_same_file.py',\n",
    "        'name': 'Spectral Subtraction',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'moderate'\n",
    "    },\n",
    "    'wiener_filtering': {\n",
    "        'script': '../src/wiener_filtering.py',\n",
    "        'name': 'Wiener Filtering',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'logmmse': {\n",
    "        'script': '../src/log_mmse.py',\n",
    "        'name': 'LogMMSE',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'moderate',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'deepfilternet': {\n",
    "    'script': '../src/denoise_with_deepfilternet.py',\n",
    "    'name': 'DeepFilterNet',\n",
    "    'category': 'deep_learning',\n",
    "    'expected_efficiency': 'low',\n",
    "    'expected_quality': 'excellent'\n",
    "}\n",
    "}\n",
    "\n",
    "# Representative conditions from Phase 2 (5dB worst-case analysis)\n",
    "REPRESENTATIVE_CONDITIONS = [\n",
    "    'patient_01_wav_5db_vacuum_cleaner',    # Mechanical high-frequency noise\n",
    "    'patient_01_wav_5db_cat',               # Animal organic sounds  \n",
    "    'patient_01_wav_5db_door_wood_creaks',  # Structural low-frequency noise\n",
    "    'patient_01_wav_5db_crying_baby',       # Human vocal interference\n",
    "    'patient_01_wav_5db_coughing'           # Respiratory interference\n",
    "]\n",
    "\n",
    "# Audio processing settings (consistent with Phase 1 & 2)\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_DURATION = 30.0\n",
    "\n",
    "# DOWNSCALED SAMPLING CONFIGURATION\n",
    "SAMPLE_SIZE_PER_CONDITION = 20  # Fixed 20 files per condition\n",
    "RANDOM_SEED = 42  # For reproducible sampling\n",
    "\n",
    "# Create output directories\n",
    "DENOISED_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"denoised_audio_downscaled\")\n",
    "RESULTS_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"phase3_downscaled_results\")\n",
    "os.makedirs(DENOISED_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Configuration loaded:\")\n",
    "print(f\"   ğŸ“ Base data directory: {BASE_DATA_DIR}\")\n",
    "print(f\"   ğŸ¤– Model path: {MODEL_PATH}\")\n",
    "print(f\"   ğŸ”Š Denoising methods: {len(DENOISING_METHODS)} traditional methods\")\n",
    "print(f\"   ğŸ“Š Representative conditions: {len(REPRESENTATIVE_CONDITIONS)}\")\n",
    "print(f\"   ğŸ“Š Output directories created\")\n",
    "print(f\"   âš¡ DOWNSCALED OPTIMIZATION: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   ğŸ¯ Total evaluations: {len(REPRESENTATIVE_CONDITIONS)} Ã— {len(DENOISING_METHODS)} = {len(REPRESENTATIVE_CONDITIONS) * len(DENOISING_METHODS)}\")\n",
    "print(f\"   ğŸš€ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\")\n",
    "\n",
    "# Set random seed for reproducible sampling\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ğŸ² Random seed set: {RANDOM_SEED} (reproducible sampling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd1c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature extraction function loaded (27 features matching training pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction function (same 27 features as training pipeline)\n",
    "def extract_comprehensive_features(audio_frame, sample_rate):\n",
    "    \"\"\"Extract the same 27 features used in training pipeline\"\"\"\n",
    "    try:\n",
    "        if len(audio_frame) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Basic acoustic features\n",
    "        rms = float(librosa.feature.rms(y=audio_frame).mean())\n",
    "        zcr = float(librosa.feature.zero_crossing_rate(y=audio_frame).mean())\n",
    "        centroid = float(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n",
    "        bandwidth = float(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n",
    "        rolloff = float(librosa.feature.spectral_rolloff(y=audio_frame, sr=sample_rate).mean())\n",
    "        \n",
    "        # MFCCs (first 8 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=8)\n",
    "        mfcc_means = mfccs.mean(axis=1)\n",
    "        mfcc_stds = mfccs.std(axis=1)\n",
    "        \n",
    "        # Temporal features for breathing patterns (5-second windows)\n",
    "        window_size = int(5 * sample_rate)  # 5 seconds\n",
    "        num_windows = len(audio_frame) // window_size\n",
    "        \n",
    "        if num_windows >= 2:\n",
    "            rms_windows = []\n",
    "            zcr_windows = []\n",
    "            \n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window = audio_frame[start_idx:end_idx]\n",
    "                \n",
    "                rms_windows.append(librosa.feature.rms(y=window).mean())\n",
    "                zcr_windows.append(librosa.feature.zero_crossing_rate(y=window).mean())\n",
    "            \n",
    "            rms_variability = float(np.std(rms_windows))\n",
    "            zcr_variability = float(np.std(zcr_windows))\n",
    "            breathing_regularity = float(1.0 / (1.0 + rms_variability))  # Higher = more regular\n",
    "        else:\n",
    "            rms_variability = 0.0\n",
    "            zcr_variability = 0.0\n",
    "            breathing_regularity = 0.5\n",
    "        \n",
    "        # Silence detection\n",
    "        silence_threshold = np.percentile(np.abs(audio_frame), 20)  # Bottom 20% as silence\n",
    "        silence_mask = np.abs(audio_frame) < silence_threshold\n",
    "        silence_ratio = float(np.mean(silence_mask))\n",
    "        \n",
    "        # Breathing pause detection (continuous silence periods)\n",
    "        silence_changes = np.diff(silence_mask.astype(int))\n",
    "        pause_starts = np.where(silence_changes == 1)[0]\n",
    "        pause_ends = np.where(silence_changes == -1)[0]\n",
    "        \n",
    "        if len(pause_starts) > 0 and len(pause_ends) > 0:\n",
    "            if len(pause_ends) < len(pause_starts):\n",
    "                pause_ends = np.append(pause_ends, len(audio_frame))\n",
    "            pause_durations = (pause_ends[:len(pause_starts)] - pause_starts) / sample_rate\n",
    "            avg_pause_duration = float(np.mean(pause_durations))\n",
    "            max_pause_duration = float(np.max(pause_durations))\n",
    "        else:\n",
    "            avg_pause_duration = 0.0\n",
    "            max_pause_duration = 0.0\n",
    "        \n",
    "        # Combine all features (same structure as training)\n",
    "        features = {\n",
    "            'clean_rms': rms,\n",
    "            'clean_zcr': zcr,\n",
    "            'clean_centroid': centroid,\n",
    "            'clean_bandwidth': bandwidth,\n",
    "            'clean_rolloff': rolloff,\n",
    "            'clean_rms_variability': rms_variability,\n",
    "            'clean_zcr_variability': zcr_variability,\n",
    "            'clean_breathing_regularity': breathing_regularity,\n",
    "            'clean_silence_ratio': silence_ratio,\n",
    "            'clean_avg_pause_duration': avg_pause_duration,\n",
    "            'clean_max_pause_duration': max_pause_duration\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs\n",
    "        for i, (mean_val, std_val) in enumerate(zip(mfcc_means, mfcc_stds), 1):\n",
    "            features[f'clean_mfcc_{i}_mean'] = float(mean_val)\n",
    "            features[f'clean_mfcc_{i}_std'] = float(std_val)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Feature extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Feature extraction function loaded (27 features matching training pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b6063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\n",
      "======================================================================\n",
      "âœ… Phase 2 results loaded: 5 noise conditions evaluated\n",
      "\n",
      "ğŸ“ˆ Phase 2 Performance Summary:\n",
      "   F1-Score Range: 0.000 - 0.218\n",
      "   Average F1-Score: 0.051 (Â±0.095)\n",
      "   Average Degradation: 93.3% (Â±12.5%)\n",
      "\n",
      "âœ… Clean baseline loaded: F1=0.758\n",
      "\n",
      "âœ… Downscaled sampling functions loaded\n",
      "   ğŸ“ Sample size: 20 files per condition\n",
      "   ğŸ² Random seed: 42 (reproducible results)\n",
      "   ğŸ“ Temporary directories will be created for each method-condition combination\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Phase 2 Results and Create Downscaled Sampling Function\n",
    "print(\"ğŸ“Š LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load Phase 2 evaluation results\n",
    "try:\n",
    "    phase2_results = pd.read_csv(PHASE2_RESULTS_PATH)\n",
    "    print(f\"âœ… Phase 2 results loaded: {len(phase2_results)} noise conditions evaluated\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\nğŸ“ˆ Phase 2 Performance Summary:\")\n",
    "    print(f\"   F1-Score Range: {phase2_results['f1_score'].min():.3f} - {phase2_results['f1_score'].max():.3f}\")\n",
    "    print(f\"   Average F1-Score: {phase2_results['f1_score'].mean():.3f} (Â±{phase2_results['f1_score'].std():.3f})\")\n",
    "    print(f\"   Average Degradation: {phase2_results['f1_degradation_pct'].mean():.1f}% (Â±{phase2_results['f1_degradation_pct'].std():.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not load Phase 2 results: {e}\")\n",
    "    print(f\"   Will proceed with fallback configuration\")\n",
    "    phase2_results = None\n",
    "\n",
    "# Load clean baseline for reference\n",
    "try:\n",
    "    with open(CLEAN_BASELINE_PATH, 'r') as f:\n",
    "        clean_baseline = json.load(f)\n",
    "    print(f\"\\nâœ… Clean baseline loaded: F1={clean_baseline['clean_f1_score']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load clean baseline: {e}\")\n",
    "    clean_baseline = {'clean_f1_score': 0.672}  # Fallback value from research plan\n",
    "\n",
    "# Downscaled sampling function for 20 files per condition\n",
    "def sample_files_downscaled(input_dir, sample_size=20):\n",
    "    \"\"\"\n",
    "    Randomly sample a fixed number of files from condition directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing audio files\n",
    "        sample_size: Number of files to sample (default 20)\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled filenames (sorted for reproducibility)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(input_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        if len(all_files) == 0:\n",
    "            print(f\"      âŒ No WAV files found in {input_dir}\")\n",
    "            return []\n",
    "        \n",
    "        if len(all_files) <= sample_size:\n",
    "            print(f\"      ğŸ“Š Using all {len(all_files)} files (less than sample size)\")\n",
    "            return sorted(all_files)\n",
    "        \n",
    "        # Random sampling with fixed seed for reproducibility\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "        \n",
    "        percentage_actual = (sample_size / len(all_files)) * 100\n",
    "        print(f\"      ğŸ“Š Sampled {sample_size} files ({percentage_actual:.1f}%) from {len(all_files)} total files\")\n",
    "        \n",
    "        return sorted(sampled_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Sampling failed for {input_dir}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_temp_sample_directory(source_dir, sampled_files, temp_suffix):\n",
    "    \"\"\"\n",
    "    Create temporary directory with only sampled files for denoising\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Source directory containing all files\n",
    "        sampled_files: List of filenames to copy\n",
    "        temp_suffix: Unique suffix for temp directory\n",
    "    \n",
    "    Returns:\n",
    "        Path to temporary directory\n",
    "    \"\"\"\n",
    "    temp_dir = f\"{source_dir}_temp_downscaled_{temp_suffix}\"\n",
    "    \n",
    "    # Clean up any existing temp directory\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy sampled files to temp directory\n",
    "    copied_count = 0\n",
    "    for filename in sampled_files:\n",
    "        src = os.path.join(source_dir, filename)\n",
    "        dst = os.path.join(temp_dir, filename)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied_count += 1\n",
    "    \n",
    "    print(f\"      ğŸ“ Created temp sample directory: {copied_count} files copied\")\n",
    "    return temp_dir\n",
    "\n",
    "print(f\"\\nâœ… Downscaled sampling functions loaded\")\n",
    "print(f\"   ğŸ“ Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   ğŸ² Random seed: {RANDOM_SEED} (reproducible results)\")\n",
    "print(f\"   ğŸ“ Temporary directories will be created for each method-condition combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e1f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\n",
      "ğŸ“‰ Representative Conditions (5dB worst-case per noise category):\n",
      "   patient_01_wav_5db_vacuum_cleaner: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_cat: F1=0.036 (-95.2%)\n",
      "   patient_01_wav_5db_door_wood_creaks: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_crying_baby: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_coughing: F1=0.218 (-71.2%)\n",
      "\n",
      "ğŸ“ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\n",
      "   âœ… patient_01_wav_5db_vacuum_cleaner: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_cat: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_door_wood_creaks: 1168 files available, will sample 20\n",
      "   âœ… patient_01_wav_5db_crying_baby: 1168 files available, will sample 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… patient_01_wav_5db_coughing: 1168 files available, will sample 20\n",
      "\n",
      "ğŸ’¾ Verified priority conditions saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_priority_conditions.csv\n",
      "\n",
      "âœ… Priority condition verification complete:\n",
      "   ğŸ“Š Verified conditions: 5\n",
      "   ğŸ¯ Sample size per condition: 20 files\n",
      "   ğŸ“ˆ Total files to process: 400\n",
      "   â±ï¸  Estimated execution time: ~30 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Select Priority Conditions and Prepare for Downscaled Evaluation\n",
    "print(\"ğŸ¯ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Select priority conditions for Phase 3 evaluation\n",
    "priority_conditions = None\n",
    "\n",
    "if phase2_results is not None:\n",
    "    # Strategy: Select representative conditions across noise types at 5dB (worst-case)\n",
    "    representative_conditions = []\n",
    "    \n",
    "    # Filter for 5dB conditions and get worst-performing per noise category\n",
    "    conditions_5db = phase2_results[phase2_results['condition_name'].str.contains('_5db_')]\n",
    "    \n",
    "    if not conditions_5db.empty:\n",
    "        for condition_name in REPRESENTATIVE_CONDITIONS:\n",
    "            condition_match = conditions_5db[conditions_5db['condition_name'] == condition_name]\n",
    "            if not condition_match.empty:\n",
    "                representative_conditions.append(condition_match.iloc[0])\n",
    "        \n",
    "        if representative_conditions:\n",
    "            priority_conditions = pd.DataFrame(representative_conditions)\n",
    "            print(f\"\\nğŸ¯ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\")\n",
    "            print(f\"ğŸ“‰ Representative Conditions (5dB worst-case per noise category):\")\n",
    "            for idx, row in priority_conditions.iterrows():\n",
    "                print(f\"   {row['condition_name']}: F1={row['f1_score']:.3f} (-{row['f1_degradation_pct']:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  No matching representative conditions found in Phase 2 results\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  No 5dB conditions found in Phase 2 results\")\n",
    "\n",
    "if priority_conditions is None:\n",
    "    print(f\"âš ï¸  Using fallback representative conditions from configuration\")\n",
    "    # Create fallback priority conditions DataFrame\n",
    "    priority_conditions = pd.DataFrame({\n",
    "        'condition_name': REPRESENTATIVE_CONDITIONS,\n",
    "        'f1_score': [0.400] * len(REPRESENTATIVE_CONDITIONS),  # Estimated based on 5dB degradation\n",
    "        'f1_degradation_pct': [47.2] * len(REPRESENTATIVE_CONDITIONS)  # Estimated degradation\n",
    "    })\n",
    "    print(f\"   Using {len(REPRESENTATIVE_CONDITIONS)} representative conditions as fallback\")\n",
    "\n",
    "# Verify condition directories exist and count available files\n",
    "print(f\"\\nğŸ“ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\")\n",
    "verified_conditions = []\n",
    "\n",
    "for _, condition_row in priority_conditions.iterrows():\n",
    "    condition_name = condition_row['condition_name']\n",
    "    condition_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "    \n",
    "    if os.path.exists(condition_dir):\n",
    "        wav_files = [f for f in os.listdir(condition_dir) if f.endswith('.wav')]\n",
    "        file_count = len(wav_files)\n",
    "        \n",
    "        if file_count > 0:\n",
    "            verified_conditions.append(condition_row)\n",
    "            sample_count = min(SAMPLE_SIZE_PER_CONDITION, file_count)\n",
    "            print(f\"   âœ… {condition_name}: {file_count} files available, will sample {sample_count}\")\n",
    "        else:\n",
    "            print(f\"   âŒ {condition_name}: No WAV files found\")\n",
    "    else:\n",
    "        print(f\"   âŒ {condition_name}: Directory not found at {condition_dir}\")\n",
    "\n",
    "if verified_conditions:\n",
    "    priority_conditions = pd.DataFrame(verified_conditions)\n",
    "    \n",
    "    # Save priority conditions for reference\n",
    "    priority_conditions_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_priority_conditions.csv\")\n",
    "    priority_conditions.to_csv(priority_conditions_path, index=False)\n",
    "    print(f\"\\nğŸ’¾ Verified priority conditions saved: {priority_conditions_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Priority condition verification complete:\")\n",
    "    print(f\"   ğŸ“Š Verified conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   ğŸ¯ Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"   ğŸ“ˆ Total files to process: {len(priority_conditions) * SAMPLE_SIZE_PER_CONDITION * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   â±ï¸  Estimated execution time: ~30 minutes\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ No verified conditions available for evaluation\")\n",
    "    print(f\"   Please check that Phase 2 noise injection has been completed\")\n",
    "    priority_conditions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c008f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded (direct): ../models/sleep_apnea_model.pkl\n",
      "ğŸ“Š Model type: RandomForestClassifier\n",
      "âœ… Audio metadata loaded: 10972 records (whitespace cleaned)\n",
      "\n",
      "âœ… Evaluation framework ready for downscaled processing\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model and Prepare Evaluation Framework\n",
    "print(\"ğŸ¤– LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    model_data = joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if isinstance(model_data, dict):\n",
    "        model = model_data['model']\n",
    "        feature_columns = model_data.get('feature_columns', None)\n",
    "        print(f\"âœ… Model loaded from: {MODEL_PATH}\")\n",
    "        print(f\"ğŸ“Š Model type: {type(model).__name__}\")\n",
    "        if feature_columns:\n",
    "            print(f\"ğŸ¯ Expected features: {len(feature_columns)}\")\n",
    "    else:\n",
    "        # Fallback if model is saved directly\n",
    "        model = model_data\n",
    "        feature_columns = None\n",
    "        print(f\"âœ… Model loaded (direct): {MODEL_PATH}\")\n",
    "        print(f\"ğŸ“Š Model type: {type(model).__name__}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load model: {e}\")\n",
    "    model = None\n",
    "    feature_columns = None\n",
    "\n",
    "# Performance evaluation function optimized for downscaled processing\n",
    "def evaluate_denoised_audio_downscaled(denoised_audio_dir, condition_name, method_name, model, feature_columns, audio_metadata):\n",
    "    \"\"\"Evaluate model performance on denoised audio with downscaled sampling\"\"\"\n",
    "    \n",
    "    print(f\"   ğŸ“Š Evaluating: {method_name} on {condition_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get WAV files in the denoised directory\n",
    "        if not os.path.exists(denoised_audio_dir):\n",
    "            print(f\"      âŒ Directory not found: {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        wav_files = [f for f in os.listdir(denoised_audio_dir) if f.lower().endswith('.wav')]\n",
    "        if not wav_files:\n",
    "            print(f\"      âŒ No WAV files found in {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"      ğŸµ Processing {len(wav_files)} denoised audio files...\")\n",
    "        \n",
    "        # Apply whitespace fix to metadata if available\n",
    "        if audio_metadata is not None and 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "        \n",
    "        # Extract features and get labels with progress monitoring\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        mismatch_count = 0\n",
    "        \n",
    "        for i, wav_file in enumerate(wav_files):\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                wav_path = os.path.join(denoised_audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features is None:\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get corresponding label from metadata\n",
    "                original_filename = wav_file.replace('mixed_', '').replace('denoised_', '').strip()\n",
    "                \n",
    "                if audio_metadata is not None:\n",
    "                    # Find matching metadata record\n",
    "                    metadata_match = audio_metadata[audio_metadata['wav_file'] == original_filename]\n",
    "                    if not metadata_match.empty:\n",
    "                        label = metadata_match.iloc[0]['apnea_label']\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(label)\n",
    "                        processed_count += 1\n",
    "                    else:\n",
    "                        mismatch_count += 1\n",
    "                        if mismatch_count <= 2:  # Show first 2 mismatches\n",
    "                            print(f\"      âš ï¸  No metadata match for: '{original_filename}'\")\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                if failed_count <= 2:  # Show first 2 errors\n",
    "                    print(f\"      âš ï¸  Error processing {wav_file}: {e}\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"      ğŸ“Š Processed: {processed_count}, Failed: {failed_count}, Mismatches: {mismatch_count}\")\n",
    "        \n",
    "        if processed_count == 0:\n",
    "            print(f\"      âŒ No files processed successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to DataFrame and make predictions\n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        labels = np.array(labels_list)\n",
    "        \n",
    "        # Ensure feature order matches training\n",
    "        if feature_columns:\n",
    "            features_df = features_df.reindex(columns=feature_columns, fill_value=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features_df)\n",
    "        prediction_probas = model.predict_proba(features_df)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)  # Sensitivity\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        # Confusion matrix for specificity\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'num_samples': processed_count,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall_sensitivity': recall,\n",
    "            'specificity': specificity,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn),\n",
    "            'sample_size_note': 'downscaled_20_files'\n",
    "        }\n",
    "        \n",
    "        print(f\"      âœ… {method_name}: F1={f1:.3f}, Sens={recall:.3f}, Spec={specificity:.3f} [n={processed_count}]\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Computational efficiency measurement function for downscaled processing\n",
    "def measure_denoising_efficiency_downscaled(input_dir, output_dir, method_script, method_name):\n",
    "    \"\"\"Measure computational efficiency of denoising method on downscaled sample\"\"\"\n",
    "    \n",
    "    print(f\"   â±ï¸  Measuring efficiency for {method_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get input file count\n",
    "        input_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.wav')]\n",
    "        print(f\"      ğŸ“ Input files to process: {len(input_files)}\")\n",
    "        \n",
    "        # Get system resources before\n",
    "        process = psutil.Process()\n",
    "        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Measure processing time\n",
    "        start_time = time.time()\n",
    "        print(f\"      ğŸš€ Starting denoising at {time.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # Run denoising method\n",
    "        cmd = [sys.executable, method_script, '--input', input_dir, '--output', output_dir]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # 5 minute timeout\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"      ğŸ Processing completed in {processing_time:.1f}s\")\n",
    "        \n",
    "        # Get system resources after\n",
    "        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        if os.path.exists(output_dir):\n",
    "            output_files = [f for f in os.listdir(output_dir) if f.lower().endswith('.wav')]\n",
    "            num_files_processed = len(output_files)\n",
    "            \n",
    "            # Estimate total audio duration (assuming 30-second files)\n",
    "            total_audio_duration = num_files_processed * 30.0  # seconds\n",
    "            real_time_factor = total_audio_duration / processing_time if processing_time > 0 else 0\n",
    "            \n",
    "            efficiency_metrics = {\n",
    "                'method_name': method_name,\n",
    "                'processing_time_sec': processing_time,\n",
    "                'files_processed': num_files_processed,\n",
    "                'total_audio_duration_sec': total_audio_duration,\n",
    "                'real_time_factor': real_time_factor,\n",
    "                'memory_usage_mb': memory_after - memory_before,\n",
    "                'peak_memory_mb': memory_after,\n",
    "                'processing_speed_files_per_sec': num_files_processed / processing_time if processing_time > 0 else 0,\n",
    "                'success': result.returncode == 0,\n",
    "                'sample_size_note': 'downscaled_20_files'\n",
    "            }\n",
    "            \n",
    "            print(f\"      âš¡ {method_name}: {processing_time:.1f}s, {real_time_factor:.2f}x RT, {num_files_processed} files\")\n",
    "            return efficiency_metrics\n",
    "            \n",
    "        else:\n",
    "            print(f\"      âŒ {method_name}: Output directory not created\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"      âŒ {method_name}: Processing timeout (>5 minutes)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Efficiency measurement failed for {method_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load audio metadata from Phase 1 for label matching\n",
    "try:\n",
    "    metadata_path = os.path.join(BASE_DATA_DIR, \"audio_metadata.csv\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        audio_metadata = pd.read_csv(metadata_path)\n",
    "        # Apply whitespace stripping immediately upon loading\n",
    "        if 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "            print(f\"âœ… Audio metadata loaded: {len(audio_metadata)} records (whitespace cleaned)\")\n",
    "        else:\n",
    "            print(f\"âœ… Audio metadata loaded: {len(audio_metadata)} records\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Audio metadata not found at {metadata_path}\")\n",
    "        audio_metadata = None\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load audio metadata: {e}\")\n",
    "    audio_metadata = None\n",
    "\n",
    "print(f\"\\nâœ… Evaluation framework ready for downscaled processing\")\n",
    "if model is None:\n",
    "    print(f\"âš ï¸  Model loading failed - evaluation will be limited\")\n",
    "if audio_metadata is None:\n",
    "    print(f\"âš ï¸  Audio metadata missing - label matching may fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04fb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Š APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\n",
      "Time started: 2025-07-30 22:35:27\n",
      "================================================================================\n",
      "ğŸš€ DOWNSCALED PROCESSING CONFIGURATION:\n",
      "   ğŸ¯ Conditions: 5\n",
      "   ğŸ”§ Methods: 4\n",
      "   ğŸ“Š Sample size: 20 files per condition\n",
      "   ğŸ“ˆ Total evaluations: 20\n",
      "   â±ï¸  Expected time: ~30 minutes\n",
      "\n",
      "ğŸ“ Condition 1/5: patient_01_wav_5db_vacuum_cleaner\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "\n",
      "Time started: 2025-07-30 22:35:27\n",
      "================================================================================\n",
      "ğŸš€ DOWNSCALED PROCESSING CONFIGURATION:\n",
      "   ğŸ¯ Conditions: 5\n",
      "   ğŸ”§ Methods: 4\n",
      "   ğŸ“Š Sample size: 20 files per condition\n",
      "   ğŸ“ˆ Total evaluations: 20\n",
      "   â±ï¸  Expected time: ~30 minutes\n",
      "\n",
      "ğŸ“ Condition 1/5: patient_01_wav_5db_vacuum_cleaner\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:36:08\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:36:08\n",
      "      ğŸ Processing completed in 56.1s\n",
      "      âš¡ DeepFilterNet: 56.1s, 10.69x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 56.1s\n",
      "      âš¡ DeepFilterNet: 56.1s, 10.69x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_vacuum_cleaner\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.250, Sens=0.222, Spec=0.545 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.250, Recovery=33.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ“ Condition 2/5: patient_01_wav_5db_cat\n",
      "   ğŸ“‰ Original performance: F1=0.036 (-95.2%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.250, Sens=0.222, Spec=0.545 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.250, Recovery=33.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ“ Condition 2/5: patient_01_wav_5db_cat\n",
      "   ğŸ“‰ Original performance: F1=0.036 (-95.2%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:37:50\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:37:50\n",
      "      ğŸ Processing completed in 46.2s\n",
      "      âš¡ DeepFilterNet: 46.2s, 12.99x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 46.2s\n",
      "      âš¡ DeepFilterNet: 46.2s, 12.99x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_cat\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.250, Sens=0.222, Spec=0.545 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.250, Recovery=29.6% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 2/5, ETA: 5.0 minutes\n",
      "\n",
      "ğŸ“ Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.250, Sens=0.222, Spec=0.545 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.250, Recovery=29.6% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 2/5, ETA: 5.0 minutes\n",
      "\n",
      "ğŸ“ Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:39:24\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:39:24\n",
      "      ğŸ Processing completed in 42.9s\n",
      "      âš¡ DeepFilterNet: 42.9s, 13.99x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 42.9s\n",
      "      âš¡ DeepFilterNet: 42.9s, 13.99x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_door_wood_creaks\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.000, Sens=0.000, Spec=0.750 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 3/5, ETA: 3.2 minutes\n",
      "\n",
      "ğŸ“ Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.000, Sens=0.000, Spec=0.750 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 3/5, ETA: 3.2 minutes\n",
      "\n",
      "ğŸ“ Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   ğŸ“‰ Original performance: F1=0.000 (-100.0%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:40:50\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:40:50\n",
      "      ğŸ Processing completed in 67.5s\n",
      "      âš¡ DeepFilterNet: 67.5s, 8.88x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 67.5s\n",
      "      âš¡ DeepFilterNet: 67.5s, 8.88x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_crying_baby\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.222, Sens=0.125, Spec=1.000 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.222, Recovery=29.3% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 4/5, ETA: 1.7 minutes\n",
      "\n",
      "ğŸ“ Condition 5/5: patient_01_wav_5db_coughing\n",
      "   ğŸ“‰ Original performance: F1=0.218 (-71.2%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.222, Sens=0.125, Spec=1.000 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.222, Recovery=29.3% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 4/5, ETA: 1.7 minutes\n",
      "\n",
      "ğŸ“ Condition 5/5: patient_01_wav_5db_coughing\n",
      "   ğŸ“‰ Original performance: F1=0.218 (-71.2%)\n",
      "   ğŸ”§ Method 1/4: Spectral Subtraction\n",
      "   ğŸ”§ Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Spectral Subtraction: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      âœ… Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 2/4: Wiener Filtering\n",
      "   ğŸ”§ Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… Wiener Filtering: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 3/4: LogMMSE\n",
      "   ğŸ”§ Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      âœ… LogMMSE: Already completed (20 files)\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      âœ… LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ğŸ”§ Method 4/4: DeepFilterNet\n",
      "   ğŸ”§ Applying DeepFilterNet to patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Sampled 20 files (1.7%) from 1168 total files\n",
      "      ğŸ“ Processing 20 sampled files (from 1168 total)\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:42:47\n",
      "      ğŸ“ Created temp sample directory: 20 files copied\n",
      "      ğŸ”„ DeepFilterNet: Incomplete (0/20) - reprocessing\n",
      "   â±ï¸  Measuring efficiency for DeepFilterNet\n",
      "      ğŸ“ Input files to process: 20\n",
      "      ğŸš€ Starting denoising at 22:42:47\n",
      "      ğŸ Processing completed in 68.9s\n",
      "      âš¡ DeepFilterNet: 68.9s, 8.71x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ Processing completed in 68.9s\n",
      "      âš¡ DeepFilterNet: 68.9s, 8.71x RT, 20 files\n",
      "      ğŸ§¹ Cleaned up temp directory\n",
      "   ğŸ“Š Evaluating: DeepFilterNet on patient_01_wav_5db_coughing\n",
      "      ğŸµ Processing 20 denoised audio files...\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.348, Sens=0.667, Spec=0.071 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.348, Recovery=24.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ’¾ Saving downscaled processing results...\n",
      "ğŸ’¾ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "ğŸ’¾ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "â±ï¸  Total time: 522.6 seconds (8.7 minutes)\n",
      "ğŸ“Š Performance evaluations: 20\n",
      "âš¡ Efficiency measurements: 20\n",
      "ğŸ“ Sample size per condition: 20 files\n",
      "ğŸ¯ Total combinations processed: 5 conditions Ã— 4 methods\n",
      "\n",
      "ğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. DeepFilterNet: 23.2% average recovery\n",
      "   2. Spectral Subtraction: 14.6% average recovery\n",
      "   3. LogMMSE: -9.1% average recovery\n",
      "   4. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 22:44:09\n",
      "      ğŸ“Š Processed: 20, Failed: 0, Mismatches: 0\n",
      "      âœ… DeepFilterNet: F1=0.348, Sens=0.667, Spec=0.071 [n=20]\n",
      "      âœ… DeepFilterNet: F1=0.348, Recovery=24.0% [Sample: 20/1168]\n",
      "   âœ… Condition completed: 4 methods successful\n",
      "   â±ï¸  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "ğŸ’¾ Saving downscaled processing results...\n",
      "ğŸ’¾ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "ğŸ’¾ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "â±ï¸  Total time: 522.6 seconds (8.7 minutes)\n",
      "ğŸ“Š Performance evaluations: 20\n",
      "âš¡ Efficiency measurements: 20\n",
      "ğŸ“ Sample size per condition: 20 files\n",
      "ğŸ¯ Total combinations processed: 5 conditions Ã— 4 methods\n",
      "\n",
      "ğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. DeepFilterNet: 23.2% average recovery\n",
      "   2. Spectral Subtraction: 14.6% average recovery\n",
      "   3. LogMMSE: -9.1% average recovery\n",
      "   4. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 22:44:09\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Apply Denoising Methods with Downscaled Processing\n",
    "print(\"ğŸ”Š APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\")\n",
    "print(f\"Time started: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def apply_and_evaluate_single_method_downscaled(condition_name, condition_row, method_key, method_config, \n",
    "                                              model, feature_columns, audio_metadata, clean_baseline):\n",
    "    \"\"\"Apply a single denoising method and evaluate performance with downscaled sampling\"\"\"\n",
    "    \n",
    "    method_name = method_config['name']\n",
    "    method_script = method_config['script']\n",
    "    \n",
    "    print(f\"   ğŸ”§ Applying {method_name} to {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Find corresponding noisy audio directory\n",
    "        noisy_audio_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "        \n",
    "        if not os.path.exists(noisy_audio_dir):\n",
    "            print(f\"      âŒ Noisy audio directory not found: {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Apply downscaled sampling (20 files)\n",
    "        sampled_files = sample_files_downscaled(noisy_audio_dir, SAMPLE_SIZE_PER_CONDITION)\n",
    "        all_files_count = len([f for f in os.listdir(noisy_audio_dir) if f.lower().endswith('.wav')])\n",
    "        \n",
    "        if not sampled_files:\n",
    "            print(f\"      âŒ No files to sample from {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"      ğŸ“ Processing {len(sampled_files)} sampled files (from {all_files_count} total)\")\n",
    "        \n",
    "        # Create temporary directory with only sampled files\n",
    "        temp_input_dir = create_temp_sample_directory(\n",
    "            source_dir=noisy_audio_dir,\n",
    "            sampled_files=sampled_files,\n",
    "            temp_suffix=f\"{method_key}_{int(time.time())}\"\n",
    "        )\n",
    "        \n",
    "        # Create output directory for this method-condition combination\n",
    "        denoised_output_path = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "        os.makedirs(denoised_output_path, exist_ok=True)\n",
    "        \n",
    "        # Check if denoising already completed (based on sampled files)\n",
    "        efficiency_metrics = None\n",
    "        existing_files = []\n",
    "        \n",
    "        if os.path.exists(denoised_output_path):\n",
    "            existing_files = [f for f in os.listdir(denoised_output_path) if f.lower().endswith('.wav')]\n",
    "            expected_output = [f for f in sampled_files if f.lower().endswith('.wav')]\n",
    "            \n",
    "            if len(existing_files) >= len(expected_output) * 0.8:  # 80% completion threshold\n",
    "                print(f\"      âœ… {method_name}: Already completed ({len(existing_files)} files)\")\n",
    "                efficiency_metrics = {\n",
    "                    'method_name': method_name,\n",
    "                    'condition_name': condition_name,\n",
    "                    'processing_time_sec': None,  # Already completed\n",
    "                    'files_processed': len(existing_files),\n",
    "                    'success': True,\n",
    "                    'note': 'Previously completed (downscaled)',\n",
    "                    'total_files_available': all_files_count,\n",
    "                    'sampled_files_count': len(sampled_files),\n",
    "                    'sample_size_note': 'downscaled_20_files'\n",
    "                }\n",
    "            else:\n",
    "                print(f\"      ğŸ”„ {method_name}: Incomplete ({len(existing_files)}/{len(expected_output)}) - reprocessing\")\n",
    "        \n",
    "        # Apply denoising method if needed\n",
    "        if efficiency_metrics is None:\n",
    "            efficiency_metrics = measure_denoising_efficiency_downscaled(\n",
    "                input_dir=temp_input_dir,\n",
    "                output_dir=denoised_output_path,\n",
    "                method_script=method_script,\n",
    "                method_name=method_name\n",
    "            )\n",
    "            \n",
    "            # Add sampling metadata to efficiency metrics\n",
    "            if efficiency_metrics:\n",
    "                efficiency_metrics['condition_name'] = condition_name\n",
    "                efficiency_metrics['total_files_available'] = all_files_count\n",
    "                efficiency_metrics['sampled_files_count'] = len(sampled_files)\n",
    "                efficiency_metrics['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "        \n",
    "        # Clean up temporary directory\n",
    "        try:\n",
    "            if os.path.exists(temp_input_dir):\n",
    "                shutil.rmtree(temp_input_dir)\n",
    "                print(f\"      ğŸ§¹ Cleaned up temp directory\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"      âš ï¸  Temp cleanup warning: {cleanup_error}\")\n",
    "        \n",
    "        # Evaluate denoised audio performance\n",
    "        performance_results = None\n",
    "        if os.path.exists(denoised_output_path):\n",
    "            performance_results = evaluate_denoised_audio_downscaled(\n",
    "                denoised_audio_dir=denoised_output_path,\n",
    "                condition_name=condition_name,\n",
    "                method_name=method_name,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata\n",
    "            )\n",
    "            \n",
    "            if performance_results:\n",
    "                # Add original noisy performance for comparison\n",
    "                performance_results['original_f1'] = condition_row['f1_score']\n",
    "                performance_results['original_degradation_pct'] = condition_row['f1_degradation_pct']\n",
    "                \n",
    "                # Calculate recovery metrics\n",
    "                if clean_baseline:\n",
    "                    clean_f1 = clean_baseline['clean_f1_score']\n",
    "                    noisy_f1 = condition_row['f1_score']\n",
    "                    denoised_f1 = performance_results['f1_score']\n",
    "                    \n",
    "                    # Recovery percentage: (denoised - noisy) / (clean - noisy) * 100\n",
    "                    if clean_f1 > noisy_f1:\n",
    "                        recovery_pct = (denoised_f1 - noisy_f1) / (clean_f1 - noisy_f1) * 100\n",
    "                    else:\n",
    "                        recovery_pct = 0\n",
    "                    \n",
    "                    performance_results['f1_recovery_pct'] = recovery_pct\n",
    "                    performance_results['clean_baseline_f1'] = clean_f1\n",
    "                \n",
    "                # Add sampling metadata to performance results\n",
    "                performance_results['total_files_available'] = all_files_count\n",
    "                performance_results['sampled_files_count'] = len(sampled_files)\n",
    "                performance_results['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "                \n",
    "                print(f\"      âœ… {method_name}: F1={performance_results['f1_score']:.3f}, Recovery={recovery_pct:.1f}% [Sample: {len(sampled_files)}/{all_files_count}]\")\n",
    "        \n",
    "        return performance_results, efficiency_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ {method_name} failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main downscaled processing execution\n",
    "if priority_conditions is not None and model is not None:\n",
    "    \n",
    "    print(f\"ğŸš€ DOWNSCALED PROCESSING CONFIGURATION:\")\n",
    "    print(f\"   ğŸ¯ Conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   ğŸ”§ Methods: {len(DENOISING_METHODS)}\")\n",
    "    print(f\"   ğŸ“Š Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    print(f\"   ğŸ“ˆ Total evaluations: {len(priority_conditions) * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   â±ï¸  Expected time: ~30 minutes\")\n",
    "    \n",
    "    all_denoising_results = []\n",
    "    all_efficiency_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each condition sequentially (methods applied sequentially within each condition)\n",
    "    for condition_idx, (_, condition_row) in enumerate(priority_conditions.iterrows()):\n",
    "        condition_name = condition_row['condition_name']\n",
    "        \n",
    "        print(f\"\\nğŸ“ Condition {condition_idx + 1}/{len(priority_conditions)}: {condition_name}\")\n",
    "        print(f\"   ğŸ“‰ Original performance: F1={condition_row['f1_score']:.3f} (-{condition_row['f1_degradation_pct']:.1f}%)\")\n",
    "        \n",
    "        condition_results = []\n",
    "        condition_efficiency = []\n",
    "        \n",
    "        # Apply each denoising method to this condition\n",
    "        for method_idx, (method_key, method_config) in enumerate(DENOISING_METHODS.items()):\n",
    "            method_name = method_config['name']\n",
    "            print(f\"   ğŸ”§ Method {method_idx + 1}/{len(DENOISING_METHODS)}: {method_name}\")\n",
    "            \n",
    "            performance_result, efficiency_result = apply_and_evaluate_single_method_downscaled(\n",
    "                condition_name=condition_name,\n",
    "                condition_row=condition_row,\n",
    "                method_key=method_key,\n",
    "                method_config=method_config,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata,\n",
    "                clean_baseline=clean_baseline\n",
    "            )\n",
    "            \n",
    "            if performance_result:\n",
    "                condition_results.append(performance_result)\n",
    "                all_denoising_results.append(performance_result)\n",
    "            if efficiency_result:\n",
    "                condition_efficiency.append(efficiency_result)\n",
    "                all_efficiency_results.append(efficiency_result)\n",
    "        \n",
    "        # Progress and timing\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_conditions = len(priority_conditions) - (condition_idx + 1)\n",
    "        eta = (elapsed / (condition_idx + 1)) * remaining_conditions if condition_idx > 0 else 0\n",
    "        \n",
    "        print(f\"   âœ… Condition completed: {len(condition_results)} methods successful\")\n",
    "        print(f\"   â±ï¸  Progress: {condition_idx + 1}/{len(priority_conditions)}, ETA: {eta/60:.1f} minutes\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nğŸ’¾ Saving downscaled processing results...\")\n",
    "    \n",
    "    if all_denoising_results:\n",
    "        denoising_df = pd.DataFrame(all_denoising_results)\n",
    "        denoising_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_performance_results.csv\")\n",
    "        denoising_df.to_csv(denoising_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Denoising performance results saved: {denoising_results_path}\")\n",
    "    \n",
    "    if all_efficiency_results:\n",
    "        efficiency_df = pd.DataFrame(all_efficiency_results)\n",
    "        efficiency_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_efficiency_results.csv\")\n",
    "        efficiency_df.to_csv(efficiency_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Denoising efficiency results saved: {efficiency_results_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ DOWNSCALED DENOISING APPLICATION COMPLETE!\")\n",
    "    print(f\"â±ï¸  Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Performance evaluations: {len(all_denoising_results)}\")\n",
    "    print(f\"âš¡ Efficiency measurements: {len(all_efficiency_results)}\")\n",
    "    print(f\"ğŸ“ Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"ğŸ¯ Total combinations processed: {len(priority_conditions)} conditions Ã— {len(DENOISING_METHODS)} methods\")\n",
    "    \n",
    "    # Method ranking preview\n",
    "    if all_denoising_results:\n",
    "        results_df = pd.DataFrame(all_denoising_results)\n",
    "        if 'f1_recovery_pct' in results_df.columns:\n",
    "            method_rankings = results_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            print(f\"\\nğŸ† PRELIMINARY METHOD RANKING (F1 Recovery):\")\n",
    "            for rank, (method, recovery) in enumerate(method_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% average recovery\")\n",
    "    \n",
    "    # Set global variables for subsequent cells\n",
    "    denoising_results = all_denoising_results\n",
    "    efficiency_results = all_efficiency_results\n",
    "    \n",
    "else:\n",
    "    if priority_conditions is None:\n",
    "        print(f\"âš ï¸  Priority conditions not available - check Phase 2 results\")\n",
    "    if model is None:\n",
    "        print(f\"âš ï¸  Model not loaded - cannot evaluate performance\")\n",
    "    \n",
    "    denoising_results = []\n",
    "    efficiency_results = []\n",
    "\n",
    "print(f\"\\nTime finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eada7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š SIGNAL QUALITY ASSESSMENT - DOWNSCALED\n",
      "==================================================\n",
      "ğŸ” Performing downscaled signal quality assessment on 20 denoising results...\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.52 dB (Â±0.99) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.52 dB (Â±0.99) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 4.36 dB (Â±1.42) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 4.36 dB (Â±1.42) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.77 dB (Â±0.51) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 6.77 dB (Â±0.51) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_vacuum_cleaner...\n",
      "      âœ… SNR improvement: 27.74 dB (Â±8.38) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 27.74 dB (Â±8.38) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.24 dB (Â±0.64) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.24 dB (Â±0.64) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 4.49 dB (Â±0.55) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 4.49 dB (Â±0.55) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.39 dB (Â±0.73) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 5.39 dB (Â±0.73) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_cat...\n",
      "      âœ… SNR improvement: 27.92 dB (Â±5.37) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 27.92 dB (Â±5.37) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.68 dB (Â±0.26) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.68 dB (Â±0.26) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 4.32 dB (Â±0.45) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 4.32 dB (Â±0.45) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.80 dB (Â±0.38) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 5.80 dB (Â±0.38) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_door_wood_creaks...\n",
      "      âœ… SNR improvement: 27.80 dB (Â±4.04) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 27.80 dB (Â±4.04) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 4.33 dB (Â±1.70) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 4.33 dB (Â±1.70) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.42 dB (Â±1.25) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.42 dB (Â±1.25) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.22 dB (Â±1.08) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 5.22 dB (Â±1.08) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_crying_baby...\n",
      "      âœ… SNR improvement: 21.72 dB (Â±1.86) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 21.72 dB (Â±1.86) [n=10]\n",
      "   ğŸ” Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 8.45 dB (Â±0.59) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 8.45 dB (Â±0.59) [n=10]\n",
      "   ğŸ” Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 5.61 dB (Â±0.81) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 5.61 dB (Â±0.81) [n=10]\n",
      "   ğŸ” Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 6.19 dB (Â±0.47) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 6.19 dB (Â±0.47) [n=10]\n",
      "   ğŸ” Assessing DeepFilterNet on patient_01_wav_5db_coughing...\n",
      "      âœ… SNR improvement: 23.78 dB (Â±2.08) [n=10]\n",
      "\n",
      "ğŸ’¾ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "ğŸ“Š Signal Quality Summary (Downscaled):\n",
      "   ğŸ“ˆ Average SNR improvement: 10.64 dB\n",
      "   ğŸ“‰ Average spectral distortion: 1.8776\n",
      "   ğŸ† Best SNR improvement: DeepFilterNet (27.92 dB)\n",
      "   ğŸ’¥ Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "âœ… Signal quality assessment complete (downscaled)\n",
      "      âœ… SNR improvement: 23.78 dB (Â±2.08) [n=10]\n",
      "\n",
      "ğŸ’¾ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "ğŸ“Š Signal Quality Summary (Downscaled):\n",
      "   ğŸ“ˆ Average SNR improvement: 10.64 dB\n",
      "   ğŸ“‰ Average spectral distortion: 1.8776\n",
      "   ğŸ† Best SNR improvement: DeepFilterNet (27.92 dB)\n",
      "   ğŸ’¥ Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "âœ… Signal quality assessment complete (downscaled)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Signal Quality Assessment (Downscaled)\n",
    "print(\"ğŸ“Š SIGNAL QUALITY ASSESSMENT - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def calculate_snr(clean_audio, noisy_audio):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio in dB\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(noisy_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        noisy_audio = noisy_audio[:min_len]\n",
    "        \n",
    "        # Calculate signal and noise power\n",
    "        signal_power = np.mean(clean_audio ** 2)\n",
    "        noise_power = np.mean((noisy_audio - clean_audio) ** 2)\n",
    "        \n",
    "        if noise_power > 0:\n",
    "            snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "        else:\n",
    "            snr_db = float('inf')\n",
    "        \n",
    "        return snr_db\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_spectral_distortion(clean_audio, processed_audio, sr=16000):\n",
    "    \"\"\"Calculate spectral distortion between clean and processed audio\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(processed_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        processed_audio = processed_audio[:min_len]\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        clean_spec = np.abs(librosa.stft(clean_audio))\n",
    "        processed_spec = np.abs(librosa.stft(processed_audio))\n",
    "        \n",
    "        # Calculate L2 distance\n",
    "        min_time = min(clean_spec.shape[1], processed_spec.shape[1])\n",
    "        clean_spec = clean_spec[:, :min_time]\n",
    "        processed_spec = processed_spec[:, :min_time]\n",
    "        \n",
    "        spectral_distance = np.sqrt(np.mean((clean_spec - processed_spec) ** 2))\n",
    "        \n",
    "        return spectral_distance\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def assess_signal_quality_downscaled(condition_name, method_name, clean_dir, noisy_dir, denoised_dir, sample_size=10):\n",
    "    \"\"\"Assess signal quality improvement for downscaled method-condition combination\"\"\"\n",
    "    \n",
    "    print(f\"   ğŸ” Assessing {method_name} on {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample files for analysis (use min between sample_size and available files)\n",
    "        denoised_files = [f for f in os.listdir(denoised_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = denoised_files[:min(sample_size, len(denoised_files))]\n",
    "        \n",
    "        snr_improvements = []\n",
    "        spectral_distortions = []\n",
    "        processed_files = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                denoised_path = os.path.join(denoised_dir, wav_file)\n",
    "                denoised_audio, sr = librosa.load(denoised_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Find corresponding noisy and clean files\n",
    "                noisy_path = os.path.join(noisy_dir, wav_file.replace('denoised_', '').replace('mixed_', 'mixed_'))\n",
    "                \n",
    "                # Try to find clean file (remove patient prefix from condition name)\n",
    "                condition_parts = condition_name.split('_')\n",
    "                patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "                clean_filename = wav_file.replace('mixed_', '').replace('denoised_', '')\n",
    "                clean_path = os.path.join(clean_dir, f\"{patient_id}_wav\", clean_filename)\n",
    "                \n",
    "                if os.path.exists(noisy_path):\n",
    "                    noisy_audio, _ = librosa.load(noisy_path, sr=TARGET_SAMPLE_RATE)\n",
    "                    \n",
    "                    if os.path.exists(clean_path):\n",
    "                        clean_audio, _ = librosa.load(clean_path, sr=TARGET_SAMPLE_RATE)\n",
    "                        \n",
    "                        # Calculate SNR improvement\n",
    "                        noisy_snr = calculate_snr(clean_audio, noisy_audio)\n",
    "                        denoised_snr = calculate_snr(clean_audio, denoised_audio)\n",
    "                        \n",
    "                        if noisy_snr is not None and denoised_snr is not None:\n",
    "                            snr_improvement = denoised_snr - noisy_snr\n",
    "                            snr_improvements.append(snr_improvement)\n",
    "                        \n",
    "                        # Calculate spectral distortion\n",
    "                        spectral_dist = calculate_spectral_distortion(clean_audio, denoised_audio)\n",
    "                        if spectral_dist is not None:\n",
    "                            spectral_distortions.append(spectral_dist)\n",
    "                    \n",
    "                    processed_files += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if processed_files < 2:  # Show first 2 errors\n",
    "                    print(f\"      âš ï¸  Error processing {wav_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        quality_metrics = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'files_analyzed': processed_files,\n",
    "            'snr_improvement_db': np.mean(snr_improvements) if snr_improvements else None,\n",
    "            'snr_improvement_std': np.std(snr_improvements) if snr_improvements else None,\n",
    "            'spectral_distortion': np.mean(spectral_distortions) if spectral_distortions else None,\n",
    "            'spectral_distortion_std': np.std(spectral_distortions) if spectral_distortions else None,\n",
    "            'sample_size_note': 'downscaled_analysis'\n",
    "        }\n",
    "        \n",
    "        if snr_improvements:\n",
    "            print(f\"      âœ… SNR improvement: {np.mean(snr_improvements):.2f} dB (Â±{np.std(snr_improvements):.2f}) [n={processed_files}]\")\n",
    "        \n",
    "        return quality_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Quality assessment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Perform signal quality assessment if denoising results are available\n",
    "signal_quality_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ” Performing downscaled signal quality assessment on {len(denoising_results)} denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    for result in denoising_results:\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            clean_dir = BASE_DATA_DIR\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(denoised_dir) and os.path.exists(noisy_dir):\n",
    "                quality_result = assess_signal_quality_downscaled(\n",
    "                    condition_name=condition_name,\n",
    "                    method_name=method_name,\n",
    "                    clean_dir=clean_dir,\n",
    "                    noisy_dir=noisy_dir,\n",
    "                    denoised_dir=denoised_dir,\n",
    "                    sample_size=10  # Analyze 10 files per condition (downscaled)\n",
    "                )\n",
    "                \n",
    "                if quality_result:\n",
    "                    signal_quality_results.append(quality_result)\n",
    "    \n",
    "    # Save signal quality results\n",
    "    if signal_quality_results:\n",
    "        quality_df = pd.DataFrame(signal_quality_results)\n",
    "        quality_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_signal_quality_results.csv\")\n",
    "        quality_df.to_csv(quality_results_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Signal quality results saved: {quality_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nğŸ“Š Signal Quality Summary (Downscaled):\")\n",
    "        valid_snr = quality_df[quality_df['snr_improvement_db'].notna()]\n",
    "        if not valid_snr.empty:\n",
    "            print(f\"   ğŸ“ˆ Average SNR improvement: {valid_snr['snr_improvement_db'].mean():.2f} dB\")\n",
    "            print(f\"   ğŸ“‰ Average spectral distortion: {quality_df['spectral_distortion'].mean():.4f}\")\n",
    "            \n",
    "            # Best and worst methods for signal quality\n",
    "            best_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmax()]\n",
    "            worst_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmin()]\n",
    "            \n",
    "            print(f\"   ğŸ† Best SNR improvement: {best_snr['method_name']} ({best_snr['snr_improvement_db']:.2f} dB)\")\n",
    "            print(f\"   ğŸ’¥ Worst SNR improvement: {worst_snr['method_name']} ({worst_snr['snr_improvement_db']:.2f} dB)\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  No valid SNR measurements available\")\n",
    "    \n",
    "    print(f\"\\nâœ… Signal quality assessment complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Signal quality assessment skipped - no denoising results available\")\n",
    "    signal_quality_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e0d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\n",
      "==================================================\n",
      "ğŸ§ª Analyzing feature preservation for downscaled denoising results...\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.022\n",
      "      âœ… Variance preservation: 5.407\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.022\n",
      "      âœ… Variance preservation: 5.407\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.088\n",
      "      âœ… Variance preservation: 9.119\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.088\n",
      "      âœ… Variance preservation: 9.119\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.389\n",
      "      âœ… Variance preservation: 11.549\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.389\n",
      "      âœ… Variance preservation: 11.549\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.062\n",
      "      âœ… Variance preservation: 9.254\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.062\n",
      "      âœ… Variance preservation: 9.254\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.588\n",
      "      âœ… Variance preservation: 2.053\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.588\n",
      "      âœ… Variance preservation: 2.053\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.180\n",
      "      âœ… Variance preservation: 7.205\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.180\n",
      "      âœ… Variance preservation: 7.205\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.020\n",
      "      âœ… Variance preservation: 9.214\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.020\n",
      "      âœ… Variance preservation: 9.214\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.227\n",
      "      âœ… Variance preservation: 9.564\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.227\n",
      "      âœ… Variance preservation: 9.564\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.101\n",
      "      âœ… Variance preservation: 8.232\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.101\n",
      "      âœ… Variance preservation: 8.232\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.096\n",
      "      âœ… Variance preservation: 1.575\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.096\n",
      "      âœ… Variance preservation: 1.575\n",
      "\n",
      "   ğŸ”¬ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.130\n",
      "      âœ… Variance preservation: 4.620\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.130\n",
      "      âœ… Variance preservation: 4.620\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.109\n",
      "      âœ… Variance preservation: 1.703\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.109\n",
      "      âœ… Variance preservation: 1.703\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.043\n",
      "      âœ… Variance preservation: 15.404\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.043\n",
      "      âœ… Variance preservation: 15.404\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.251\n",
      "      âœ… Variance preservation: 3.196\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: 0.251\n",
      "      âœ… Variance preservation: 3.196\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.006\n",
      "      âœ… Variance preservation: 40.163\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.006\n",
      "      âœ… Variance preservation: 40.163\n",
      "\n",
      "   ğŸ”¬ Analyzing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.309\n",
      "      âœ… Variance preservation: 2.137\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.309\n",
      "      âœ… Variance preservation: 2.137\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.920\n",
      "      âœ… Variance preservation: 10.945\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.920\n",
      "      âœ… Variance preservation: 10.945\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.153\n",
      "      âœ… Variance preservation: 19.279\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.153\n",
      "      âœ… Variance preservation: 19.279\n",
      "\n",
      "   ğŸ”¬ Analyzing DeepFilterNet on patient_01_wav_5db_cat...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.224\n",
      "      âœ… Variance preservation: 6.250\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.224\n",
      "      âœ… Variance preservation: 6.250\n",
      "\n",
      "   ğŸ”¬ Analyzing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      ğŸ“Š Extracting features for comparison (downscaled)...\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.323\n",
      "      âœ… Variance preservation: 21.046\n",
      "\n",
      "ğŸ’¾ Feature preservation results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_feature_preservation_results.csv\n",
      "\n",
      "ğŸ§¬ Feature Preservation Summary (Downscaled):\n",
      "   ğŸ“Š Average correlation recovery: -0.095\n",
      "   ğŸ“Š Average variance preservation: 9.896\n",
      "   ğŸ“Š Average mean error: 1.594\n",
      "   ğŸ† Best preservation: Wiener Filtering (0.389)\n",
      "   ğŸ’¥ Worst preservation: Wiener Filtering (-0.920)\n",
      "\n",
      "âœ… Feature preservation analysis complete (downscaled)\n",
      "      ğŸ“ˆ Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      âœ… Correlation recovery: -0.323\n",
      "      âœ… Variance preservation: 21.046\n",
      "\n",
      "ğŸ’¾ Feature preservation results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_feature_preservation_results.csv\n",
      "\n",
      "ğŸ§¬ Feature Preservation Summary (Downscaled):\n",
      "   ğŸ“Š Average correlation recovery: -0.095\n",
      "   ğŸ“Š Average variance preservation: 9.896\n",
      "   ğŸ“Š Average mean error: 1.594\n",
      "   ğŸ† Best preservation: Wiener Filtering (0.389)\n",
      "   ğŸ’¥ Worst preservation: Wiener Filtering (-0.920)\n",
      "\n",
      "âœ… Feature preservation analysis complete (downscaled)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Feature Preservation Analysis (Downscaled)\n",
    "print(\"ğŸ§¬ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def analyze_feature_preservation_downscaled(clean_features, noisy_features, denoised_features):\n",
    "    \"\"\"Analyze how well denoising preserves important breathing features (downscaled)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to DataFrames if needed\n",
    "        if isinstance(clean_features, list):\n",
    "            clean_df = pd.DataFrame(clean_features)\n",
    "        else:\n",
    "            clean_df = clean_features\n",
    "            \n",
    "        if isinstance(noisy_features, list):\n",
    "            noisy_df = pd.DataFrame(noisy_features)\n",
    "        else:\n",
    "            noisy_df = noisy_features\n",
    "            \n",
    "        if isinstance(denoised_features, list):\n",
    "            denoised_df = pd.DataFrame(denoised_features)\n",
    "        else:\n",
    "            denoised_df = denoised_features\n",
    "        \n",
    "        # Ensure same features and sample size\n",
    "        common_features = list(set(clean_df.columns) & set(denoised_df.columns) & set(noisy_df.columns))\n",
    "        min_samples = min(len(clean_df), len(noisy_df), len(denoised_df))\n",
    "        \n",
    "        clean_df = clean_df[common_features].iloc[:min_samples]\n",
    "        noisy_df = noisy_df[common_features].iloc[:min_samples]\n",
    "        denoised_df = denoised_df[common_features].iloc[:min_samples]\n",
    "        \n",
    "        preservation_metrics = {}\n",
    "        \n",
    "        for feature in common_features:\n",
    "            # Correlation preservation\n",
    "            clean_values = clean_df[feature].values\n",
    "            noisy_values = noisy_df[feature].values\n",
    "            denoised_values = denoised_df[feature].values\n",
    "            \n",
    "            # Calculate correlations with original clean values\n",
    "            try:\n",
    "                clean_noisy_corr = np.corrcoef(clean_values, noisy_values)[0, 1]\n",
    "                clean_denoised_corr = np.corrcoef(clean_values, denoised_values)[0, 1]\n",
    "                \n",
    "                # Correlation recovery: how much of the original correlation is restored\n",
    "                if not np.isnan(clean_noisy_corr) and not np.isnan(clean_denoised_corr):\n",
    "                    correlation_recovery = clean_denoised_corr / clean_noisy_corr if clean_noisy_corr != 0 else 1\n",
    "                else:\n",
    "                    correlation_recovery = 0\n",
    "            except:\n",
    "                clean_noisy_corr = 0\n",
    "                clean_denoised_corr = 0\n",
    "                correlation_recovery = 0\n",
    "            \n",
    "            # Variance preservation\n",
    "            clean_var = np.var(clean_values)\n",
    "            noisy_var = np.var(noisy_values)\n",
    "            denoised_var = np.var(denoised_values)\n",
    "            \n",
    "            variance_ratio = denoised_var / clean_var if clean_var > 0 else 0\n",
    "            \n",
    "            # Mean preservation\n",
    "            clean_mean = np.mean(clean_values)\n",
    "            denoised_mean = np.mean(denoised_values)\n",
    "            mean_error = abs(clean_mean - denoised_mean) / abs(clean_mean) if clean_mean != 0 else 0\n",
    "            \n",
    "            preservation_metrics[feature] = {\n",
    "                'clean_noisy_correlation': clean_noisy_corr,\n",
    "                'clean_denoised_correlation': clean_denoised_corr,\n",
    "                'correlation_recovery': correlation_recovery,\n",
    "                'variance_ratio': variance_ratio,\n",
    "                'mean_relative_error': mean_error\n",
    "            }\n",
    "        \n",
    "        return preservation_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Feature preservation analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features_from_audio_dir_downscaled(audio_dir, sample_size=15):\n",
    "    \"\"\"Extract features from a directory of audio files (downscaled sample)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        wav_files = [f for f in os.listdir(audio_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = wav_files[:min(sample_size, len(wav_files))]\n",
    "        \n",
    "        features_list = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                wav_path = os.path.join(audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features:\n",
    "                    features_list.append(features)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return features_list, processed_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Feature extraction from directory failed: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Perform feature preservation analysis\n",
    "feature_preservation_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ§ª Analyzing feature preservation for downscaled denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    method_condition_pairs = list(set([(r['condition_name'], r['method_name']) for r in denoising_results]))\n",
    "    \n",
    "    for condition_name, method_name in method_condition_pairs:\n",
    "        print(f\"\\n   ğŸ”¬ Analyzing {method_name} on {condition_name}...\")\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            condition_parts = condition_name.split('_')\n",
    "            patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "            clean_dir = os.path.join(BASE_DATA_DIR, f\"{patient_id}_wav\")\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(clean_dir) and os.path.exists(noisy_dir) and os.path.exists(denoised_dir):\n",
    "                # Extract features from each audio type (downscaled samples)\n",
    "                print(f\"      ğŸ“Š Extracting features for comparison (downscaled)...\")\n",
    "                \n",
    "                clean_features, clean_count = extract_features_from_audio_dir_downscaled(clean_dir, sample_size=15)\n",
    "                noisy_features, noisy_count = extract_features_from_audio_dir_downscaled(noisy_dir, sample_size=15)\n",
    "                denoised_features, denoised_count = extract_features_from_audio_dir_downscaled(denoised_dir, sample_size=15)\n",
    "                \n",
    "                print(f\"      ğŸ“ˆ Features extracted: Clean={clean_count}, Noisy={noisy_count}, Denoised={denoised_count}\")\n",
    "                \n",
    "                if clean_features and noisy_features and denoised_features:\n",
    "                    preservation_metrics = analyze_feature_preservation_downscaled(\n",
    "                        clean_features=clean_features,\n",
    "                        noisy_features=noisy_features,\n",
    "                        denoised_features=denoised_features\n",
    "                    )\n",
    "                    \n",
    "                    if preservation_metrics:\n",
    "                        # Calculate aggregate preservation scores\n",
    "                        correlation_recoveries = [m['correlation_recovery'] for m in preservation_metrics.values() if not np.isnan(m['correlation_recovery']) and not np.isinf(m['correlation_recovery'])]\n",
    "                        variance_ratios = [m['variance_ratio'] for m in preservation_metrics.values() if not np.isnan(m['variance_ratio']) and not np.isinf(m['variance_ratio'])]\n",
    "                        mean_errors = [m['mean_relative_error'] for m in preservation_metrics.values() if not np.isnan(m['mean_relative_error']) and not np.isinf(m['mean_relative_error'])]\n",
    "                        \n",
    "                        aggregate_result = {\n",
    "                            'condition_name': condition_name,\n",
    "                            'method_name': method_name,\n",
    "                            'avg_correlation_recovery': np.mean(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'std_correlation_recovery': np.std(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'avg_variance_ratio': np.mean(variance_ratios) if variance_ratios else 0,\n",
    "                            'std_variance_ratio': np.std(variance_ratios) if variance_ratios else 0,\n",
    "                            'avg_mean_error': np.mean(mean_errors) if mean_errors else 0,\n",
    "                            'features_analyzed': len(preservation_metrics),\n",
    "                            'sample_size_note': 'downscaled_15_files',\n",
    "                            'detailed_metrics': preservation_metrics\n",
    "                        }\n",
    "                        \n",
    "                        feature_preservation_results.append(aggregate_result)\n",
    "                        \n",
    "                        print(f\"      âœ… Correlation recovery: {aggregate_result['avg_correlation_recovery']:.3f}\")\n",
    "                        print(f\"      âœ… Variance preservation: {aggregate_result['avg_variance_ratio']:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                missing_dirs = []\n",
    "                if not os.path.exists(clean_dir): missing_dirs.append(f\"clean ({clean_dir})\")\n",
    "                if not os.path.exists(noisy_dir): missing_dirs.append(f\"noisy ({noisy_dir})\")\n",
    "                if not os.path.exists(denoised_dir): missing_dirs.append(f\"denoised ({denoised_dir})\")\n",
    "                print(f\"      âš ï¸  Missing directories: {', '.join(missing_dirs)}\")\n",
    "    \n",
    "    # Save feature preservation results\n",
    "    if feature_preservation_results:\n",
    "        # Save aggregate results\n",
    "        preservation_summary = [{\n",
    "            'condition_name': r['condition_name'],\n",
    "            'method_name': r['method_name'],\n",
    "            'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "            'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "            'avg_mean_error': r['avg_mean_error'],\n",
    "            'features_analyzed': r['features_analyzed'],\n",
    "            'sample_size_note': r['sample_size_note']\n",
    "        } for r in feature_preservation_results]\n",
    "        \n",
    "        preservation_df = pd.DataFrame(preservation_summary)\n",
    "        preservation_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_feature_preservation_results.csv\")\n",
    "        preservation_df.to_csv(preservation_results_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Feature preservation results saved: {preservation_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nğŸ§¬ Feature Preservation Summary (Downscaled):\")\n",
    "        print(f\"   ğŸ“Š Average correlation recovery: {preservation_df['avg_correlation_recovery'].mean():.3f}\")\n",
    "        print(f\"   ğŸ“Š Average variance preservation: {preservation_df['avg_variance_ratio'].mean():.3f}\")\n",
    "        print(f\"   ğŸ“Š Average mean error: {preservation_df['avg_mean_error'].mean():.3f}\")\n",
    "        \n",
    "        # Best and worst methods for feature preservation\n",
    "        if len(preservation_df) > 1:\n",
    "            best_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmax()]\n",
    "            worst_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmin()]\n",
    "            \n",
    "            print(f\"   ğŸ† Best preservation: {best_preservation['method_name']} ({best_preservation['avg_correlation_recovery']:.3f})\")\n",
    "            print(f\"   ğŸ’¥ Worst preservation: {worst_preservation['method_name']} ({worst_preservation['avg_correlation_recovery']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nâœ… Feature preservation analysis complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Feature preservation analysis skipped - no denoising results available\")\n",
    "    feature_preservation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93653a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” COMPILING COMPREHENSIVE RESULTS AND MULTI-CRITERIA ANALYSIS - DOWNSCALED\n",
      "======================================================================\n",
      "ğŸ“Š Compiling results from 20 downscaled denoising evaluations...\n",
      "âœ… Compiled 20 comprehensive result records\n",
      "ğŸ’¾ Comprehensive results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_comprehensive_results.csv\n",
      "\n",
      "ğŸ† SMARTPHONE SUITABILITY RANKING (DOWNSCALED):\n",
      "   1. DeepFilterNet: 0.697\n",
      "   2. Spectral Subtraction: 0.298\n",
      "   3. LogMMSE: 0.215\n",
      "   4. Wiener Filtering: 0.192\n",
      "\n",
      "ğŸ“Š PERFORMANCE RECOVERY RANKING (DOWNSCALED):\n",
      "   1. DeepFilterNet: 23.2% F1 recovery\n",
      "   2. Spectral Subtraction: 14.6% F1 recovery\n",
      "   3. LogMMSE: -9.1% F1 recovery\n",
      "   4. Wiener Filtering: -9.1% F1 recovery\n",
      "\n",
      "ğŸ“‹ DOWNSCALED PHASE 3 FINAL SUMMARY REPORT:\n",
      "\n",
      "ğŸ¯ Research Objective Achievement:\n",
      "   âœ… Evaluated 4 denoising methods\n",
      "   âœ… Tested on 5 representative worst-case conditions (5dB SNR)\n",
      "   âœ… Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\n",
      "   âœ… Generated smartphone deployment recommendations\n",
      "   âœ… Downscaled scope: 20 files per condition for rapid evaluation\n",
      "\n",
      "ğŸ† Key Findings from Downscaled Sampling:\n",
      "   ğŸ¥‡ Best Overall Method: DeepFilterNet (Score: 0.748)\n",
      "   ğŸ¯ Best Performance Recovery: Spectral Subtraction (78.2% F1 recovery)\n",
      "   âš¡ Most Efficient: DeepFilterNet (13.99x real-time)\n",
      "\n",
      "ğŸ“Š Downscaled Performance Statistics:\n",
      "   ğŸ“ˆ Average F1 Recovery: 4.9% (Â±25.5%)\n",
      "   ğŸ“ˆ Methods achieving >50% recovery: 1 / 20\n",
      "   ğŸ“ˆ Methods achieving >75% recovery: 1 / 20\n",
      "   ğŸ¯ Conditions tested: 5 worst-case (5dB) per noise category\n",
      "   ğŸ“ Sample size: 20 files per condition\n",
      "\n",
      "ğŸ’¾ Final summary saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_phase3_final_summary.json\n",
      "\n",
      "ğŸ‰ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\n",
      "\n",
      "ğŸ“‹ Research Contributions Achieved:\n",
      "   âœ… Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\n",
      "   âœ… Downscaled sampling methodology for efficient evaluation\n",
      "   âœ… Smartphone deployment feasibility assessment\n",
      "   âœ… Evidence-based method recommendations for mobile health applications\n",
      "   âœ… Performance-efficiency trade-off quantification for worst-case scenarios\n",
      "   âœ… Proof-of-concept validation with 20-file sampling approach\n",
      "\n",
      "âœ… Comprehensive results compilation complete (downscaled)\n",
      "ğŸ’¾ Comprehensive results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_comprehensive_results.csv\n",
      "\n",
      "ğŸ† SMARTPHONE SUITABILITY RANKING (DOWNSCALED):\n",
      "   1. DeepFilterNet: 0.697\n",
      "   2. Spectral Subtraction: 0.298\n",
      "   3. LogMMSE: 0.215\n",
      "   4. Wiener Filtering: 0.192\n",
      "\n",
      "ğŸ“Š PERFORMANCE RECOVERY RANKING (DOWNSCALED):\n",
      "   1. DeepFilterNet: 23.2% F1 recovery\n",
      "   2. Spectral Subtraction: 14.6% F1 recovery\n",
      "   3. LogMMSE: -9.1% F1 recovery\n",
      "   4. Wiener Filtering: -9.1% F1 recovery\n",
      "\n",
      "ğŸ“‹ DOWNSCALED PHASE 3 FINAL SUMMARY REPORT:\n",
      "\n",
      "ğŸ¯ Research Objective Achievement:\n",
      "   âœ… Evaluated 4 denoising methods\n",
      "   âœ… Tested on 5 representative worst-case conditions (5dB SNR)\n",
      "   âœ… Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\n",
      "   âœ… Generated smartphone deployment recommendations\n",
      "   âœ… Downscaled scope: 20 files per condition for rapid evaluation\n",
      "\n",
      "ğŸ† Key Findings from Downscaled Sampling:\n",
      "   ğŸ¥‡ Best Overall Method: DeepFilterNet (Score: 0.748)\n",
      "   ğŸ¯ Best Performance Recovery: Spectral Subtraction (78.2% F1 recovery)\n",
      "   âš¡ Most Efficient: DeepFilterNet (13.99x real-time)\n",
      "\n",
      "ğŸ“Š Downscaled Performance Statistics:\n",
      "   ğŸ“ˆ Average F1 Recovery: 4.9% (Â±25.5%)\n",
      "   ğŸ“ˆ Methods achieving >50% recovery: 1 / 20\n",
      "   ğŸ“ˆ Methods achieving >75% recovery: 1 / 20\n",
      "   ğŸ¯ Conditions tested: 5 worst-case (5dB) per noise category\n",
      "   ğŸ“ Sample size: 20 files per condition\n",
      "\n",
      "ğŸ’¾ Final summary saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_phase3_final_summary.json\n",
      "\n",
      "ğŸ‰ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\n",
      "\n",
      "ğŸ“‹ Research Contributions Achieved:\n",
      "   âœ… Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\n",
      "   âœ… Downscaled sampling methodology for efficient evaluation\n",
      "   âœ… Smartphone deployment feasibility assessment\n",
      "   âœ… Evidence-based method recommendations for mobile health applications\n",
      "   âœ… Performance-efficiency trade-off quantification for worst-case scenarios\n",
      "   âœ… Proof-of-concept validation with 20-file sampling approach\n",
      "\n",
      "âœ… Comprehensive results compilation complete (downscaled)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Compile Comprehensive Results and Multi-Criteria Analysis (Downscaled)\n",
    "print(\"ğŸ” COMPILING COMPREHENSIVE RESULTS AND MULTI-CRITERIA ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Compile all results into comprehensive dataset\n",
    "comprehensive_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"ğŸ“Š Compiling results from {len(denoising_results)} downscaled denoising evaluations...\")\n",
    "    \n",
    "    # Convert results lists to DataFrames for easier merging\n",
    "    denoising_df = pd.DataFrame(denoising_results) if denoising_results else pd.DataFrame()\n",
    "    efficiency_df = pd.DataFrame(efficiency_results) if 'efficiency_results' in locals() and efficiency_results else pd.DataFrame()\n",
    "    quality_df = pd.DataFrame(signal_quality_results) if 'signal_quality_results' in locals() and signal_quality_results else pd.DataFrame()\n",
    "    preservation_df = pd.DataFrame([{\n",
    "        'condition_name': r['condition_name'],\n",
    "        'method_name': r['method_name'],\n",
    "        'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "        'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "        'avg_mean_error': r['avg_mean_error']\n",
    "    } for r in feature_preservation_results]) if 'feature_preservation_results' in locals() and feature_preservation_results else pd.DataFrame()\n",
    "    \n",
    "    # Merge all results on condition_name and method_name\n",
    "    for _, result in denoising_df.iterrows():\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Start with denoising performance results\n",
    "        comprehensive_result = result.to_dict()\n",
    "        \n",
    "        # Add efficiency metrics\n",
    "        efficiency_match = efficiency_df[\n",
    "            (efficiency_df['condition_name'] == condition_name) & \n",
    "            (efficiency_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not efficiency_match.empty:\n",
    "            eff_result = efficiency_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'processing_time_sec': eff_result.get('processing_time_sec'),\n",
    "                'real_time_factor': eff_result.get('real_time_factor'),\n",
    "                'memory_usage_mb': eff_result.get('memory_usage_mb'),\n",
    "                'processing_speed_files_per_sec': eff_result.get('processing_speed_files_per_sec')\n",
    "            })\n",
    "        \n",
    "        # Add signal quality metrics\n",
    "        quality_match = quality_df[\n",
    "            (quality_df['condition_name'] == condition_name) & \n",
    "            (quality_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not quality_match.empty:\n",
    "            qual_result = quality_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'snr_improvement_db': qual_result.get('snr_improvement_db'),\n",
    "                'spectral_distortion': qual_result.get('spectral_distortion')\n",
    "            })\n",
    "        \n",
    "        # Add feature preservation metrics\n",
    "        preservation_match = preservation_df[\n",
    "            (preservation_df['condition_name'] == condition_name) & \n",
    "            (preservation_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not preservation_match.empty:\n",
    "            pres_result = preservation_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'avg_correlation_recovery': pres_result.get('avg_correlation_recovery'),\n",
    "                'avg_variance_ratio': pres_result.get('avg_variance_ratio'),\n",
    "                'avg_mean_error': pres_result.get('avg_mean_error')\n",
    "            })\n",
    "        \n",
    "        comprehensive_results.append(comprehensive_result)\n",
    "    \n",
    "    print(f\"âœ… Compiled {len(comprehensive_results)} comprehensive result records\")\n",
    "    \n",
    "    # Calculate normalized scores for multi-criteria analysis\n",
    "    if comprehensive_results:\n",
    "        comp_df = pd.DataFrame(comprehensive_results)\n",
    "        \n",
    "        # Normalize scores (0-1 scale) for smartphone suitability calculation\n",
    "        def normalize_score(values, higher_is_better=True):\n",
    "            values = pd.Series(values).fillna(0)  # Handle NaN values\n",
    "            if values.std() == 0:  # All values are the same\n",
    "                return pd.Series([0.5] * len(values))\n",
    "            if higher_is_better:\n",
    "                return (values - values.min()) / (values.max() - values.min())\n",
    "            else:\n",
    "                return (values.max() - values) / (values.max() - values.min())\n",
    "        \n",
    "        # Calculate individual dimension scores (handle NaN values gracefully)\n",
    "        comp_df['f1_recovery_score'] = normalize_score(comp_df.get('f1_recovery_pct', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['efficiency_score'] = normalize_score(comp_df.get('real_time_factor', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['signal_quality_score'] = normalize_score(comp_df.get('snr_improvement_db', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['feature_preservation_score'] = normalize_score(comp_df.get('avg_correlation_recovery', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        \n",
    "        # Calculate smartphone suitability composite score\n",
    "        SMARTPHONE_WEIGHTS = {\n",
    "            'f1_recovery': 0.40,        # Detection performance recovery (most critical)\n",
    "            'efficiency': 0.25,         # Processing speed + memory usage\n",
    "            'signal_quality': 0.20,     # SNR improvement + artifact control\n",
    "            'feature_preservation': 0.15 # Biomarker stability\n",
    "        }\n",
    "        \n",
    "        comp_df['smartphone_suitability_score'] = (\n",
    "            comp_df['f1_recovery_score'] * SMARTPHONE_WEIGHTS['f1_recovery'] +\n",
    "            comp_df['efficiency_score'] * SMARTPHONE_WEIGHTS['efficiency'] +\n",
    "            comp_df['signal_quality_score'] * SMARTPHONE_WEIGHTS['signal_quality'] +\n",
    "            comp_df['feature_preservation_score'] * SMARTPHONE_WEIGHTS['feature_preservation']\n",
    "        )\n",
    "        \n",
    "        # Update comprehensive_results with calculated scores\n",
    "        comprehensive_results = comp_df.to_dict('records')\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        comprehensive_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_comprehensive_results.csv\")\n",
    "        comp_df.to_csv(comprehensive_results_path, index=False)\n",
    "        print(f\"ğŸ’¾ Comprehensive results saved: {comprehensive_results_path}\")\n",
    "        \n",
    "        # Display ranking summary\n",
    "        print(f\"\\nğŸ† SMARTPHONE SUITABILITY RANKING (DOWNSCALED):\")\n",
    "        method_rankings = comp_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=False)\n",
    "        for rank, (method, score) in enumerate(method_rankings.items(), 1):\n",
    "            print(f\"   {rank}. {method}: {score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š PERFORMANCE RECOVERY RANKING (DOWNSCALED):\")\n",
    "        if 'f1_recovery_pct' in comp_df.columns:\n",
    "            performance_rankings = comp_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            for rank, (method, recovery) in enumerate(performance_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% F1 recovery\")\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸  No denoising results available for comprehensive analysis\")\n",
    "    print(f\"   Please ensure Cell 5 (denoising application) has been executed successfully\")\n",
    "    comprehensive_results = []\n",
    "\n",
    "if comprehensive_results:\n",
    "    # Generate final summary report for downscaled analysis\n",
    "    print(f\"\\nğŸ“‹ DOWNSCALED PHASE 3 FINAL SUMMARY REPORT:\")\n",
    "    print(f\"\\nğŸ¯ Research Objective Achievement:\")\n",
    "    print(f\"   âœ… Evaluated {len(DENOISING_METHODS)} denoising methods\")\n",
    "    print(f\"   âœ… Tested on {len(REPRESENTATIVE_CONDITIONS)} representative worst-case conditions (5dB SNR)\")\n",
    "    print(f\"   âœ… Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\")\n",
    "    print(f\"   âœ… Generated smartphone deployment recommendations\")\n",
    "    print(f\"   âœ… Downscaled scope: {SAMPLE_SIZE_PER_CONDITION} files per condition for rapid evaluation\")\n",
    "    \n",
    "    # Key findings\n",
    "    comp_df = pd.DataFrame(comprehensive_results)\n",
    "    best_overall = comp_df.loc[comp_df['smartphone_suitability_score'].idxmax()]\n",
    "    \n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        best_performance = comp_df.loc[comp_df['f1_recovery_pct'].idxmax()]\n",
    "    else:\n",
    "        best_performance = None\n",
    "    \n",
    "    valid_efficiency = comp_df[comp_df['real_time_factor'].notna()] if 'real_time_factor' in comp_df.columns else pd.DataFrame()\n",
    "    best_efficiency = valid_efficiency.loc[valid_efficiency['real_time_factor'].idxmax()] if not valid_efficiency.empty else None\n",
    "    \n",
    "    print(f\"\\nğŸ† Key Findings from Downscaled Sampling:\")\n",
    "    print(f\"   ğŸ¥‡ Best Overall Method: {best_overall['method_name']} (Score: {best_overall['smartphone_suitability_score']:.3f})\")\n",
    "    if best_performance is not None:\n",
    "        print(f\"   ğŸ¯ Best Performance Recovery: {best_performance['method_name']} ({best_performance['f1_recovery_pct']:.1f}% F1 recovery)\")\n",
    "    if best_efficiency is not None:\n",
    "        print(f\"   âš¡ Most Efficient: {best_efficiency['method_name']} ({best_efficiency['real_time_factor']:.2f}x real-time)\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        avg_recovery = comp_df['f1_recovery_pct'].mean()\n",
    "        recovery_std = comp_df['f1_recovery_pct'].std()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Downscaled Performance Statistics:\")\n",
    "        print(f\"   ğŸ“ˆ Average F1 Recovery: {avg_recovery:.1f}% (Â±{recovery_std:.1f}%)\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >50% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 50])} / {len(comp_df)}\")\n",
    "        print(f\"   ğŸ“ˆ Methods achieving >75% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 75])} / {len(comp_df)}\")\n",
    "        print(f\"   ğŸ¯ Conditions tested: {len(REPRESENTATIVE_CONDITIONS)} worst-case (5dB) per noise category\")\n",
    "        print(f\"   ğŸ“ Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    # Save final summary\n",
    "    final_summary = {\n",
    "        'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'phase3_status': 'completed_downscaled',\n",
    "        'downscaled_optimization': {\n",
    "            'sample_size_per_condition': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'total_files_per_evaluation': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'execution_approach': 'rapid_prototyping'\n",
    "        },\n",
    "        'representative_conditions': REPRESENTATIVE_CONDITIONS,\n",
    "        'methods_evaluated': len(DENOISING_METHODS),\n",
    "        'conditions_tested': len(REPRESENTATIVE_CONDITIONS),\n",
    "        'total_evaluations': len(comp_df),\n",
    "        'best_overall_method': best_overall['method_name'],\n",
    "        'best_overall_score': float(best_overall['smartphone_suitability_score']),\n",
    "        'research_contributions': [\n",
    "            'Downscaled systematic multi-dimensional evaluation of denoising for sleep apnea detection',\n",
    "            'Rapid prototyping methodology for efficient method comparison',\n",
    "            'Smartphone deployment feasibility assessment with reduced computational requirements',\n",
    "            'Evidence-based method recommendations for mobile health applications',\n",
    "            'Performance-efficiency trade-off quantification for worst-case scenarios'\n",
    "        ],\n",
    "        'files_generated': [\n",
    "            'downscaled_comprehensive_results.csv',\n",
    "            'downscaled_denoising_performance_results.csv',\n",
    "            'downscaled_denoising_efficiency_results.csv',\n",
    "            'downscaled_signal_quality_results.csv',\n",
    "            'downscaled_feature_preservation_results.csv'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if best_performance is not None:\n",
    "        final_summary['best_performance_method'] = best_performance['method_name']\n",
    "        final_summary['best_performance_recovery'] = float(best_performance['f1_recovery_pct'])\n",
    "        final_summary['average_f1_recovery'] = float(avg_recovery)\n",
    "    \n",
    "    summary_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_phase3_final_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    print(f\"\\nğŸ’¾ Final summary saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\")\n",
    "    print(f\"\\nğŸ“‹ Research Contributions Achieved:\")\n",
    "    print(f\"   âœ… Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\")\n",
    "    print(f\"   âœ… Downscaled sampling methodology for efficient evaluation\")\n",
    "    print(f\"   âœ… Smartphone deployment feasibility assessment\")\n",
    "    print(f\"   âœ… Evidence-based method recommendations for mobile health applications\")\n",
    "    print(f\"   âœ… Performance-efficiency trade-off quantification for worst-case scenarios\")\n",
    "    print(f\"   âœ… Proof-of-concept validation with {SAMPLE_SIZE_PER_CONDITION}-file sampling approach\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Comprehensive analysis skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive results compilation complete (downscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ GENERATING INDIVIDUAL METRIC VISUALIZATIONS FOR REPORT\n",
      "======================================================================\n",
      "ğŸ“Š Generating: Smartphone Suitability Rankings...\n",
      "ğŸ“Š Generating: F1 Recovery Performance...\n",
      "ğŸ“Š Generating: F1 Recovery Performance...\n",
      "ğŸ“Š Generating: Computational Efficiency...\n",
      "ğŸ“Š Generating: Computational Efficiency...\n",
      "ğŸ“Š Generating: Signal Quality Improvement...\n",
      "ğŸ“Š Generating: Signal Quality Improvement...\n",
      "ğŸ“Š Generating: Feature Preservation...\n",
      "ğŸ“Š Generating: Feature Preservation...\n",
      "ğŸ“Š Generating: Performance vs Efficiency Trade-off...\n",
      "ğŸ“Š Generating: Performance vs Efficiency Trade-off...\n",
      "ğŸ“Š Generating: Multi-Dimensional Method Comparison...\n",
      "ğŸ“Š Generating: Multi-Dimensional Method Comparison...\n",
      "ğŸ“Š Generating: Recovery Effectiveness...\n",
      "ğŸ“Š Generating: Recovery Effectiveness...\n",
      "ğŸ“Š Generating: F1 Recovery (%) Heatmap...\n",
      "ğŸ“Š Generating: F1 Recovery (%) Heatmap...\n",
      "ğŸ“Š Generating: Real-Time Factor Heatmap...\n",
      "ğŸ“Š Generating: SNR Improvement (dB) Heatmap...\n",
      "ğŸ“Š Generating: Real-Time Factor Heatmap...\n",
      "ğŸ“Š Generating: SNR Improvement (dB) Heatmap...\n",
      "ğŸ“Š Generating: Feature Preservation Heatmap...\n",
      "ğŸ“Š Generating: Feature Preservation Heatmap...\n",
      "\n",
      "âœ… INDIVIDUAL PLOT GENERATION COMPLETE!\n",
      "ğŸ“ Individual plots directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\individual_plots\n",
      "ğŸ“Š Generated 11 individual visualization files:\n",
      "   ğŸ“ˆ smartphone_suitability_ranking.png  - Smartphone Suitability Rankings\n",
      "   ğŸ“ˆ f1_recovery_performance.png         - F1 Recovery Performance\n",
      "   ğŸ“ˆ computational_efficiency.png        - Computational Efficiency\n",
      "   ğŸ“ˆ signal_quality_improvement.png      - Signal Quality Improvement\n",
      "   ğŸ“ˆ feature_preservation.png            - Feature Preservation\n",
      "   ğŸ“ˆ performance_vs_efficiency.png       - Performance vs Efficiency Trade-off\n",
      "   ğŸ“ˆ multi_dimensional_comparison.png    - Multi-Dimensional Method Comparison\n",
      "   ğŸ“ˆ recovery_effectiveness.png          - Recovery Effectiveness\n",
      "   ğŸ“ˆ f1_recovery_pct_heatmap.png         - F1 Recovery (%) Heatmap\n",
      "   ğŸ“ˆ snr_improvement_db_heatmap.png      - SNR Improvement (dB) Heatmap\n",
      "   ğŸ“ˆ avg_correlation_recovery_heatmap.png - Feature Preservation Heatmap\n",
      "\n",
      "ğŸ“‹ REPORT INTEGRATION GUIDE:\n",
      "   ğŸ“„ Methodology Section: Use smartphone_suitability_ranking.png\n",
      "   ğŸ“„ Results Section: Use f1_recovery_performance.png, computational_efficiency.png\n",
      "   ğŸ“„ Performance Analysis: Use signal_quality_improvement.png, feature_preservation.png\n",
      "   ğŸ“„ Trade-off Discussion: Use performance_vs_efficiency.png\n",
      "   ğŸ“„ Detailed Analysis: Use heatmap plots for condition-specific results\n",
      "   ğŸ“„ Method Comparison: Use multi_dimensional_comparison.png\n",
      "\n",
      "ğŸ“Š DOWNSCALED PHASE 3 INDIVIDUAL VISUALIZATIONS SUMMARY:\n",
      "   âœ… Generated 11 publication-ready individual plots\n",
      "   âœ… High-resolution (300 DPI) PNG format for report integration\n",
      "   âœ… Consistent styling and professional formatting\n",
      "   âœ… Each plot focuses on a specific research metric\n",
      "   âœ… Color-coded performance thresholds for interpretation\n",
      "   âœ… Value labels and legends for clarity\n",
      "\n",
      "ğŸ Individual plot generation complete!\n",
      "Time finished: 2025-07-30 23:52:30\n",
      "\n",
      "âœ… INDIVIDUAL PLOT GENERATION COMPLETE!\n",
      "ğŸ“ Individual plots directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\individual_plots\n",
      "ğŸ“Š Generated 11 individual visualization files:\n",
      "   ğŸ“ˆ smartphone_suitability_ranking.png  - Smartphone Suitability Rankings\n",
      "   ğŸ“ˆ f1_recovery_performance.png         - F1 Recovery Performance\n",
      "   ğŸ“ˆ computational_efficiency.png        - Computational Efficiency\n",
      "   ğŸ“ˆ signal_quality_improvement.png      - Signal Quality Improvement\n",
      "   ğŸ“ˆ feature_preservation.png            - Feature Preservation\n",
      "   ğŸ“ˆ performance_vs_efficiency.png       - Performance vs Efficiency Trade-off\n",
      "   ğŸ“ˆ multi_dimensional_comparison.png    - Multi-Dimensional Method Comparison\n",
      "   ğŸ“ˆ recovery_effectiveness.png          - Recovery Effectiveness\n",
      "   ğŸ“ˆ f1_recovery_pct_heatmap.png         - F1 Recovery (%) Heatmap\n",
      "   ğŸ“ˆ snr_improvement_db_heatmap.png      - SNR Improvement (dB) Heatmap\n",
      "   ğŸ“ˆ avg_correlation_recovery_heatmap.png - Feature Preservation Heatmap\n",
      "\n",
      "ğŸ“‹ REPORT INTEGRATION GUIDE:\n",
      "   ğŸ“„ Methodology Section: Use smartphone_suitability_ranking.png\n",
      "   ğŸ“„ Results Section: Use f1_recovery_performance.png, computational_efficiency.png\n",
      "   ğŸ“„ Performance Analysis: Use signal_quality_improvement.png, feature_preservation.png\n",
      "   ğŸ“„ Trade-off Discussion: Use performance_vs_efficiency.png\n",
      "   ğŸ“„ Detailed Analysis: Use heatmap plots for condition-specific results\n",
      "   ğŸ“„ Method Comparison: Use multi_dimensional_comparison.png\n",
      "\n",
      "ğŸ“Š DOWNSCALED PHASE 3 INDIVIDUAL VISUALIZATIONS SUMMARY:\n",
      "   âœ… Generated 11 publication-ready individual plots\n",
      "   âœ… High-resolution (300 DPI) PNG format for report integration\n",
      "   âœ… Consistent styling and professional formatting\n",
      "   âœ… Each plot focuses on a specific research metric\n",
      "   âœ… Color-coded performance thresholds for interpretation\n",
      "   âœ… Value labels and legends for clarity\n",
      "\n",
      "ğŸ Individual plot generation complete!\n",
      "Time finished: 2025-07-30 23:52:30\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Individual Metric Visualizations for Report Integration (Downscaled)\n",
    "print(\"ğŸ“ˆ GENERATING INDIVIDUAL METRIC VISUALIZATIONS FOR REPORT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if 'comprehensive_results' in locals() and comprehensive_results:\n",
    "    comprehensive_df = pd.DataFrame(comprehensive_results)\n",
    "    \n",
    "    # Create individual plots directory in the notebook's folder\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('phase3_downscaled_denoising_evaluation.ipynb'))\n",
    "    individual_plots_dir = os.path.join(notebook_dir, \"individual_plots\")\n",
    "    os.makedirs(individual_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Set consistent style for all plots\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    saved_plots = []\n",
    "    \n",
    "    # 1. Smartphone Suitability Score Ranking\n",
    "    print(\"ğŸ“Š Generating: Smartphone Suitability Rankings...\")\n",
    "    if 'smartphone_suitability_score' in comprehensive_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        method_scores = comprehensive_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=True)\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(method_scores)))\n",
    "        bars = ax.barh(range(len(method_scores)), method_scores.values, color=colors)\n",
    "        ax.set_yticks(range(len(method_scores)))\n",
    "        ax.set_yticklabels(method_scores.index, fontsize=11)\n",
    "        ax.set_xlabel('Smartphone Suitability Score', fontsize=12)\n",
    "        ax.set_title('Overall Smartphone Suitability Ranking\\n(Higher is Better)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, method_scores.values)):\n",
    "            ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{value:.3f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"smartphone_suitability_ranking.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"smartphone_suitability_ranking.png\", \"Smartphone Suitability Rankings\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. F1 Recovery Performance\n",
    "    print(\"ğŸ“Š Generating: F1 Recovery Performance...\")\n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        method_f1_recovery = comprehensive_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "        colors = ['#2E8B57' if x >= 75 else '#FF8C00' if x >= 50 else '#DC143C' for x in method_f1_recovery.values]\n",
    "        bars = ax.bar(range(len(method_f1_recovery)), method_f1_recovery.values, color=colors)\n",
    "        ax.set_xticks(range(len(method_f1_recovery)))\n",
    "        ax.set_xticklabels(method_f1_recovery.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('F1 Recovery (%)', fontsize=12)\n",
    "        ax.set_title('Detection Performance Recovery by Method\\n(Higher is Better)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add performance thresholds\n",
    "        ax.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Minimum Acceptable (50%)')\n",
    "        ax.axhline(y=75, color='orange', linestyle='--', alpha=0.7, label='Good Recovery (75%)')\n",
    "        ax.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='Excellent Recovery (90%)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, method_f1_recovery.values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{value:.1f}%', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"f1_recovery_performance.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"f1_recovery_performance.png\", \"F1 Recovery Performance\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Computational Efficiency\n",
    "    print(\"ğŸ“Š Generating: Computational Efficiency...\")\n",
    "    valid_efficiency = comprehensive_df[comprehensive_df['real_time_factor'].notna()] if 'real_time_factor' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_efficiency.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        method_efficiency = valid_efficiency.groupby('method_name')['real_time_factor'].mean().sort_values(ascending=False)\n",
    "        colors = ['#2E8B57' if x >= 1.0 else '#FF8C00' if x >= 0.5 else '#DC143C' for x in method_efficiency.values]\n",
    "        bars = ax.bar(range(len(method_efficiency)), method_efficiency.values, color=colors)\n",
    "        ax.set_xticks(range(len(method_efficiency)))\n",
    "        ax.set_xticklabels(method_efficiency.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('Real-Time Factor', fontsize=12)\n",
    "        ax.set_title('Computational Efficiency by Method\\n(Higher is Better, â‰¥1.0 = Real-Time Capable)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Real-Time Threshold')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, method_efficiency.values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{value:.2f}x', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"computational_efficiency.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"computational_efficiency.png\", \"Computational Efficiency\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Signal Quality Improvement (SNR)\n",
    "    print(\"ğŸ“Š Generating: Signal Quality Improvement...\")\n",
    "    valid_quality = comprehensive_df[comprehensive_df['snr_improvement_db'].notna()] if 'snr_improvement_db' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_quality.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        method_quality = valid_quality.groupby('method_name')['snr_improvement_db'].mean().sort_values(ascending=False)\n",
    "        colors = ['#2E8B57' if x >= 5 else '#FF8C00' if x >= 0 else '#DC143C' for x in method_quality.values]\n",
    "        bars = ax.bar(range(len(method_quality)), method_quality.values, color=colors)\n",
    "        ax.set_xticks(range(len(method_quality)))\n",
    "        ax.set_xticklabels(method_quality.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('SNR Improvement (dB)', fontsize=12)\n",
    "        ax.set_title('Signal Quality Enhancement by Method\\n(Higher is Better)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.7, label='No Improvement')\n",
    "        ax.axhline(y=5, color='green', linestyle='--', alpha=0.7, label='Good Improvement (5dB)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, method_quality.values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{value:.1f}dB', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"signal_quality_improvement.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"signal_quality_improvement.png\", \"Signal Quality Improvement\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Feature Preservation\n",
    "    print(\"ğŸ“Š Generating: Feature Preservation...\")\n",
    "    valid_preservation = comprehensive_df[comprehensive_df['avg_correlation_recovery'].notna()] if 'avg_correlation_recovery' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_preservation.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        method_preservation = valid_preservation.groupby('method_name')['avg_correlation_recovery'].mean().sort_values(ascending=False)\n",
    "        colors = ['#2E8B57' if x >= 0.8 else '#FF8C00' if x >= 0.5 else '#DC143C' for x in method_preservation.values]\n",
    "        bars = ax.bar(range(len(method_preservation)), method_preservation.values, color=colors)\n",
    "        ax.set_xticks(range(len(method_preservation)))\n",
    "        ax.set_xticklabels(method_preservation.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('Correlation Recovery', fontsize=12)\n",
    "        ax.set_title('Feature Preservation by Method\\n(Higher is Better)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Moderate Preservation')\n",
    "        ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Good Preservation')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, method_preservation.values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{value:.3f}', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"feature_preservation.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"feature_preservation.png\", \"Feature Preservation\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. Performance vs Efficiency Scatter Plot\n",
    "    print(\"ğŸ“Š Generating: Performance vs Efficiency Trade-off...\")\n",
    "    valid_scatter = comprehensive_df[(comprehensive_df['f1_recovery_pct'].notna()) & \n",
    "                                   (comprehensive_df['real_time_factor'].notna())] if all(col in comprehensive_df.columns for col in ['f1_recovery_pct', 'real_time_factor']) else pd.DataFrame()\n",
    "    if not valid_scatter.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        methods = valid_scatter['method_name'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_scatter[valid_scatter['method_name'] == method]\n",
    "            ax.scatter(method_data['real_time_factor'], method_data['f1_recovery_pct'], \n",
    "                      label=method, alpha=0.8, s=100, color=colors[i], edgecolors='black', linewidth=1)\n",
    "        \n",
    "        ax.set_xlabel('Real-Time Factor (Higher = Faster)', fontsize=12)\n",
    "        ax.set_ylabel('F1 Recovery (%) (Higher = Better)', fontsize=12)\n",
    "        ax.set_title('Performance vs Efficiency Trade-off\\n(Top-Right Quadrant is Optimal)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=11, loc='best')\n",
    "        \n",
    "        # Add quadrant lines\n",
    "        ax.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% Recovery Threshold')\n",
    "        ax.axvline(x=1.0, color='blue', linestyle='--', alpha=0.5, label='Real-Time Threshold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"performance_vs_efficiency.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"performance_vs_efficiency.png\", \"Performance vs Efficiency Trade-off\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Method Comparison Radar Chart (if enough methods)\n",
    "    print(\"ğŸ“Š Generating: Multi-Dimensional Method Comparison...\")\n",
    "    radar_metrics = ['f1_recovery_score', 'efficiency_score', 'signal_quality_score', 'feature_preservation_score']\n",
    "    radar_metrics_available = [col for col in radar_metrics if col in comprehensive_df.columns]\n",
    "    \n",
    "    if radar_metrics_available and len(comprehensive_df['method_name'].unique()) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        method_radar_scores = comprehensive_df.groupby('method_name')[radar_metrics_available].mean()\n",
    "        \n",
    "        if not method_radar_scores.empty:\n",
    "            method_names = method_radar_scores.index\n",
    "            x_pos = np.arange(len(radar_metrics_available))\n",
    "            width = 0.8 / len(method_names)\n",
    "            colors = plt.cm.Set2(np.linspace(0, 1, len(method_names)))\n",
    "            \n",
    "            for i, method in enumerate(method_names):\n",
    "                scores = method_radar_scores.loc[method, radar_metrics_available].values\n",
    "                bars = ax.bar(x_pos + i*width, scores, width, label=method, alpha=0.8, color=colors[i])\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, score in zip(bars, scores):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{score:.2f}', \n",
    "                           ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "            \n",
    "            ax.set_xlabel('Evaluation Dimensions', fontsize=12)\n",
    "            ax.set_ylabel('Normalized Scores (0-1)', fontsize=12)\n",
    "            ax.set_title('Multi-Dimensional Method Comparison\\n(Higher is Better in All Dimensions)', fontsize=14, fontweight='bold')\n",
    "            radar_labels = ['F1 Recovery', 'Efficiency', 'Signal Quality', 'Feature Preservation'][:len(radar_metrics_available)]\n",
    "            ax.set_xticks(x_pos + width*(len(method_names)-1)/2)\n",
    "            ax.set_xticklabels(radar_labels, fontsize=11)\n",
    "            ax.legend(fontsize=11, loc='upper left')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(individual_plots_dir, \"multi_dimensional_comparison.png\")\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            saved_plots.append((\"multi_dimensional_comparison.png\", \"Multi-Dimensional Method Comparison\"))\n",
    "            plt.close()\n",
    "    \n",
    "    # 8. Recovery vs Original Performance\n",
    "    print(\"ğŸ“Š Generating: Recovery Effectiveness...\")\n",
    "    valid_recovery = comprehensive_df[(comprehensive_df['original_f1'].notna()) & \n",
    "                                    (comprehensive_df['f1_score'].notna())] if all(col in comprehensive_df.columns for col in ['original_f1', 'f1_score']) else pd.DataFrame()\n",
    "    if not valid_recovery.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        methods = valid_recovery['method_name'].unique()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_recovery[valid_recovery['method_name'] == method]\n",
    "            ax.scatter(method_data['original_f1'], method_data['f1_score'], \n",
    "                      label=method, alpha=0.8, s=100, color=colors[i], edgecolors='black', linewidth=1)\n",
    "        \n",
    "        # Add diagonal line for reference (no improvement)\n",
    "        min_val = min(valid_recovery['original_f1'].min(), valid_recovery['f1_score'].min())\n",
    "        max_val = max(valid_recovery['original_f1'].max(), valid_recovery['f1_score'].max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2, label='No Improvement Line')\n",
    "        \n",
    "        ax.set_xlabel('Original Noisy F1-Score', fontsize=12)\n",
    "        ax.set_ylabel('Denoised F1-Score', fontsize=12)\n",
    "        ax.set_title('Recovery Effectiveness by Method\\n(Points Above Diagonal = Improvement)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(individual_plots_dir, \"recovery_effectiveness.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        saved_plots.append((\"recovery_effectiveness.png\", \"Recovery Effectiveness\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 9-12. Method Performance Heatmaps\n",
    "    heatmap_metrics = [\n",
    "        ('f1_recovery_pct', 'F1 Recovery (%) by Method and Condition'),\n",
    "        ('real_time_factor', 'Real-Time Factor by Method and Condition'),\n",
    "        ('snr_improvement_db', 'SNR Improvement (dB) by Method and Condition'),\n",
    "        ('avg_correlation_recovery', 'Feature Preservation by Method and Condition')\n",
    "    ]\n",
    "    \n",
    "    for metric, title in heatmap_metrics:\n",
    "        if metric in comprehensive_df.columns:\n",
    "            print(f\"ğŸ“Š Generating: {title.split(' by ')[0]} Heatmap...\")\n",
    "            valid_data = comprehensive_df[comprehensive_df[metric].notna()]\n",
    "            if not valid_data.empty and len(valid_data['method_name'].unique()) > 1:\n",
    "                try:\n",
    "                    pivot_data = valid_data.pivot_table(index='method_name', columns='condition_name', values=metric, aggfunc='mean')\n",
    "                    if not pivot_data.empty:\n",
    "                        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                        \n",
    "                        # Choose colormap based on metric\n",
    "                        if 'recovery' in metric or 'improvement' in metric or 'factor' in metric:\n",
    "                            cmap = 'RdYlGn'  # Red-Yellow-Green (higher is better)\n",
    "                        else:\n",
    "                            cmap = 'viridis'  # Default\n",
    "                        \n",
    "                        im = ax.imshow(pivot_data.values, cmap=cmap, aspect='auto')\n",
    "                        \n",
    "                        # Set ticks and labels\n",
    "                        ax.set_xticks(range(len(pivot_data.columns)))\n",
    "                        ax.set_yticks(range(len(pivot_data.index)))\n",
    "                        ax.set_xticklabels(pivot_data.columns, rotation=45, ha='right', fontsize=10)\n",
    "                        ax.set_yticklabels(pivot_data.index, fontsize=11)\n",
    "                        \n",
    "                        # Add value annotations\n",
    "                        for i in range(len(pivot_data.index)):\n",
    "                            for j in range(len(pivot_data.columns)):\n",
    "                                value = pivot_data.iloc[i, j]\n",
    "                                if not pd.isna(value):\n",
    "                                    text_color = 'white' if abs(value - pivot_data.values.mean()) > pivot_data.values.std() else 'black'\n",
    "                                    ax.text(j, i, f'{value:.2f}', ha='center', va='center', \n",
    "                                           color=text_color, fontsize=9, fontweight='bold')\n",
    "                        \n",
    "                        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "                        ax.set_xlabel('Noise Condition', fontsize=12)\n",
    "                        ax.set_ylabel('Denoising Method', fontsize=12)\n",
    "                        \n",
    "                        # Add colorbar\n",
    "                        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "                        cbar.set_label(metric.replace('_', ' ').title(), fontsize=11)\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # Clean filename\n",
    "                        clean_metric = metric.replace('_', '_').replace('%', 'pct')\n",
    "                        plot_path = os.path.join(individual_plots_dir, f\"{clean_metric}_heatmap.png\")\n",
    "                        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                        saved_plots.append((f\"{clean_metric}_heatmap.png\", f\"{title.split(' by ')[0]} Heatmap\"))\n",
    "                        plt.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸ Could not generate heatmap for {metric}: {str(e)}\")\n",
    "    \n",
    "    # Summary Report\n",
    "    print(f\"\\nâœ… INDIVIDUAL PLOT GENERATION COMPLETE!\")\n",
    "    print(f\"ğŸ“ Individual plots directory: {individual_plots_dir}\")\n",
    "    print(f\"ğŸ“Š Generated {len(saved_plots)} individual visualization files:\")\n",
    "    \n",
    "    for filename, description in saved_plots:\n",
    "        print(f\"   ğŸ“ˆ {filename:<35} - {description}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ REPORT INTEGRATION GUIDE:\")\n",
    "    print(f\"   ğŸ“„ Methodology Section: Use smartphone_suitability_ranking.png\")\n",
    "    print(f\"   ğŸ“„ Results Section: Use f1_recovery_performance.png, computational_efficiency.png\")\n",
    "    print(f\"   ğŸ“„ Performance Analysis: Use signal_quality_improvement.png, feature_preservation.png\")\n",
    "    print(f\"   ğŸ“„ Trade-off Discussion: Use performance_vs_efficiency.png\")\n",
    "    print(f\"   ğŸ“„ Detailed Analysis: Use heatmap plots for condition-specific results\")\n",
    "    print(f\"   ğŸ“„ Method Comparison: Use multi_dimensional_comparison.png\")\n",
    "    \n",
    "    # Generate final summary\n",
    "    print(f\"\\nğŸ“Š DOWNSCALED PHASE 3 INDIVIDUAL VISUALIZATIONS SUMMARY:\")\n",
    "    print(f\"   âœ… Generated {len(saved_plots)} publication-ready individual plots\")\n",
    "    print(f\"   âœ… High-resolution (300 DPI) PNG format for report integration\")\n",
    "    print(f\"   âœ… Consistent styling and professional formatting\")\n",
    "    print(f\"   âœ… Each plot focuses on a specific research metric\")\n",
    "    print(f\"   âœ… Color-coded performance thresholds for interpretation\")\n",
    "    print(f\"   âœ… Value labels and legends for clarity\")\n",
    "    print(f\"   ğŸ“ All plots saved in: {individual_plots_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Individual visualization generation skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "\n",
    "print(f\"\\nğŸ Individual plot generation complete!\")\n",
    "print(f\"Time finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
