{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97226e8",
   "metadata": {},
   "source": [
    "# Phase 3: Downscaled Comprehensive Denoising Method Evaluation\n",
    "\n",
    "## Research Objective\n",
    "Systematically evaluate 3 denoising methods across 4 dimensions to determine optimal approaches for smartphone-based sleep apnea detection under realistic noise conditions using **downscaled sampling for faster execution**.\n",
    "\n",
    "## This Notebook:\n",
    "1. **Downscaled Sampling Focus**: Evaluate **20 randomly sampled files per condition** for rapid prototyping\n",
    "2. **Multi-Method Denoising**: Apply 3 denoising techniques to representative priority conditions\n",
    "3. **Four-Dimensional Evaluation**: Performance recovery, signal quality, computational efficiency, feature preservation\n",
    "4. **Smartphone Suitability Scoring**: Weighted composite metrics for deployment decisions\n",
    "5. **Method Ranking**: Evidence-based recommendations for mobile health applications\n",
    "\n",
    "## Denoising Methods Under Evaluation:\n",
    "- **Spectral Subtraction**: Fast, lightweight, potential musical noise artifacts\n",
    "- **Wiener Filtering**: Balanced statistical approach with moderate complexity\n",
    "- **LogMMSE**: Advanced statistical method with better artifact control\n",
    "\n",
    "## Representative Test Conditions (5dB SNR - Worst Case):\n",
    "- **patient_01_wav_5db_vacuum_cleaner**: Mechanical high-frequency noise\n",
    "- **patient_01_wav_5db_cat**: Animal organic sounds\n",
    "- **patient_01_wav_5db_door_wood_creaks**: Structural low-frequency noise\n",
    "- **patient_01_wav_5db_crying_baby**: Human vocal interference\n",
    "- **patient_01_wav_5db_coughing**: Respiratory interference (most challenging)\n",
    "\n",
    "## Downscaled Optimization:\n",
    "- **Sample Size**: 20 randomly selected files per condition (vs 1,168 full dataset)\n",
    "- **Total Evaluations**: 5 conditions √ó 3 methods = 15 evaluations (~30 minutes)\n",
    "- **Strategy**: Rapid prototyping and method comparison for proof-of-concept\n",
    "- **Scientific Validity**: Random sampling maintains representativeness for comparative analysis\n",
    "\n",
    "## Expected Outcomes:\n",
    "- Recovery targets: 50% (minimum), 75% (good), 90% (excellent), 100% (perfect)\n",
    "- Computational trade-offs: Traditional signal processing methods comparison\n",
    "- Feature preservation analysis: Which methods maintain breathing biomarkers\n",
    "- Smartphone deployment recommendations: Optimal method per use case\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\n",
      "‚úÖ Configuration loaded:\n",
      "   üìÅ Base data directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\n",
      "   ü§ñ Model path: ../models/sleep_apnea_model.pkl\n",
      "   üîä Denoising methods: 3 traditional methods\n",
      "   üìä Representative conditions: 5\n",
      "   üìä Output directories created\n",
      "   ‚ö° DOWNSCALED OPTIMIZATION: 20 files per condition\n",
      "   üéØ Total evaluations: 5 √ó 3 = 15\n",
      "   üöÄ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\n",
      "   üé≤ Random seed set: 42 (reproducible sampling)\n",
      "‚úÖ Configuration loaded:\n",
      "   üìÅ Base data directory: F:/Solo All In One Docs/Scidb Sleep Data/processed\n",
      "   ü§ñ Model path: ../models/sleep_apnea_model.pkl\n",
      "   üîä Denoising methods: 3 traditional methods\n",
      "   üìä Representative conditions: 5\n",
      "   üìä Output directories created\n",
      "   ‚ö° DOWNSCALED OPTIMIZATION: 20 files per condition\n",
      "   üéØ Total evaluations: 5 √ó 3 = 15\n",
      "   üöÄ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\n",
      "   üé≤ Random seed set: 42 (reproducible sampling)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "print(\"=== Phase 3: Downscaled Comprehensive Denoising Method Evaluation ===\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_score, recall_score, accuracy_score\n",
    ")\n",
    "import joblib\n",
    "import random\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration paths\n",
    "BASE_DATA_DIR = \"F:/Solo All In One Docs/Scidb Sleep Data/processed\"\n",
    "MODEL_PATH = \"../models/sleep_apnea_model.pkl\"\n",
    "PHASE2_RESULTS_PATH = os.path.join(BASE_DATA_DIR, \"noise_evaluation_results.csv\")\n",
    "PHASE3_CONFIG_PATH = os.path.join(BASE_DATA_DIR, \"phase3_preparation_config.json\")\n",
    "CLEAN_BASELINE_PATH = os.path.join(BASE_DATA_DIR, \"clean_audio_baseline_results.json\")\n",
    "\n",
    "# Denoising methods configuration (3 traditional methods for speed)\n",
    "DENOISING_METHODS = {\n",
    "    'spectral_subtraction': {\n",
    "        'script': '../src/spec_subtraction_same_file.py',\n",
    "        'name': 'Spectral Subtraction',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'moderate'\n",
    "    },\n",
    "    'wiener_filtering': {\n",
    "        'script': '../src/wiener_filtering.py',\n",
    "        'name': 'Wiener Filtering',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'high',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'logmmse': {\n",
    "        'script': '../src/log_mmse.py',\n",
    "        'name': 'LogMMSE',\n",
    "        'category': 'traditional',\n",
    "        'expected_efficiency': 'moderate',\n",
    "        'expected_quality': 'good'\n",
    "    },\n",
    "    'deepfilternet': {\n",
    "    'script': '../src/denoise_with_deepfilternet.py',\n",
    "    'name': 'DeepFilterNet',\n",
    "    'category': 'deep_learning',\n",
    "    'expected_efficiency': 'low',\n",
    "    'expected_quality': 'excellent'\n",
    "}\n",
    "}\n",
    "\n",
    "# Representative conditions from Phase 2 (5dB worst-case analysis)\n",
    "REPRESENTATIVE_CONDITIONS = [\n",
    "    'patient_01_wav_5db_vacuum_cleaner',    # Mechanical high-frequency noise\n",
    "    'patient_01_wav_5db_cat',               # Animal organic sounds  \n",
    "    'patient_01_wav_5db_door_wood_creaks',  # Structural low-frequency noise\n",
    "    'patient_01_wav_5db_crying_baby',       # Human vocal interference\n",
    "    'patient_01_wav_5db_coughing'           # Respiratory interference\n",
    "]\n",
    "\n",
    "# Audio processing settings (consistent with Phase 1 & 2)\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_DURATION = 30.0\n",
    "\n",
    "# DOWNSCALED SAMPLING CONFIGURATION\n",
    "SAMPLE_SIZE_PER_CONDITION = 20  # Fixed 20 files per condition\n",
    "RANDOM_SEED = 42  # For reproducible sampling\n",
    "\n",
    "# Create output directories\n",
    "DENOISED_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"denoised_audio_downscaled\")\n",
    "RESULTS_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"phase3_downscaled_results\")\n",
    "os.makedirs(DENOISED_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   üìÅ Base data directory: {BASE_DATA_DIR}\")\n",
    "print(f\"   ü§ñ Model path: {MODEL_PATH}\")\n",
    "print(f\"   üîä Denoising methods: {len(DENOISING_METHODS)} traditional methods\")\n",
    "print(f\"   üìä Representative conditions: {len(REPRESENTATIVE_CONDITIONS)}\")\n",
    "print(f\"   üìä Output directories created\")\n",
    "print(f\"   ‚ö° DOWNSCALED OPTIMIZATION: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   üéØ Total evaluations: {len(REPRESENTATIVE_CONDITIONS)} √ó {len(DENOISING_METHODS)} = {len(REPRESENTATIVE_CONDITIONS) * len(DENOISING_METHODS)}\")\n",
    "print(f\"   üöÄ Expected execution time: ~30 minutes (vs 2+ hours full sampling)\")\n",
    "\n",
    "# Set random seed for reproducible sampling\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   üé≤ Random seed set: {RANDOM_SEED} (reproducible sampling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd1c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature extraction function loaded (27 features matching training pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction function (same 27 features as training pipeline)\n",
    "def extract_comprehensive_features(audio_frame, sample_rate):\n",
    "    \"\"\"Extract the same 27 features used in training pipeline\"\"\"\n",
    "    try:\n",
    "        if len(audio_frame) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Basic acoustic features\n",
    "        rms = float(librosa.feature.rms(y=audio_frame).mean())\n",
    "        zcr = float(librosa.feature.zero_crossing_rate(y=audio_frame).mean())\n",
    "        centroid = float(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n",
    "        bandwidth = float(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n",
    "        rolloff = float(librosa.feature.spectral_rolloff(y=audio_frame, sr=sample_rate).mean())\n",
    "        \n",
    "        # MFCCs (first 8 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=8)\n",
    "        mfcc_means = mfccs.mean(axis=1)\n",
    "        mfcc_stds = mfccs.std(axis=1)\n",
    "        \n",
    "        # Temporal features for breathing patterns (5-second windows)\n",
    "        window_size = int(5 * sample_rate)  # 5 seconds\n",
    "        num_windows = len(audio_frame) // window_size\n",
    "        \n",
    "        if num_windows >= 2:\n",
    "            rms_windows = []\n",
    "            zcr_windows = []\n",
    "            \n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window = audio_frame[start_idx:end_idx]\n",
    "                \n",
    "                rms_windows.append(librosa.feature.rms(y=window).mean())\n",
    "                zcr_windows.append(librosa.feature.zero_crossing_rate(y=window).mean())\n",
    "            \n",
    "            rms_variability = float(np.std(rms_windows))\n",
    "            zcr_variability = float(np.std(zcr_windows))\n",
    "            breathing_regularity = float(1.0 / (1.0 + rms_variability))  # Higher = more regular\n",
    "        else:\n",
    "            rms_variability = 0.0\n",
    "            zcr_variability = 0.0\n",
    "            breathing_regularity = 0.5\n",
    "        \n",
    "        # Silence detection\n",
    "        silence_threshold = np.percentile(np.abs(audio_frame), 20)  # Bottom 20% as silence\n",
    "        silence_mask = np.abs(audio_frame) < silence_threshold\n",
    "        silence_ratio = float(np.mean(silence_mask))\n",
    "        \n",
    "        # Breathing pause detection (continuous silence periods)\n",
    "        silence_changes = np.diff(silence_mask.astype(int))\n",
    "        pause_starts = np.where(silence_changes == 1)[0]\n",
    "        pause_ends = np.where(silence_changes == -1)[0]\n",
    "        \n",
    "        if len(pause_starts) > 0 and len(pause_ends) > 0:\n",
    "            if len(pause_ends) < len(pause_starts):\n",
    "                pause_ends = np.append(pause_ends, len(audio_frame))\n",
    "            pause_durations = (pause_ends[:len(pause_starts)] - pause_starts) / sample_rate\n",
    "            avg_pause_duration = float(np.mean(pause_durations))\n",
    "            max_pause_duration = float(np.max(pause_durations))\n",
    "        else:\n",
    "            avg_pause_duration = 0.0\n",
    "            max_pause_duration = 0.0\n",
    "        \n",
    "        # Combine all features (same structure as training)\n",
    "        features = {\n",
    "            'clean_rms': rms,\n",
    "            'clean_zcr': zcr,\n",
    "            'clean_centroid': centroid,\n",
    "            'clean_bandwidth': bandwidth,\n",
    "            'clean_rolloff': rolloff,\n",
    "            'clean_rms_variability': rms_variability,\n",
    "            'clean_zcr_variability': zcr_variability,\n",
    "            'clean_breathing_regularity': breathing_regularity,\n",
    "            'clean_silence_ratio': silence_ratio,\n",
    "            'clean_avg_pause_duration': avg_pause_duration,\n",
    "            'clean_max_pause_duration': max_pause_duration\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs\n",
    "        for i, (mean_val, std_val) in enumerate(zip(mfcc_means, mfcc_stds), 1):\n",
    "            features[f'clean_mfcc_{i}_mean'] = float(mean_val)\n",
    "            features[f'clean_mfcc_{i}_std'] = float(std_val)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Feature extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Feature extraction function loaded (27 features matching training pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b6063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\n",
      "======================================================================\n",
      "‚úÖ Phase 2 results loaded: 5 noise conditions evaluated\n",
      "\n",
      "üìà Phase 2 Performance Summary:\n",
      "   F1-Score Range: 0.000 - 0.218\n",
      "   Average F1-Score: 0.051 (¬±0.095)\n",
      "   Average Degradation: 93.3% (¬±12.5%)\n",
      "\n",
      "‚úÖ Clean baseline loaded: F1=0.758\n",
      "\n",
      "‚úÖ Downscaled sampling functions loaded\n",
      "   üìè Sample size: 20 files per condition\n",
      "   üé≤ Random seed: 42 (reproducible results)\n",
      "   üìÅ Temporary directories will be created for each method-condition combination\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Phase 2 Results and Create Downscaled Sampling Function\n",
    "print(\"üìä LOADING PHASE 2 RESULTS AND SETTING UP DOWNSCALED SAMPLING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load Phase 2 evaluation results\n",
    "try:\n",
    "    phase2_results = pd.read_csv(PHASE2_RESULTS_PATH)\n",
    "    print(f\"‚úÖ Phase 2 results loaded: {len(phase2_results)} noise conditions evaluated\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\nüìà Phase 2 Performance Summary:\")\n",
    "    print(f\"   F1-Score Range: {phase2_results['f1_score'].min():.3f} - {phase2_results['f1_score'].max():.3f}\")\n",
    "    print(f\"   Average F1-Score: {phase2_results['f1_score'].mean():.3f} (¬±{phase2_results['f1_score'].std():.3f})\")\n",
    "    print(f\"   Average Degradation: {phase2_results['f1_degradation_pct'].mean():.1f}% (¬±{phase2_results['f1_degradation_pct'].std():.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load Phase 2 results: {e}\")\n",
    "    print(f\"   Will proceed with fallback configuration\")\n",
    "    phase2_results = None\n",
    "\n",
    "# Load clean baseline for reference\n",
    "try:\n",
    "    with open(CLEAN_BASELINE_PATH, 'r') as f:\n",
    "        clean_baseline = json.load(f)\n",
    "    print(f\"\\n‚úÖ Clean baseline loaded: F1={clean_baseline['clean_f1_score']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load clean baseline: {e}\")\n",
    "    clean_baseline = {'clean_f1_score': 0.672}  # Fallback value from research plan\n",
    "\n",
    "# Downscaled sampling function for 20 files per condition\n",
    "def sample_files_downscaled(input_dir, sample_size=20):\n",
    "    \"\"\"\n",
    "    Randomly sample a fixed number of files from condition directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing audio files\n",
    "        sample_size: Number of files to sample (default 20)\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled filenames (sorted for reproducibility)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(input_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        if len(all_files) == 0:\n",
    "            print(f\"      ‚ùå No WAV files found in {input_dir}\")\n",
    "            return []\n",
    "        \n",
    "        if len(all_files) <= sample_size:\n",
    "            print(f\"      üìä Using all {len(all_files)} files (less than sample size)\")\n",
    "            return sorted(all_files)\n",
    "        \n",
    "        # Random sampling with fixed seed for reproducibility\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "        \n",
    "        percentage_actual = (sample_size / len(all_files)) * 100\n",
    "        print(f\"      üìä Sampled {sample_size} files ({percentage_actual:.1f}%) from {len(all_files)} total files\")\n",
    "        \n",
    "        return sorted(sampled_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Sampling failed for {input_dir}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_temp_sample_directory(source_dir, sampled_files, temp_suffix):\n",
    "    \"\"\"\n",
    "    Create temporary directory with only sampled files for denoising\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Source directory containing all files\n",
    "        sampled_files: List of filenames to copy\n",
    "        temp_suffix: Unique suffix for temp directory\n",
    "    \n",
    "    Returns:\n",
    "        Path to temporary directory\n",
    "    \"\"\"\n",
    "    temp_dir = f\"{source_dir}_temp_downscaled_{temp_suffix}\"\n",
    "    \n",
    "    # Clean up any existing temp directory\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy sampled files to temp directory\n",
    "    copied_count = 0\n",
    "    for filename in sampled_files:\n",
    "        src = os.path.join(source_dir, filename)\n",
    "        dst = os.path.join(temp_dir, filename)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied_count += 1\n",
    "    \n",
    "    print(f\"      üìÅ Created temp sample directory: {copied_count} files copied\")\n",
    "    return temp_dir\n",
    "\n",
    "print(f\"\\n‚úÖ Downscaled sampling functions loaded\")\n",
    "print(f\"   üìè Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "print(f\"   üé≤ Random seed: {RANDOM_SEED} (reproducible results)\")\n",
    "print(f\"   üìÅ Temporary directories will be created for each method-condition combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\n",
      "============================================================\n",
      "\n",
      "üéØ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\n",
      "üìâ Representative Conditions (5dB worst-case per noise category):\n",
      "   patient_01_wav_5db_vacuum_cleaner: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_cat: F1=0.036 (-95.2%)\n",
      "   patient_01_wav_5db_door_wood_creaks: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_crying_baby: F1=0.000 (-100.0%)\n",
      "   patient_01_wav_5db_coughing: F1=0.218 (-71.2%)\n",
      "\n",
      "üìÅ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\n",
      "   ‚úÖ patient_01_wav_5db_vacuum_cleaner: 1168 files available, will sample 20\n",
      "   ‚úÖ patient_01_wav_5db_cat: 1168 files available, will sample 20\n",
      "   ‚úÖ patient_01_wav_5db_door_wood_creaks: 1168 files available, will sample 20\n",
      "   ‚úÖ patient_01_wav_5db_crying_baby: 1168 files available, will sample 20\n",
      "   ‚úÖ patient_01_wav_5db_coughing: 1168 files available, will sample 20\n",
      "\n",
      "üíæ Verified priority conditions saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_priority_conditions.csv\n",
      "\n",
      "‚úÖ Priority condition verification complete:\n",
      "   üìä Verified conditions: 5\n",
      "   üéØ Sample size per condition: 20 files\n",
      "   üìà Total files to process: 300\n",
      "   ‚è±Ô∏è  Estimated execution time: ~30 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Select Priority Conditions and Prepare for Downscaled Evaluation\n",
    "print(\"üéØ SELECTING PRIORITY CONDITIONS FOR DOWNSCALED EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Select priority conditions for Phase 3 evaluation\n",
    "priority_conditions = None\n",
    "\n",
    "if phase2_results is not None:\n",
    "    # Strategy: Select representative conditions across noise types at 5dB (worst-case)\n",
    "    representative_conditions = []\n",
    "    \n",
    "    # Filter for 5dB conditions and get worst-performing per noise category\n",
    "    conditions_5db = phase2_results[phase2_results['condition_name'].str.contains('_5db_')]\n",
    "    \n",
    "    if not conditions_5db.empty:\n",
    "        for condition_name in REPRESENTATIVE_CONDITIONS:\n",
    "            condition_match = conditions_5db[conditions_5db['condition_name'] == condition_name]\n",
    "            if not condition_match.empty:\n",
    "                representative_conditions.append(condition_match.iloc[0])\n",
    "        \n",
    "        if representative_conditions:\n",
    "            priority_conditions = pd.DataFrame(representative_conditions)\n",
    "            print(f\"\\nüéØ PRIORITY CONDITIONS SELECTED FOR DOWNSCALED EVALUATION:\")\n",
    "            print(f\"üìâ Representative Conditions (5dB worst-case per noise category):\")\n",
    "            for idx, row in priority_conditions.iterrows():\n",
    "                print(f\"   {row['condition_name']}: F1={row['f1_score']:.3f} (-{row['f1_degradation_pct']:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No matching representative conditions found in Phase 2 results\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No 5dB conditions found in Phase 2 results\")\n",
    "\n",
    "if priority_conditions is None:\n",
    "    print(f\"‚ö†Ô∏è  Using fallback representative conditions from configuration\")\n",
    "    # Create fallback priority conditions DataFrame\n",
    "    priority_conditions = pd.DataFrame({\n",
    "        'condition_name': REPRESENTATIVE_CONDITIONS,\n",
    "        'f1_score': [0.400] * len(REPRESENTATIVE_CONDITIONS),  # Estimated based on 5dB degradation\n",
    "        'f1_degradation_pct': [47.2] * len(REPRESENTATIVE_CONDITIONS)  # Estimated degradation\n",
    "    })\n",
    "    print(f\"   Using {len(REPRESENTATIVE_CONDITIONS)} representative conditions as fallback\")\n",
    "\n",
    "# Verify condition directories exist and count available files\n",
    "print(f\"\\nüìÅ VERIFYING CONDITION DIRECTORIES AND FILE COUNTS:\")\n",
    "verified_conditions = []\n",
    "\n",
    "for _, condition_row in priority_conditions.iterrows():\n",
    "    condition_name = condition_row['condition_name']\n",
    "    condition_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "    \n",
    "    if os.path.exists(condition_dir):\n",
    "        wav_files = [f for f in os.listdir(condition_dir) if f.endswith('.wav')]\n",
    "        file_count = len(wav_files)\n",
    "        \n",
    "        if file_count > 0:\n",
    "            verified_conditions.append(condition_row)\n",
    "            sample_count = min(SAMPLE_SIZE_PER_CONDITION, file_count)\n",
    "            print(f\"   ‚úÖ {condition_name}: {file_count} files available, will sample {sample_count}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {condition_name}: No WAV files found\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {condition_name}: Directory not found at {condition_dir}\")\n",
    "\n",
    "if verified_conditions:\n",
    "    priority_conditions = pd.DataFrame(verified_conditions)\n",
    "    \n",
    "    # Save priority conditions for reference\n",
    "    priority_conditions_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_priority_conditions.csv\")\n",
    "    priority_conditions.to_csv(priority_conditions_path, index=False)\n",
    "    print(f\"\\nüíæ Verified priority conditions saved: {priority_conditions_path}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Priority condition verification complete:\")\n",
    "    print(f\"   üìä Verified conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   üéØ Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"   üìà Total files to process: {len(priority_conditions) * SAMPLE_SIZE_PER_CONDITION * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   ‚è±Ô∏è  Estimated execution time: ~30 minutes\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå No verified conditions available for evaluation\")\n",
    "    print(f\"   Please check that Phase 2 noise injection has been completed\")\n",
    "    priority_conditions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c008f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\n",
      "============================================================\n",
      "‚úÖ Model loaded (direct): ../models/sleep_apnea_model.pkl\n",
      "üìä Model type: RandomForestClassifier\n",
      "‚úÖ Audio metadata loaded: 10972 records (whitespace cleaned)\n",
      "\n",
      "‚úÖ Evaluation framework ready for downscaled processing\n",
      "‚úÖ Model loaded (direct): ../models/sleep_apnea_model.pkl\n",
      "üìä Model type: RandomForestClassifier\n",
      "‚úÖ Audio metadata loaded: 10972 records (whitespace cleaned)\n",
      "\n",
      "‚úÖ Evaluation framework ready for downscaled processing\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model and Prepare Evaluation Framework\n",
    "print(\"ü§ñ LOADING MODEL AND PREPARING EVALUATION FRAMEWORK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    model_data = joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if isinstance(model_data, dict):\n",
    "        model = model_data['model']\n",
    "        feature_columns = model_data.get('feature_columns', None)\n",
    "        print(f\"‚úÖ Model loaded from: {MODEL_PATH}\")\n",
    "        print(f\"üìä Model type: {type(model).__name__}\")\n",
    "        if feature_columns:\n",
    "            print(f\"üéØ Expected features: {len(feature_columns)}\")\n",
    "    else:\n",
    "        # Fallback if model is saved directly\n",
    "        model = model_data\n",
    "        feature_columns = None\n",
    "        print(f\"‚úÖ Model loaded (direct): {MODEL_PATH}\")\n",
    "        print(f\"üìä Model type: {type(model).__name__}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    model = None\n",
    "    feature_columns = None\n",
    "\n",
    "# Performance evaluation function optimized for downscaled processing\n",
    "def evaluate_denoised_audio_downscaled(denoised_audio_dir, condition_name, method_name, model, feature_columns, audio_metadata):\n",
    "    \"\"\"Evaluate model performance on denoised audio with downscaled sampling\"\"\"\n",
    "    \n",
    "    print(f\"   üìä Evaluating: {method_name} on {condition_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get WAV files in the denoised directory\n",
    "        if not os.path.exists(denoised_audio_dir):\n",
    "            print(f\"      ‚ùå Directory not found: {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        wav_files = [f for f in os.listdir(denoised_audio_dir) if f.lower().endswith('.wav')]\n",
    "        if not wav_files:\n",
    "            print(f\"      ‚ùå No WAV files found in {denoised_audio_dir}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"      üéµ Processing {len(wav_files)} denoised audio files...\")\n",
    "        \n",
    "        # Apply whitespace fix to metadata if available\n",
    "        if audio_metadata is not None and 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "        \n",
    "        # Extract features and get labels with progress monitoring\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        mismatch_count = 0\n",
    "        \n",
    "        for i, wav_file in enumerate(wav_files):\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                wav_path = os.path.join(denoised_audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features is None:\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get corresponding label from metadata\n",
    "                original_filename = wav_file.replace('mixed_', '').replace('denoised_', '').strip()\n",
    "                \n",
    "                if audio_metadata is not None:\n",
    "                    # Find matching metadata record\n",
    "                    metadata_match = audio_metadata[audio_metadata['wav_file'] == original_filename]\n",
    "                    if not metadata_match.empty:\n",
    "                        label = metadata_match.iloc[0]['apnea_label']\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(label)\n",
    "                        processed_count += 1\n",
    "                    else:\n",
    "                        mismatch_count += 1\n",
    "                        if mismatch_count <= 2:  # Show first 2 mismatches\n",
    "                            print(f\"      ‚ö†Ô∏è  No metadata match for: '{original_filename}'\")\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                if failed_count <= 2:  # Show first 2 errors\n",
    "                    print(f\"      ‚ö†Ô∏è  Error processing {wav_file}: {e}\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"      üìä Processed: {processed_count}, Failed: {failed_count}, Mismatches: {mismatch_count}\")\n",
    "        \n",
    "        if processed_count == 0:\n",
    "            print(f\"      ‚ùå No files processed successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to DataFrame and make predictions\n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        labels = np.array(labels_list)\n",
    "        \n",
    "        # Ensure feature order matches training\n",
    "        if feature_columns:\n",
    "            features_df = features_df.reindex(columns=feature_columns, fill_value=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features_df)\n",
    "        prediction_probas = model.predict_proba(features_df)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)  # Sensitivity\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        # Confusion matrix for specificity\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'num_samples': processed_count,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall_sensitivity': recall,\n",
    "            'specificity': specificity,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn),\n",
    "            'sample_size_note': 'downscaled_20_files'\n",
    "        }\n",
    "        \n",
    "        print(f\"      ‚úÖ {method_name}: F1={f1:.3f}, Sens={recall:.3f}, Spec={specificity:.3f} [n={processed_count}]\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Computational efficiency measurement function for downscaled processing\n",
    "def measure_denoising_efficiency_downscaled(input_dir, output_dir, method_script, method_name):\n",
    "    \"\"\"Measure computational efficiency of denoising method on downscaled sample\"\"\"\n",
    "    \n",
    "    print(f\"   ‚è±Ô∏è  Measuring efficiency for {method_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get input file count\n",
    "        input_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.wav')]\n",
    "        print(f\"      üìÅ Input files to process: {len(input_files)}\")\n",
    "        \n",
    "        # Get system resources before\n",
    "        process = psutil.Process()\n",
    "        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Measure processing time\n",
    "        start_time = time.time()\n",
    "        print(f\"      üöÄ Starting denoising at {time.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # Run denoising method\n",
    "        cmd = [sys.executable, method_script, '--input', input_dir, '--output', output_dir]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # 5 minute timeout\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"      üèÅ Processing completed in {processing_time:.1f}s\")\n",
    "        \n",
    "        # Get system resources after\n",
    "        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        if os.path.exists(output_dir):\n",
    "            output_files = [f for f in os.listdir(output_dir) if f.lower().endswith('.wav')]\n",
    "            num_files_processed = len(output_files)\n",
    "            \n",
    "            # Estimate total audio duration (assuming 30-second files)\n",
    "            total_audio_duration = num_files_processed * 30.0  # seconds\n",
    "            real_time_factor = total_audio_duration / processing_time if processing_time > 0 else 0\n",
    "            \n",
    "            efficiency_metrics = {\n",
    "                'method_name': method_name,\n",
    "                'processing_time_sec': processing_time,\n",
    "                'files_processed': num_files_processed,\n",
    "                'total_audio_duration_sec': total_audio_duration,\n",
    "                'real_time_factor': real_time_factor,\n",
    "                'memory_usage_mb': memory_after - memory_before,\n",
    "                'peak_memory_mb': memory_after,\n",
    "                'processing_speed_files_per_sec': num_files_processed / processing_time if processing_time > 0 else 0,\n",
    "                'success': result.returncode == 0,\n",
    "                'sample_size_note': 'downscaled_20_files'\n",
    "            }\n",
    "            \n",
    "            print(f\"      ‚ö° {method_name}: {processing_time:.1f}s, {real_time_factor:.2f}x RT, {num_files_processed} files\")\n",
    "            return efficiency_metrics\n",
    "            \n",
    "        else:\n",
    "            print(f\"      ‚ùå {method_name}: Output directory not created\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"      ‚ùå {method_name}: Processing timeout (>5 minutes)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Efficiency measurement failed for {method_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load audio metadata from Phase 1 for label matching\n",
    "try:\n",
    "    metadata_path = os.path.join(BASE_DATA_DIR, \"audio_metadata.csv\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        audio_metadata = pd.read_csv(metadata_path)\n",
    "        # Apply whitespace stripping immediately upon loading\n",
    "        if 'wav_file' in audio_metadata.columns:\n",
    "            audio_metadata['wav_file'] = audio_metadata['wav_file'].str.strip()\n",
    "            print(f\"‚úÖ Audio metadata loaded: {len(audio_metadata)} records (whitespace cleaned)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Audio metadata loaded: {len(audio_metadata)} records\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Audio metadata not found at {metadata_path}\")\n",
    "        audio_metadata = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load audio metadata: {e}\")\n",
    "    audio_metadata = None\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation framework ready for downscaled processing\")\n",
    "if model is None:\n",
    "    print(f\"‚ö†Ô∏è  Model loading failed - evaluation will be limited\")\n",
    "if audio_metadata is None:\n",
    "    print(f\"‚ö†Ô∏è  Audio metadata missing - label matching may fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04fb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\n",
      "Time started: 2025-07-30 21:55:07\n",
      "================================================================================\n",
      "üöÄ DOWNSCALED PROCESSING CONFIGURATION:\n",
      "   üéØ Conditions: 5\n",
      "   üîß Methods: 3\n",
      "   üìä Sample size: 20 files per condition\n",
      "   üìà Total evaluations: 15\n",
      "   ‚è±Ô∏è  Expected time: ~30 minutes\n",
      "\n",
      "üìç Condition 1/5: patient_01_wav_5db_vacuum_cleaner\n",
      "   üìâ Original performance: F1=0.000 (-100.0%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:07\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:07\n",
      "      üèÅ Processing completed in 7.3s\n",
      "      ‚ö° Spectral Subtraction: 7.3s, 82.17x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 7.3s\n",
      "      ‚ö° Spectral Subtraction: 7.3s, 82.17x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:28\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:28\n",
      "      üèÅ Processing completed in 5.6s\n",
      "      ‚ö° Wiener Filtering: 5.6s, 106.82x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.6s\n",
      "      ‚ö° Wiener Filtering: 5.6s, 106.82x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:43\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:55:43\n",
      "      üèÅ Processing completed in 17.1s\n",
      "      ‚ö° LogMMSE: 17.1s, 35.01x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 17.1s\n",
      "      ‚ö° LogMMSE: 17.1s, 35.01x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_vacuum_cleaner\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "üìç Condition 2/5: patient_01_wav_5db_cat\n",
      "   üìâ Original performance: F1=0.036 (-95.2%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 1/5, ETA: 0.0 minutes\n",
      "\n",
      "üìç Condition 2/5: patient_01_wav_5db_cat\n",
      "   üìâ Original performance: F1=0.036 (-95.2%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:11\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:11\n",
      "      üèÅ Processing completed in 5.6s\n",
      "      ‚ö° Spectral Subtraction: 5.6s, 108.02x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.6s\n",
      "      ‚ö° Spectral Subtraction: 5.6s, 108.02x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:26\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:26\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Wiener Filtering: 5.7s, 105.69x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Wiener Filtering: 5.7s, 105.69x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_cat...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:41\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:56:41\n",
      "      üèÅ Processing completed in 12.8s\n",
      "      ‚ö° LogMMSE: 12.8s, 46.89x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 12.8s\n",
      "      ‚ö° LogMMSE: 12.8s, 46.89x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_cat\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 2/5, ETA: 2.9 minutes\n",
      "\n",
      "üìç Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   üìâ Original performance: F1=0.000 (-100.0%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=-5.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 2/5, ETA: 2.9 minutes\n",
      "\n",
      "üìç Condition 3/5: patient_01_wav_5db_door_wood_creaks\n",
      "   üìâ Original performance: F1=0.000 (-100.0%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:05\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:05\n",
      "      üèÅ Processing completed in 6.6s\n",
      "      ‚ö° Spectral Subtraction: 6.6s, 90.65x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 6.6s\n",
      "      ‚ö° Spectral Subtraction: 6.6s, 90.65x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:22\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:22\n",
      "      üèÅ Processing completed in 5.8s\n",
      "      ‚ö° Wiener Filtering: 5.8s, 103.93x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.8s\n",
      "      ‚ö° Wiener Filtering: 5.8s, 103.93x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:38\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:57:38\n",
      "      üèÅ Processing completed in 14.5s\n",
      "      ‚ö° LogMMSE: 14.5s, 41.27x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 14.5s\n",
      "      ‚ö° LogMMSE: 14.5s, 41.27x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_door_wood_creaks\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 3/5, ETA: 1.9 minutes\n",
      "\n",
      "üìç Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   üìâ Original performance: F1=0.000 (-100.0%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 3/5, ETA: 1.9 minutes\n",
      "\n",
      "üìç Condition 4/5: patient_01_wav_5db_crying_baby\n",
      "   üìâ Original performance: F1=0.000 (-100.0%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:01\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:01\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Spectral Subtraction: 5.7s, 105.96x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Spectral Subtraction: 5.7s, 105.96x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:16\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:16\n",
      "      üèÅ Processing completed in 5.9s\n",
      "      ‚ö° Wiener Filtering: 5.9s, 101.67x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.9s\n",
      "      ‚ö° Wiener Filtering: 5.9s, 101.67x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_crying_baby...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:31\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:31\n",
      "      üèÅ Processing completed in 13.0s\n",
      "      ‚ö° LogMMSE: 13.0s, 46.11x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 13.0s\n",
      "      ‚ö° LogMMSE: 13.0s, 46.11x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_crying_baby\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 4/5, ETA: 0.9 minutes\n",
      "\n",
      "üìç Condition 5/5: patient_01_wav_5db_coughing\n",
      "   üìâ Original performance: F1=0.218 (-71.2%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=0.0% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 4/5, ETA: 0.9 minutes\n",
      "\n",
      "üìç Condition 5/5: patient_01_wav_5db_coughing\n",
      "   üìâ Original performance: F1=0.218 (-71.2%)\n",
      "   üîß Method 1/3: Spectral Subtraction\n",
      "   üîß Applying Spectral Subtraction to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:53\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Spectral Subtraction: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Spectral Subtraction\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:58:53\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Spectral Subtraction: 5.7s, 105.25x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.7s\n",
      "      ‚ö° Spectral Subtraction: 5.7s, 105.25x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Spectral Subtraction on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Spectral Subtraction: F1=0.640, Sens=1.000, Spec=0.250 [n=20]\n",
      "      ‚úÖ Spectral Subtraction: F1=0.640, Recovery=78.2% [Sample: 20/1168]\n",
      "   üîß Method 2/3: Wiener Filtering\n",
      "   üîß Applying Wiener Filtering to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:59:08\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ Wiener Filtering: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for Wiener Filtering\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:59:08\n",
      "      üèÅ Processing completed in 5.8s\n",
      "      ‚ö° Wiener Filtering: 5.8s, 102.75x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 5.8s\n",
      "      ‚ö° Wiener Filtering: 5.8s, 102.75x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: Wiener Filtering on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ Wiener Filtering: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   üîß Method 3/3: LogMMSE\n",
      "   üîß Applying LogMMSE to patient_01_wav_5db_coughing...\n",
      "      üìä Sampled 20 files (1.7%) from 1168 total files\n",
      "      üìÅ Processing 20 sampled files (from 1168 total)\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:59:23\n",
      "      üìÅ Created temp sample directory: 20 files copied\n",
      "      üîÑ LogMMSE: Incomplete (0/20) - reprocessing\n",
      "   ‚è±Ô∏è  Measuring efficiency for LogMMSE\n",
      "      üìÅ Input files to process: 20\n",
      "      üöÄ Starting denoising at 21:59:23\n",
      "      üèÅ Processing completed in 12.2s\n",
      "      ‚ö° LogMMSE: 12.2s, 49.07x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üèÅ Processing completed in 12.2s\n",
      "      ‚ö° LogMMSE: 12.2s, 49.07x RT, 20 files\n",
      "      üßπ Cleaned up temp directory\n",
      "   üìä Evaluating: LogMMSE on patient_01_wav_5db_coughing\n",
      "      üéµ Processing 20 denoised audio files...\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "üíæ Saving downscaled processing results...\n",
      "üíæ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "üíæ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "üèÅ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "‚è±Ô∏è  Total time: 276.0 seconds (4.6 minutes)\n",
      "üìä Performance evaluations: 15\n",
      "‚ö° Efficiency measurements: 15\n",
      "üìè Sample size per condition: 20 files\n",
      "üéØ Total combinations processed: 5 conditions √ó 3 methods\n",
      "\n",
      "üèÜ PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. Spectral Subtraction: 14.6% average recovery\n",
      "   2. LogMMSE: -9.1% average recovery\n",
      "   3. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 21:59:43\n",
      "      üìä Processed: 20, Failed: 0, Mismatches: 0\n",
      "      ‚úÖ LogMMSE: F1=0.000, Sens=0.000, Spec=1.000 [n=20]\n",
      "      ‚úÖ LogMMSE: F1=0.000, Recovery=-40.4% [Sample: 20/1168]\n",
      "   ‚úÖ Condition completed: 3 methods successful\n",
      "   ‚è±Ô∏è  Progress: 5/5, ETA: 0.0 minutes\n",
      "\n",
      "üíæ Saving downscaled processing results...\n",
      "üíæ Denoising performance results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_performance_results.csv\n",
      "üíæ Denoising efficiency results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_denoising_efficiency_results.csv\n",
      "\n",
      "================================================================================\n",
      "üèÅ DOWNSCALED DENOISING APPLICATION COMPLETE!\n",
      "‚è±Ô∏è  Total time: 276.0 seconds (4.6 minutes)\n",
      "üìä Performance evaluations: 15\n",
      "‚ö° Efficiency measurements: 15\n",
      "üìè Sample size per condition: 20 files\n",
      "üéØ Total combinations processed: 5 conditions √ó 3 methods\n",
      "\n",
      "üèÜ PRELIMINARY METHOD RANKING (F1 Recovery):\n",
      "   1. Spectral Subtraction: 14.6% average recovery\n",
      "   2. LogMMSE: -9.1% average recovery\n",
      "   3. Wiener Filtering: -9.1% average recovery\n",
      "\n",
      "Time finished: 2025-07-30 21:59:43\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Apply Denoising Methods with Downscaled Processing\n",
    "print(\"üîä APPLYING DENOISING METHODS WITH DOWNSCALED PROCESSING\")\n",
    "print(f\"Time started: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def apply_and_evaluate_single_method_downscaled(condition_name, condition_row, method_key, method_config, \n",
    "                                              model, feature_columns, audio_metadata, clean_baseline):\n",
    "    \"\"\"Apply a single denoising method and evaluate performance with downscaled sampling\"\"\"\n",
    "    \n",
    "    method_name = method_config['name']\n",
    "    method_script = method_config['script']\n",
    "    \n",
    "    print(f\"   üîß Applying {method_name} to {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Find corresponding noisy audio directory\n",
    "        noisy_audio_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "        \n",
    "        if not os.path.exists(noisy_audio_dir):\n",
    "            print(f\"      ‚ùå Noisy audio directory not found: {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Apply downscaled sampling (20 files)\n",
    "        sampled_files = sample_files_downscaled(noisy_audio_dir, SAMPLE_SIZE_PER_CONDITION)\n",
    "        all_files_count = len([f for f in os.listdir(noisy_audio_dir) if f.lower().endswith('.wav')])\n",
    "        \n",
    "        if not sampled_files:\n",
    "            print(f\"      ‚ùå No files to sample from {noisy_audio_dir}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"      üìÅ Processing {len(sampled_files)} sampled files (from {all_files_count} total)\")\n",
    "        \n",
    "        # Create temporary directory with only sampled files\n",
    "        temp_input_dir = create_temp_sample_directory(\n",
    "            source_dir=noisy_audio_dir,\n",
    "            sampled_files=sampled_files,\n",
    "            temp_suffix=f\"{method_key}_{int(time.time())}\"\n",
    "        )\n",
    "        \n",
    "        # Create output directory for this method-condition combination\n",
    "        denoised_output_path = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "        os.makedirs(denoised_output_path, exist_ok=True)\n",
    "        \n",
    "        # Check if denoising already completed (based on sampled files)\n",
    "        efficiency_metrics = None\n",
    "        existing_files = []\n",
    "        \n",
    "        if os.path.exists(denoised_output_path):\n",
    "            existing_files = [f for f in os.listdir(denoised_output_path) if f.lower().endswith('.wav')]\n",
    "            expected_output = [f for f in sampled_files if f.lower().endswith('.wav')]\n",
    "            \n",
    "            if len(existing_files) >= len(expected_output) * 0.8:  # 80% completion threshold\n",
    "                print(f\"      ‚úÖ {method_name}: Already completed ({len(existing_files)} files)\")\n",
    "                efficiency_metrics = {\n",
    "                    'method_name': method_name,\n",
    "                    'condition_name': condition_name,\n",
    "                    'processing_time_sec': None,  # Already completed\n",
    "                    'files_processed': len(existing_files),\n",
    "                    'success': True,\n",
    "                    'note': 'Previously completed (downscaled)',\n",
    "                    'total_files_available': all_files_count,\n",
    "                    'sampled_files_count': len(sampled_files),\n",
    "                    'sample_size_note': 'downscaled_20_files'\n",
    "                }\n",
    "            else:\n",
    "                print(f\"      üîÑ {method_name}: Incomplete ({len(existing_files)}/{len(expected_output)}) - reprocessing\")\n",
    "        \n",
    "        # Apply denoising method if needed\n",
    "        if efficiency_metrics is None:\n",
    "            efficiency_metrics = measure_denoising_efficiency_downscaled(\n",
    "                input_dir=temp_input_dir,\n",
    "                output_dir=denoised_output_path,\n",
    "                method_script=method_script,\n",
    "                method_name=method_name\n",
    "            )\n",
    "            \n",
    "            # Add sampling metadata to efficiency metrics\n",
    "            if efficiency_metrics:\n",
    "                efficiency_metrics['condition_name'] = condition_name\n",
    "                efficiency_metrics['total_files_available'] = all_files_count\n",
    "                efficiency_metrics['sampled_files_count'] = len(sampled_files)\n",
    "                efficiency_metrics['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "        \n",
    "        # Clean up temporary directory\n",
    "        try:\n",
    "            if os.path.exists(temp_input_dir):\n",
    "                shutil.rmtree(temp_input_dir)\n",
    "                print(f\"      üßπ Cleaned up temp directory\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"      ‚ö†Ô∏è  Temp cleanup warning: {cleanup_error}\")\n",
    "        \n",
    "        # Evaluate denoised audio performance\n",
    "        performance_results = None\n",
    "        if os.path.exists(denoised_output_path):\n",
    "            performance_results = evaluate_denoised_audio_downscaled(\n",
    "                denoised_audio_dir=denoised_output_path,\n",
    "                condition_name=condition_name,\n",
    "                method_name=method_name,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata\n",
    "            )\n",
    "            \n",
    "            if performance_results:\n",
    "                # Add original noisy performance for comparison\n",
    "                performance_results['original_f1'] = condition_row['f1_score']\n",
    "                performance_results['original_degradation_pct'] = condition_row['f1_degradation_pct']\n",
    "                \n",
    "                # Calculate recovery metrics\n",
    "                if clean_baseline:\n",
    "                    clean_f1 = clean_baseline['clean_f1_score']\n",
    "                    noisy_f1 = condition_row['f1_score']\n",
    "                    denoised_f1 = performance_results['f1_score']\n",
    "                    \n",
    "                    # Recovery percentage: (denoised - noisy) / (clean - noisy) * 100\n",
    "                    if clean_f1 > noisy_f1:\n",
    "                        recovery_pct = (denoised_f1 - noisy_f1) / (clean_f1 - noisy_f1) * 100\n",
    "                    else:\n",
    "                        recovery_pct = 0\n",
    "                    \n",
    "                    performance_results['f1_recovery_pct'] = recovery_pct\n",
    "                    performance_results['clean_baseline_f1'] = clean_f1\n",
    "                \n",
    "                # Add sampling metadata to performance results\n",
    "                performance_results['total_files_available'] = all_files_count\n",
    "                performance_results['sampled_files_count'] = len(sampled_files)\n",
    "                performance_results['sampling_percentage'] = (len(sampled_files) / all_files_count) * 100\n",
    "                \n",
    "                print(f\"      ‚úÖ {method_name}: F1={performance_results['f1_score']:.3f}, Recovery={recovery_pct:.1f}% [Sample: {len(sampled_files)}/{all_files_count}]\")\n",
    "        \n",
    "        return performance_results, efficiency_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå {method_name} failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main downscaled processing execution\n",
    "if priority_conditions is not None and model is not None:\n",
    "    \n",
    "    print(f\"üöÄ DOWNSCALED PROCESSING CONFIGURATION:\")\n",
    "    print(f\"   üéØ Conditions: {len(priority_conditions)}\")\n",
    "    print(f\"   üîß Methods: {len(DENOISING_METHODS)}\")\n",
    "    print(f\"   üìä Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    print(f\"   üìà Total evaluations: {len(priority_conditions) * len(DENOISING_METHODS)}\")\n",
    "    print(f\"   ‚è±Ô∏è  Expected time: ~30 minutes\")\n",
    "    \n",
    "    all_denoising_results = []\n",
    "    all_efficiency_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each condition sequentially (methods applied sequentially within each condition)\n",
    "    for condition_idx, (_, condition_row) in enumerate(priority_conditions.iterrows()):\n",
    "        condition_name = condition_row['condition_name']\n",
    "        \n",
    "        print(f\"\\nüìç Condition {condition_idx + 1}/{len(priority_conditions)}: {condition_name}\")\n",
    "        print(f\"   üìâ Original performance: F1={condition_row['f1_score']:.3f} (-{condition_row['f1_degradation_pct']:.1f}%)\")\n",
    "        \n",
    "        condition_results = []\n",
    "        condition_efficiency = []\n",
    "        \n",
    "        # Apply each denoising method to this condition\n",
    "        for method_idx, (method_key, method_config) in enumerate(DENOISING_METHODS.items()):\n",
    "            method_name = method_config['name']\n",
    "            print(f\"   üîß Method {method_idx + 1}/{len(DENOISING_METHODS)}: {method_name}\")\n",
    "            \n",
    "            performance_result, efficiency_result = apply_and_evaluate_single_method_downscaled(\n",
    "                condition_name=condition_name,\n",
    "                condition_row=condition_row,\n",
    "                method_key=method_key,\n",
    "                method_config=method_config,\n",
    "                model=model,\n",
    "                feature_columns=feature_columns,\n",
    "                audio_metadata=audio_metadata,\n",
    "                clean_baseline=clean_baseline\n",
    "            )\n",
    "            \n",
    "            if performance_result:\n",
    "                condition_results.append(performance_result)\n",
    "                all_denoising_results.append(performance_result)\n",
    "            if efficiency_result:\n",
    "                condition_efficiency.append(efficiency_result)\n",
    "                all_efficiency_results.append(efficiency_result)\n",
    "        \n",
    "        # Progress and timing\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_conditions = len(priority_conditions) - (condition_idx + 1)\n",
    "        eta = (elapsed / (condition_idx + 1)) * remaining_conditions if condition_idx > 0 else 0\n",
    "        \n",
    "        print(f\"   ‚úÖ Condition completed: {len(condition_results)} methods successful\")\n",
    "        print(f\"   ‚è±Ô∏è  Progress: {condition_idx + 1}/{len(priority_conditions)}, ETA: {eta/60:.1f} minutes\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nüíæ Saving downscaled processing results...\")\n",
    "    \n",
    "    if all_denoising_results:\n",
    "        denoising_df = pd.DataFrame(all_denoising_results)\n",
    "        denoising_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_performance_results.csv\")\n",
    "        denoising_df.to_csv(denoising_results_path, index=False)\n",
    "        print(f\"üíæ Denoising performance results saved: {denoising_results_path}\")\n",
    "    \n",
    "    if all_efficiency_results:\n",
    "        efficiency_df = pd.DataFrame(all_efficiency_results)\n",
    "        efficiency_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_denoising_efficiency_results.csv\")\n",
    "        efficiency_df.to_csv(efficiency_results_path, index=False)\n",
    "        print(f\"üíæ Denoising efficiency results saved: {efficiency_results_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üèÅ DOWNSCALED DENOISING APPLICATION COMPLETE!\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"üìä Performance evaluations: {len(all_denoising_results)}\")\n",
    "    print(f\"‚ö° Efficiency measurements: {len(all_efficiency_results)}\")\n",
    "    print(f\"üìè Sample size per condition: {SAMPLE_SIZE_PER_CONDITION} files\")\n",
    "    print(f\"üéØ Total combinations processed: {len(priority_conditions)} conditions √ó {len(DENOISING_METHODS)} methods\")\n",
    "    \n",
    "    # Method ranking preview\n",
    "    if all_denoising_results:\n",
    "        results_df = pd.DataFrame(all_denoising_results)\n",
    "        if 'f1_recovery_pct' in results_df.columns:\n",
    "            method_rankings = results_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            print(f\"\\nüèÜ PRELIMINARY METHOD RANKING (F1 Recovery):\")\n",
    "            for rank, (method, recovery) in enumerate(method_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% average recovery\")\n",
    "    \n",
    "    # Set global variables for subsequent cells\n",
    "    denoising_results = all_denoising_results\n",
    "    efficiency_results = all_efficiency_results\n",
    "    \n",
    "else:\n",
    "    if priority_conditions is None:\n",
    "        print(f\"‚ö†Ô∏è  Priority conditions not available - check Phase 2 results\")\n",
    "    if model is None:\n",
    "        print(f\"‚ö†Ô∏è  Model not loaded - cannot evaluate performance\")\n",
    "    \n",
    "    denoising_results = []\n",
    "    efficiency_results = []\n",
    "\n",
    "print(f\"\\nTime finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eada7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä SIGNAL QUALITY ASSESSMENT - DOWNSCALED\n",
      "==================================================\n",
      "üîç Performing downscaled signal quality assessment on 15 denoising results...\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ‚úÖ SNR improvement: 6.52 dB (¬±0.99) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ‚úÖ SNR improvement: 6.52 dB (¬±0.99) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ‚úÖ SNR improvement: 4.36 dB (¬±1.42) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ‚úÖ SNR improvement: 4.36 dB (¬±1.42) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      ‚úÖ SNR improvement: 6.77 dB (¬±0.51) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 6.77 dB (¬±0.51) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 5.24 dB (¬±0.64) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 5.24 dB (¬±0.64) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 4.49 dB (¬±0.55) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 4.49 dB (¬±0.55) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_cat...\n",
      "      ‚úÖ SNR improvement: 5.39 dB (¬±0.73) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 5.39 dB (¬±0.73) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 5.68 dB (¬±0.26) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 5.68 dB (¬±0.26) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 4.32 dB (¬±0.45) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 4.32 dB (¬±0.45) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      ‚úÖ SNR improvement: 5.80 dB (¬±0.38) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 5.80 dB (¬±0.38) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 4.33 dB (¬±1.70) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 4.33 dB (¬±1.70) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 5.42 dB (¬±1.25) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 5.42 dB (¬±1.25) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      ‚úÖ SNR improvement: 5.22 dB (¬±1.08) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 5.22 dB (¬±1.08) [n=10]\n",
      "   üîç Assessing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 8.45 dB (¬±0.59) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 8.45 dB (¬±0.59) [n=10]\n",
      "   üîç Assessing Wiener Filtering on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 5.61 dB (¬±0.81) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 5.61 dB (¬±0.81) [n=10]\n",
      "   üîç Assessing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      ‚úÖ SNR improvement: 6.19 dB (¬±0.47) [n=10]\n",
      "\n",
      "üíæ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "üìä Signal Quality Summary (Downscaled):\n",
      "   üìà Average SNR improvement: 5.59 dB\n",
      "   üìâ Average spectral distortion: 2.4072\n",
      "   üèÜ Best SNR improvement: Spectral Subtraction (8.45 dB)\n",
      "   üí• Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "‚úÖ Signal quality assessment complete (downscaled)\n",
      "      ‚úÖ SNR improvement: 6.19 dB (¬±0.47) [n=10]\n",
      "\n",
      "üíæ Signal quality results saved: F:/Solo All In One Docs/Scidb Sleep Data/processed\\phase3_downscaled_results\\downscaled_signal_quality_results.csv\n",
      "\n",
      "üìä Signal Quality Summary (Downscaled):\n",
      "   üìà Average SNR improvement: 5.59 dB\n",
      "   üìâ Average spectral distortion: 2.4072\n",
      "   üèÜ Best SNR improvement: Spectral Subtraction (8.45 dB)\n",
      "   üí• Worst SNR improvement: Wiener Filtering (4.32 dB)\n",
      "\n",
      "‚úÖ Signal quality assessment complete (downscaled)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Signal Quality Assessment (Downscaled)\n",
    "print(\"üìä SIGNAL QUALITY ASSESSMENT - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def calculate_snr(clean_audio, noisy_audio):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio in dB\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(noisy_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        noisy_audio = noisy_audio[:min_len]\n",
    "        \n",
    "        # Calculate signal and noise power\n",
    "        signal_power = np.mean(clean_audio ** 2)\n",
    "        noise_power = np.mean((noisy_audio - clean_audio) ** 2)\n",
    "        \n",
    "        if noise_power > 0:\n",
    "            snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "        else:\n",
    "            snr_db = float('inf')\n",
    "        \n",
    "        return snr_db\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_spectral_distortion(clean_audio, processed_audio, sr=16000):\n",
    "    \"\"\"Calculate spectral distortion between clean and processed audio\"\"\"\n",
    "    try:\n",
    "        # Ensure same length\n",
    "        min_len = min(len(clean_audio), len(processed_audio))\n",
    "        clean_audio = clean_audio[:min_len]\n",
    "        processed_audio = processed_audio[:min_len]\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        clean_spec = np.abs(librosa.stft(clean_audio))\n",
    "        processed_spec = np.abs(librosa.stft(processed_audio))\n",
    "        \n",
    "        # Calculate L2 distance\n",
    "        min_time = min(clean_spec.shape[1], processed_spec.shape[1])\n",
    "        clean_spec = clean_spec[:, :min_time]\n",
    "        processed_spec = processed_spec[:, :min_time]\n",
    "        \n",
    "        spectral_distance = np.sqrt(np.mean((clean_spec - processed_spec) ** 2))\n",
    "        \n",
    "        return spectral_distance\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def assess_signal_quality_downscaled(condition_name, method_name, clean_dir, noisy_dir, denoised_dir, sample_size=10):\n",
    "    \"\"\"Assess signal quality improvement for downscaled method-condition combination\"\"\"\n",
    "    \n",
    "    print(f\"   üîç Assessing {method_name} on {condition_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample files for analysis (use min between sample_size and available files)\n",
    "        denoised_files = [f for f in os.listdir(denoised_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = denoised_files[:min(sample_size, len(denoised_files))]\n",
    "        \n",
    "        snr_improvements = []\n",
    "        spectral_distortions = []\n",
    "        processed_files = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                # Load denoised audio\n",
    "                denoised_path = os.path.join(denoised_dir, wav_file)\n",
    "                denoised_audio, sr = librosa.load(denoised_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                # Find corresponding noisy and clean files\n",
    "                noisy_path = os.path.join(noisy_dir, wav_file.replace('denoised_', '').replace('mixed_', 'mixed_'))\n",
    "                \n",
    "                # Try to find clean file (remove patient prefix from condition name)\n",
    "                condition_parts = condition_name.split('_')\n",
    "                patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "                clean_filename = wav_file.replace('mixed_', '').replace('denoised_', '')\n",
    "                clean_path = os.path.join(clean_dir, f\"{patient_id}_wav\", clean_filename)\n",
    "                \n",
    "                if os.path.exists(noisy_path):\n",
    "                    noisy_audio, _ = librosa.load(noisy_path, sr=TARGET_SAMPLE_RATE)\n",
    "                    \n",
    "                    if os.path.exists(clean_path):\n",
    "                        clean_audio, _ = librosa.load(clean_path, sr=TARGET_SAMPLE_RATE)\n",
    "                        \n",
    "                        # Calculate SNR improvement\n",
    "                        noisy_snr = calculate_snr(clean_audio, noisy_audio)\n",
    "                        denoised_snr = calculate_snr(clean_audio, denoised_audio)\n",
    "                        \n",
    "                        if noisy_snr is not None and denoised_snr is not None:\n",
    "                            snr_improvement = denoised_snr - noisy_snr\n",
    "                            snr_improvements.append(snr_improvement)\n",
    "                        \n",
    "                        # Calculate spectral distortion\n",
    "                        spectral_dist = calculate_spectral_distortion(clean_audio, denoised_audio)\n",
    "                        if spectral_dist is not None:\n",
    "                            spectral_distortions.append(spectral_dist)\n",
    "                    \n",
    "                    processed_files += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if processed_files < 2:  # Show first 2 errors\n",
    "                    print(f\"      ‚ö†Ô∏è  Error processing {wav_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        quality_metrics = {\n",
    "            'condition_name': condition_name,\n",
    "            'method_name': method_name,\n",
    "            'files_analyzed': processed_files,\n",
    "            'snr_improvement_db': np.mean(snr_improvements) if snr_improvements else None,\n",
    "            'snr_improvement_std': np.std(snr_improvements) if snr_improvements else None,\n",
    "            'spectral_distortion': np.mean(spectral_distortions) if spectral_distortions else None,\n",
    "            'spectral_distortion_std': np.std(spectral_distortions) if spectral_distortions else None,\n",
    "            'sample_size_note': 'downscaled_analysis'\n",
    "        }\n",
    "        \n",
    "        if snr_improvements:\n",
    "            print(f\"      ‚úÖ SNR improvement: {np.mean(snr_improvements):.2f} dB (¬±{np.std(snr_improvements):.2f}) [n={processed_files}]\")\n",
    "        \n",
    "        return quality_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Quality assessment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Perform signal quality assessment if denoising results are available\n",
    "signal_quality_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"üîç Performing downscaled signal quality assessment on {len(denoising_results)} denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    for result in denoising_results:\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            clean_dir = BASE_DATA_DIR\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(denoised_dir) and os.path.exists(noisy_dir):\n",
    "                quality_result = assess_signal_quality_downscaled(\n",
    "                    condition_name=condition_name,\n",
    "                    method_name=method_name,\n",
    "                    clean_dir=clean_dir,\n",
    "                    noisy_dir=noisy_dir,\n",
    "                    denoised_dir=denoised_dir,\n",
    "                    sample_size=10  # Analyze 10 files per condition (downscaled)\n",
    "                )\n",
    "                \n",
    "                if quality_result:\n",
    "                    signal_quality_results.append(quality_result)\n",
    "    \n",
    "    # Save signal quality results\n",
    "    if signal_quality_results:\n",
    "        quality_df = pd.DataFrame(signal_quality_results)\n",
    "        quality_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_signal_quality_results.csv\")\n",
    "        quality_df.to_csv(quality_results_path, index=False)\n",
    "        print(f\"\\nüíæ Signal quality results saved: {quality_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nüìä Signal Quality Summary (Downscaled):\")\n",
    "        valid_snr = quality_df[quality_df['snr_improvement_db'].notna()]\n",
    "        if not valid_snr.empty:\n",
    "            print(f\"   üìà Average SNR improvement: {valid_snr['snr_improvement_db'].mean():.2f} dB\")\n",
    "            print(f\"   üìâ Average spectral distortion: {quality_df['spectral_distortion'].mean():.4f}\")\n",
    "            \n",
    "            # Best and worst methods for signal quality\n",
    "            best_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmax()]\n",
    "            worst_snr = valid_snr.loc[valid_snr['snr_improvement_db'].idxmin()]\n",
    "            \n",
    "            print(f\"   üèÜ Best SNR improvement: {best_snr['method_name']} ({best_snr['snr_improvement_db']:.2f} dB)\")\n",
    "            print(f\"   üí• Worst SNR improvement: {worst_snr['method_name']} ({worst_snr['snr_improvement_db']:.2f} dB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No valid SNR measurements available\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Signal quality assessment complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Signal quality assessment skipped - no denoising results available\")\n",
    "    signal_quality_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e0d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\n",
      "==================================================\n",
      "üß™ Analyzing feature preservation for downscaled denoising results...\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_coughing...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.309\n",
      "      ‚úÖ Variance preservation: 2.137\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.309\n",
      "      ‚úÖ Variance preservation: 2.137\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.227\n",
      "      ‚úÖ Variance preservation: 9.564\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.227\n",
      "      ‚úÖ Variance preservation: 9.564\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_cat...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.022\n",
      "      ‚úÖ Variance preservation: 5.407\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.022\n",
      "      ‚úÖ Variance preservation: 5.407\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.062\n",
      "      ‚úÖ Variance preservation: 9.254\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.062\n",
      "      ‚úÖ Variance preservation: 9.254\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.588\n",
      "      ‚úÖ Variance preservation: 2.053\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.588\n",
      "      ‚úÖ Variance preservation: 2.053\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.006\n",
      "      ‚úÖ Variance preservation: 40.163\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.006\n",
      "      ‚úÖ Variance preservation: 40.163\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_coughing...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: 0.180\n",
      "      ‚úÖ Variance preservation: 7.205\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: 0.180\n",
      "      ‚úÖ Variance preservation: 7.205\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_vacuum_cleaner...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.088\n",
      "      ‚úÖ Variance preservation: 9.119\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.088\n",
      "      ‚úÖ Variance preservation: 9.119\n",
      "\n",
      "   üî¨ Analyzing LogMMSE on patient_01_wav_5db_cat...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: 0.130\n",
      "      ‚úÖ Variance preservation: 4.620\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: 0.130\n",
      "      ‚úÖ Variance preservation: 4.620\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_door_wood_creaks...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.920\n",
      "      ‚úÖ Variance preservation: 10.945\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.920\n",
      "      ‚úÖ Variance preservation: 10.945\n",
      "\n",
      "   üî¨ Analyzing Wiener Filtering on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.153\n",
      "      ‚úÖ Variance preservation: 19.279\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n",
      "      üìà Features extracted: Clean=15, Noisy=15, Denoised=15\n",
      "      ‚úÖ Correlation recovery: -0.153\n",
      "      ‚úÖ Variance preservation: 19.279\n",
      "\n",
      "   üî¨ Analyzing Spectral Subtraction on patient_01_wav_5db_crying_baby...\n",
      "      üìä Extracting features for comparison (downscaled)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m clean_features, clean_count = extract_features_from_audio_dir_downscaled(clean_dir, sample_size=\u001b[32m15\u001b[39m)\n\u001b[32m    143\u001b[39m noisy_features, noisy_count = extract_features_from_audio_dir_downscaled(noisy_dir, sample_size=\u001b[32m15\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m denoised_features, denoised_count = \u001b[43mextract_features_from_audio_dir_downscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoised_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      üìà Features extracted: Clean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Noisy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoisy_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Denoised=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdenoised_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_features \u001b[38;5;129;01mand\u001b[39;00m noisy_features \u001b[38;5;129;01mand\u001b[39;00m denoised_features:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mextract_features_from_audio_dir_downscaled\u001b[39m\u001b[34m(audio_dir, sample_size)\u001b[39m\n\u001b[32m     94\u001b[39m wav_path = os.path.join(audio_dir, wav_file)\n\u001b[32m     95\u001b[39m audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m features = \u001b[43mextract_comprehensive_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features:\n\u001b[32m     99\u001b[39m     features_list.append(features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mextract_comprehensive_features\u001b[39m\u001b[34m(audio_frame, sample_rate)\u001b[39m\n\u001b[32m     11\u001b[39m centroid = \u001b[38;5;28mfloat\u001b[39m(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n\u001b[32m     12\u001b[39m bandwidth = \u001b[38;5;28mfloat\u001b[39m(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m rolloff = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectral_rolloff\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m.mean())\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# MFCCs (first 8 coefficients)\u001b[39;00m\n\u001b[32m     16\u001b[39m mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=\u001b[32m8\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\feature\\spectral.py:638\u001b[39m, in \u001b[36mspectral_rolloff\u001b[39m\u001b[34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, freq, roll_percent)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0.0\u001b[39m < roll_percent < \u001b[32m1.0\u001b[39m:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\u001b[33m\"\u001b[39m\u001b[33mroll_percent must lie in the range (0, 1)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m S, n_fft = \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[43m=\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isrealobj(S):\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpectral rolloff is only defined \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mwith real-valued input\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:2945\u001b[39m, in \u001b[36m_spectrogram\u001b[39m\u001b[34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[39m\n\u001b[32m   2939\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2940\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[32m   2941\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2942\u001b[39m         )\n\u001b[32m   2943\u001b[39m     S = (\n\u001b[32m   2944\u001b[39m         np.abs(\n\u001b[32m-> \u001b[39m\u001b[32m2945\u001b[39m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2949\u001b[39m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2950\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2951\u001b[39m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2952\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2954\u001b[39m         )\n\u001b[32m   2955\u001b[39m         ** power\n\u001b[32m   2956\u001b[39m     )\n\u001b[32m   2958\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:387\u001b[39m, in \u001b[36mstft\u001b[39m\u001b[34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, y_frames.shape[-\u001b[32m1\u001b[39m], n_columns):\n\u001b[32m    385\u001b[39m     bl_t = \u001b[38;5;28mmin\u001b[39m(bl_s + n_columns, y_frames.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     stft_matrix[..., bl_s + off_start : bl_t + off_start] = \u001b[43mfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:91\u001b[39m, in \u001b[36mrfft\u001b[39m\u001b[34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrfft\u001b[39m(x, n=\u001b[38;5;28;01mNone\u001b[39;00m, axis=-\u001b[32m1\u001b[39m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     90\u001b[39m          overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrfft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[39m, in \u001b[36m_execute_1D\u001b[39m\u001b[34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     31\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:61\u001b[39m, in \u001b[36mr2c\u001b[39m\u001b[34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp.shape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: Feature Preservation Analysis (Downscaled)\n",
    "print(\"üß¨ FEATURE PRESERVATION ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def analyze_feature_preservation_downscaled(clean_features, noisy_features, denoised_features):\n",
    "    \"\"\"Analyze how well denoising preserves important breathing features (downscaled)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to DataFrames if needed\n",
    "        if isinstance(clean_features, list):\n",
    "            clean_df = pd.DataFrame(clean_features)\n",
    "        else:\n",
    "            clean_df = clean_features\n",
    "            \n",
    "        if isinstance(noisy_features, list):\n",
    "            noisy_df = pd.DataFrame(noisy_features)\n",
    "        else:\n",
    "            noisy_df = noisy_features\n",
    "            \n",
    "        if isinstance(denoised_features, list):\n",
    "            denoised_df = pd.DataFrame(denoised_features)\n",
    "        else:\n",
    "            denoised_df = denoised_features\n",
    "        \n",
    "        # Ensure same features and sample size\n",
    "        common_features = list(set(clean_df.columns) & set(denoised_df.columns) & set(noisy_df.columns))\n",
    "        min_samples = min(len(clean_df), len(noisy_df), len(denoised_df))\n",
    "        \n",
    "        clean_df = clean_df[common_features].iloc[:min_samples]\n",
    "        noisy_df = noisy_df[common_features].iloc[:min_samples]\n",
    "        denoised_df = denoised_df[common_features].iloc[:min_samples]\n",
    "        \n",
    "        preservation_metrics = {}\n",
    "        \n",
    "        for feature in common_features:\n",
    "            # Correlation preservation\n",
    "            clean_values = clean_df[feature].values\n",
    "            noisy_values = noisy_df[feature].values\n",
    "            denoised_values = denoised_df[feature].values\n",
    "            \n",
    "            # Calculate correlations with original clean values\n",
    "            try:\n",
    "                clean_noisy_corr = np.corrcoef(clean_values, noisy_values)[0, 1]\n",
    "                clean_denoised_corr = np.corrcoef(clean_values, denoised_values)[0, 1]\n",
    "                \n",
    "                # Correlation recovery: how much of the original correlation is restored\n",
    "                if not np.isnan(clean_noisy_corr) and not np.isnan(clean_denoised_corr):\n",
    "                    correlation_recovery = clean_denoised_corr / clean_noisy_corr if clean_noisy_corr != 0 else 1\n",
    "                else:\n",
    "                    correlation_recovery = 0\n",
    "            except:\n",
    "                clean_noisy_corr = 0\n",
    "                clean_denoised_corr = 0\n",
    "                correlation_recovery = 0\n",
    "            \n",
    "            # Variance preservation\n",
    "            clean_var = np.var(clean_values)\n",
    "            noisy_var = np.var(noisy_values)\n",
    "            denoised_var = np.var(denoised_values)\n",
    "            \n",
    "            variance_ratio = denoised_var / clean_var if clean_var > 0 else 0\n",
    "            \n",
    "            # Mean preservation\n",
    "            clean_mean = np.mean(clean_values)\n",
    "            denoised_mean = np.mean(denoised_values)\n",
    "            mean_error = abs(clean_mean - denoised_mean) / abs(clean_mean) if clean_mean != 0 else 0\n",
    "            \n",
    "            preservation_metrics[feature] = {\n",
    "                'clean_noisy_correlation': clean_noisy_corr,\n",
    "                'clean_denoised_correlation': clean_denoised_corr,\n",
    "                'correlation_recovery': correlation_recovery,\n",
    "                'variance_ratio': variance_ratio,\n",
    "                'mean_relative_error': mean_error\n",
    "            }\n",
    "        \n",
    "        return preservation_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Feature preservation analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features_from_audio_dir_downscaled(audio_dir, sample_size=15):\n",
    "    \"\"\"Extract features from a directory of audio files (downscaled sample)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        wav_files = [f for f in os.listdir(audio_dir) if f.lower().endswith('.wav')]\n",
    "        sample_files = wav_files[:min(sample_size, len(wav_files))]\n",
    "        \n",
    "        features_list = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for wav_file in sample_files:\n",
    "            try:\n",
    "                wav_path = os.path.join(audio_dir, wav_file)\n",
    "                audio_data, sr = librosa.load(wav_path, sr=TARGET_SAMPLE_RATE)\n",
    "                \n",
    "                features = extract_comprehensive_features(audio_data, sr)\n",
    "                if features:\n",
    "                    features_list.append(features)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return features_list, processed_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Feature extraction from directory failed: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Perform feature preservation analysis\n",
    "feature_preservation_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"üß™ Analyzing feature preservation for downscaled denoising results...\")\n",
    "    \n",
    "    # Group results by condition and method\n",
    "    method_condition_pairs = list(set([(r['condition_name'], r['method_name']) for r in denoising_results]))\n",
    "    \n",
    "    for condition_name, method_name in method_condition_pairs:\n",
    "        print(f\"\\n   üî¨ Analyzing {method_name} on {condition_name}...\")\n",
    "        \n",
    "        # Find method key\n",
    "        method_key = None\n",
    "        for key, config in DENOISING_METHODS.items():\n",
    "            if config['name'] == method_name:\n",
    "                method_key = key\n",
    "                break\n",
    "        \n",
    "        if method_key:\n",
    "            # Define directories\n",
    "            condition_parts = condition_name.split('_')\n",
    "            patient_id = condition_parts[0] + '_' + condition_parts[1]  # e.g., 'patient_01'\n",
    "            clean_dir = os.path.join(BASE_DATA_DIR, f\"{patient_id}_wav\")\n",
    "            noisy_dir = os.path.join(BASE_DATA_DIR, condition_name)\n",
    "            denoised_dir = os.path.join(DENOISED_OUTPUT_DIR, f\"{condition_name}_{method_key}\")\n",
    "            \n",
    "            if os.path.exists(clean_dir) and os.path.exists(noisy_dir) and os.path.exists(denoised_dir):\n",
    "                # Extract features from each audio type (downscaled samples)\n",
    "                print(f\"      üìä Extracting features for comparison (downscaled)...\")\n",
    "                \n",
    "                clean_features, clean_count = extract_features_from_audio_dir_downscaled(clean_dir, sample_size=15)\n",
    "                noisy_features, noisy_count = extract_features_from_audio_dir_downscaled(noisy_dir, sample_size=15)\n",
    "                denoised_features, denoised_count = extract_features_from_audio_dir_downscaled(denoised_dir, sample_size=15)\n",
    "                \n",
    "                print(f\"      üìà Features extracted: Clean={clean_count}, Noisy={noisy_count}, Denoised={denoised_count}\")\n",
    "                \n",
    "                if clean_features and noisy_features and denoised_features:\n",
    "                    preservation_metrics = analyze_feature_preservation_downscaled(\n",
    "                        clean_features=clean_features,\n",
    "                        noisy_features=noisy_features,\n",
    "                        denoised_features=denoised_features\n",
    "                    )\n",
    "                    \n",
    "                    if preservation_metrics:\n",
    "                        # Calculate aggregate preservation scores\n",
    "                        correlation_recoveries = [m['correlation_recovery'] for m in preservation_metrics.values() if not np.isnan(m['correlation_recovery']) and not np.isinf(m['correlation_recovery'])]\n",
    "                        variance_ratios = [m['variance_ratio'] for m in preservation_metrics.values() if not np.isnan(m['variance_ratio']) and not np.isinf(m['variance_ratio'])]\n",
    "                        mean_errors = [m['mean_relative_error'] for m in preservation_metrics.values() if not np.isnan(m['mean_relative_error']) and not np.isinf(m['mean_relative_error'])]\n",
    "                        \n",
    "                        aggregate_result = {\n",
    "                            'condition_name': condition_name,\n",
    "                            'method_name': method_name,\n",
    "                            'avg_correlation_recovery': np.mean(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'std_correlation_recovery': np.std(correlation_recoveries) if correlation_recoveries else 0,\n",
    "                            'avg_variance_ratio': np.mean(variance_ratios) if variance_ratios else 0,\n",
    "                            'std_variance_ratio': np.std(variance_ratios) if variance_ratios else 0,\n",
    "                            'avg_mean_error': np.mean(mean_errors) if mean_errors else 0,\n",
    "                            'features_analyzed': len(preservation_metrics),\n",
    "                            'sample_size_note': 'downscaled_15_files',\n",
    "                            'detailed_metrics': preservation_metrics\n",
    "                        }\n",
    "                        \n",
    "                        feature_preservation_results.append(aggregate_result)\n",
    "                        \n",
    "                        print(f\"      ‚úÖ Correlation recovery: {aggregate_result['avg_correlation_recovery']:.3f}\")\n",
    "                        print(f\"      ‚úÖ Variance preservation: {aggregate_result['avg_variance_ratio']:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                missing_dirs = []\n",
    "                if not os.path.exists(clean_dir): missing_dirs.append(f\"clean ({clean_dir})\")\n",
    "                if not os.path.exists(noisy_dir): missing_dirs.append(f\"noisy ({noisy_dir})\")\n",
    "                if not os.path.exists(denoised_dir): missing_dirs.append(f\"denoised ({denoised_dir})\")\n",
    "                print(f\"      ‚ö†Ô∏è  Missing directories: {', '.join(missing_dirs)}\")\n",
    "    \n",
    "    # Save feature preservation results\n",
    "    if feature_preservation_results:\n",
    "        # Save aggregate results\n",
    "        preservation_summary = [{\n",
    "            'condition_name': r['condition_name'],\n",
    "            'method_name': r['method_name'],\n",
    "            'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "            'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "            'avg_mean_error': r['avg_mean_error'],\n",
    "            'features_analyzed': r['features_analyzed'],\n",
    "            'sample_size_note': r['sample_size_note']\n",
    "        } for r in feature_preservation_results]\n",
    "        \n",
    "        preservation_df = pd.DataFrame(preservation_summary)\n",
    "        preservation_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_feature_preservation_results.csv\")\n",
    "        preservation_df.to_csv(preservation_results_path, index=False)\n",
    "        print(f\"\\nüíæ Feature preservation results saved: {preservation_results_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nüß¨ Feature Preservation Summary (Downscaled):\")\n",
    "        print(f\"   üìä Average correlation recovery: {preservation_df['avg_correlation_recovery'].mean():.3f}\")\n",
    "        print(f\"   üìä Average variance preservation: {preservation_df['avg_variance_ratio'].mean():.3f}\")\n",
    "        print(f\"   üìä Average mean error: {preservation_df['avg_mean_error'].mean():.3f}\")\n",
    "        \n",
    "        # Best and worst methods for feature preservation\n",
    "        if len(preservation_df) > 1:\n",
    "            best_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmax()]\n",
    "            worst_preservation = preservation_df.loc[preservation_df['avg_correlation_recovery'].idxmin()]\n",
    "            \n",
    "            print(f\"   üèÜ Best preservation: {best_preservation['method_name']} ({best_preservation['avg_correlation_recovery']:.3f})\")\n",
    "            print(f\"   üí• Worst preservation: {worst_preservation['method_name']} ({worst_preservation['avg_correlation_recovery']:.3f})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature preservation analysis complete (downscaled)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Feature preservation analysis skipped - no denoising results available\")\n",
    "    feature_preservation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compile Comprehensive Results and Multi-Criteria Analysis (Downscaled)\n",
    "print(\"üîç COMPILING COMPREHENSIVE RESULTS AND MULTI-CRITERIA ANALYSIS - DOWNSCALED\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Compile all results into comprehensive dataset\n",
    "comprehensive_results = []\n",
    "\n",
    "if 'denoising_results' in locals() and denoising_results:\n",
    "    print(f\"üìä Compiling results from {len(denoising_results)} downscaled denoising evaluations...\")\n",
    "    \n",
    "    # Convert results lists to DataFrames for easier merging\n",
    "    denoising_df = pd.DataFrame(denoising_results) if denoising_results else pd.DataFrame()\n",
    "    efficiency_df = pd.DataFrame(efficiency_results) if 'efficiency_results' in locals() and efficiency_results else pd.DataFrame()\n",
    "    quality_df = pd.DataFrame(signal_quality_results) if 'signal_quality_results' in locals() and signal_quality_results else pd.DataFrame()\n",
    "    preservation_df = pd.DataFrame([{\n",
    "        'condition_name': r['condition_name'],\n",
    "        'method_name': r['method_name'],\n",
    "        'avg_correlation_recovery': r['avg_correlation_recovery'],\n",
    "        'avg_variance_ratio': r['avg_variance_ratio'],\n",
    "        'avg_mean_error': r['avg_mean_error']\n",
    "    } for r in feature_preservation_results]) if 'feature_preservation_results' in locals() and feature_preservation_results else pd.DataFrame()\n",
    "    \n",
    "    # Merge all results on condition_name and method_name\n",
    "    for _, result in denoising_df.iterrows():\n",
    "        condition_name = result['condition_name']\n",
    "        method_name = result['method_name']\n",
    "        \n",
    "        # Start with denoising performance results\n",
    "        comprehensive_result = result.to_dict()\n",
    "        \n",
    "        # Add efficiency metrics\n",
    "        efficiency_match = efficiency_df[\n",
    "            (efficiency_df['condition_name'] == condition_name) & \n",
    "            (efficiency_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not efficiency_match.empty:\n",
    "            eff_result = efficiency_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'processing_time_sec': eff_result.get('processing_time_sec'),\n",
    "                'real_time_factor': eff_result.get('real_time_factor'),\n",
    "                'memory_usage_mb': eff_result.get('memory_usage_mb'),\n",
    "                'processing_speed_files_per_sec': eff_result.get('processing_speed_files_per_sec')\n",
    "            })\n",
    "        \n",
    "        # Add signal quality metrics\n",
    "        quality_match = quality_df[\n",
    "            (quality_df['condition_name'] == condition_name) & \n",
    "            (quality_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not quality_match.empty:\n",
    "            qual_result = quality_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'snr_improvement_db': qual_result.get('snr_improvement_db'),\n",
    "                'spectral_distortion': qual_result.get('spectral_distortion')\n",
    "            })\n",
    "        \n",
    "        # Add feature preservation metrics\n",
    "        preservation_match = preservation_df[\n",
    "            (preservation_df['condition_name'] == condition_name) & \n",
    "            (preservation_df['method_name'] == method_name)\n",
    "        ]\n",
    "        if not preservation_match.empty:\n",
    "            pres_result = preservation_match.iloc[0]\n",
    "            comprehensive_result.update({\n",
    "                'avg_correlation_recovery': pres_result.get('avg_correlation_recovery'),\n",
    "                'avg_variance_ratio': pres_result.get('avg_variance_ratio'),\n",
    "                'avg_mean_error': pres_result.get('avg_mean_error')\n",
    "            })\n",
    "        \n",
    "        comprehensive_results.append(comprehensive_result)\n",
    "    \n",
    "    print(f\"‚úÖ Compiled {len(comprehensive_results)} comprehensive result records\")\n",
    "    \n",
    "    # Calculate normalized scores for multi-criteria analysis\n",
    "    if comprehensive_results:\n",
    "        comp_df = pd.DataFrame(comprehensive_results)\n",
    "        \n",
    "        # Normalize scores (0-1 scale) for smartphone suitability calculation\n",
    "        def normalize_score(values, higher_is_better=True):\n",
    "            values = pd.Series(values).fillna(0)  # Handle NaN values\n",
    "            if values.std() == 0:  # All values are the same\n",
    "                return pd.Series([0.5] * len(values))\n",
    "            if higher_is_better:\n",
    "                return (values - values.min()) / (values.max() - values.min())\n",
    "            else:\n",
    "                return (values.max() - values) / (values.max() - values.min())\n",
    "        \n",
    "        # Calculate individual dimension scores (handle NaN values gracefully)\n",
    "        comp_df['f1_recovery_score'] = normalize_score(comp_df.get('f1_recovery_pct', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['efficiency_score'] = normalize_score(comp_df.get('real_time_factor', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['signal_quality_score'] = normalize_score(comp_df.get('snr_improvement_db', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        comp_df['feature_preservation_score'] = normalize_score(comp_df.get('avg_correlation_recovery', pd.Series([0] * len(comp_df))), higher_is_better=True)\n",
    "        \n",
    "        # Calculate smartphone suitability composite score\n",
    "        SMARTPHONE_WEIGHTS = {\n",
    "            'f1_recovery': 0.40,        # Detection performance recovery (most critical)\n",
    "            'efficiency': 0.25,         # Processing speed + memory usage\n",
    "            'signal_quality': 0.20,     # SNR improvement + artifact control\n",
    "            'feature_preservation': 0.15 # Biomarker stability\n",
    "        }\n",
    "        \n",
    "        comp_df['smartphone_suitability_score'] = (\n",
    "            comp_df['f1_recovery_score'] * SMARTPHONE_WEIGHTS['f1_recovery'] +\n",
    "            comp_df['efficiency_score'] * SMARTPHONE_WEIGHTS['efficiency'] +\n",
    "            comp_df['signal_quality_score'] * SMARTPHONE_WEIGHTS['signal_quality'] +\n",
    "            comp_df['feature_preservation_score'] * SMARTPHONE_WEIGHTS['feature_preservation']\n",
    "        )\n",
    "        \n",
    "        # Update comprehensive_results with calculated scores\n",
    "        comprehensive_results = comp_df.to_dict('records')\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        comprehensive_results_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_comprehensive_results.csv\")\n",
    "        comp_df.to_csv(comprehensive_results_path, index=False)\n",
    "        print(f\"üíæ Comprehensive results saved: {comprehensive_results_path}\")\n",
    "        \n",
    "        # Display ranking summary\n",
    "        print(f\"\\nüèÜ SMARTPHONE SUITABILITY RANKING (DOWNSCALED):\")\n",
    "        method_rankings = comp_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=False)\n",
    "        for rank, (method, score) in enumerate(method_rankings.items(), 1):\n",
    "            print(f\"   {rank}. {method}: {score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüìä PERFORMANCE RECOVERY RANKING (DOWNSCALED):\")\n",
    "        if 'f1_recovery_pct' in comp_df.columns:\n",
    "            performance_rankings = comp_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "            for rank, (method, recovery) in enumerate(performance_rankings.items(), 1):\n",
    "                print(f\"   {rank}. {method}: {recovery:.1f}% F1 recovery\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No denoising results available for comprehensive analysis\")\n",
    "    print(f\"   Please ensure Cell 5 (denoising application) has been executed successfully\")\n",
    "    comprehensive_results = []\n",
    "\n",
    "if comprehensive_results:\n",
    "    # Generate final summary report for downscaled analysis\n",
    "    print(f\"\\nüìã DOWNSCALED PHASE 3 FINAL SUMMARY REPORT:\")\n",
    "    print(f\"\\nüéØ Research Objective Achievement:\")\n",
    "    print(f\"   ‚úÖ Evaluated {len(DENOISING_METHODS)} denoising methods\")\n",
    "    print(f\"   ‚úÖ Tested on {len(REPRESENTATIVE_CONDITIONS)} representative worst-case conditions (5dB SNR)\")\n",
    "    print(f\"   ‚úÖ Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\")\n",
    "    print(f\"   ‚úÖ Generated smartphone deployment recommendations\")\n",
    "    print(f\"   ‚úÖ Downscaled scope: {SAMPLE_SIZE_PER_CONDITION} files per condition for rapid evaluation\")\n",
    "    \n",
    "    # Key findings\n",
    "    comp_df = pd.DataFrame(comprehensive_results)\n",
    "    best_overall = comp_df.loc[comp_df['smartphone_suitability_score'].idxmax()]\n",
    "    \n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        best_performance = comp_df.loc[comp_df['f1_recovery_pct'].idxmax()]\n",
    "    else:\n",
    "        best_performance = None\n",
    "    \n",
    "    valid_efficiency = comp_df[comp_df['real_time_factor'].notna()] if 'real_time_factor' in comp_df.columns else pd.DataFrame()\n",
    "    best_efficiency = valid_efficiency.loc[valid_efficiency['real_time_factor'].idxmax()] if not valid_efficiency.empty else None\n",
    "    \n",
    "    print(f\"\\nüèÜ Key Findings from Downscaled Sampling:\")\n",
    "    print(f\"   ü•á Best Overall Method: {best_overall['method_name']} (Score: {best_overall['smartphone_suitability_score']:.3f})\")\n",
    "    if best_performance is not None:\n",
    "        print(f\"   üéØ Best Performance Recovery: {best_performance['method_name']} ({best_performance['f1_recovery_pct']:.1f}% F1 recovery)\")\n",
    "    if best_efficiency is not None:\n",
    "        print(f\"   ‚ö° Most Efficient: {best_efficiency['method_name']} ({best_efficiency['real_time_factor']:.2f}x real-time)\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    if 'f1_recovery_pct' in comp_df.columns:\n",
    "        avg_recovery = comp_df['f1_recovery_pct'].mean()\n",
    "        recovery_std = comp_df['f1_recovery_pct'].std()\n",
    "        \n",
    "        print(f\"\\nüìä Downscaled Performance Statistics:\")\n",
    "        print(f\"   üìà Average F1 Recovery: {avg_recovery:.1f}% (¬±{recovery_std:.1f}%)\")\n",
    "        print(f\"   üìà Methods achieving >50% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 50])} / {len(comp_df)}\")\n",
    "        print(f\"   üìà Methods achieving >75% recovery: {len(comp_df[comp_df['f1_recovery_pct'] >= 75])} / {len(comp_df)}\")\n",
    "        print(f\"   üéØ Conditions tested: {len(REPRESENTATIVE_CONDITIONS)} worst-case (5dB) per noise category\")\n",
    "        print(f\"   üìè Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    # Save final summary\n",
    "    final_summary = {\n",
    "        'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'phase3_status': 'completed_downscaled',\n",
    "        'downscaled_optimization': {\n",
    "            'sample_size_per_condition': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'total_files_per_evaluation': SAMPLE_SIZE_PER_CONDITION,\n",
    "            'execution_approach': 'rapid_prototyping'\n",
    "        },\n",
    "        'representative_conditions': REPRESENTATIVE_CONDITIONS,\n",
    "        'methods_evaluated': len(DENOISING_METHODS),\n",
    "        'conditions_tested': len(REPRESENTATIVE_CONDITIONS),\n",
    "        'total_evaluations': len(comp_df),\n",
    "        'best_overall_method': best_overall['method_name'],\n",
    "        'best_overall_score': float(best_overall['smartphone_suitability_score']),\n",
    "        'research_contributions': [\n",
    "            'Downscaled systematic multi-dimensional evaluation of denoising for sleep apnea detection',\n",
    "            'Rapid prototyping methodology for efficient method comparison',\n",
    "            'Smartphone deployment feasibility assessment with reduced computational requirements',\n",
    "            'Evidence-based method recommendations for mobile health applications',\n",
    "            'Performance-efficiency trade-off quantification for worst-case scenarios'\n",
    "        ],\n",
    "        'files_generated': [\n",
    "            'downscaled_comprehensive_results.csv',\n",
    "            'downscaled_denoising_performance_results.csv',\n",
    "            'downscaled_denoising_efficiency_results.csv',\n",
    "            'downscaled_signal_quality_results.csv',\n",
    "            'downscaled_feature_preservation_results.csv'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if best_performance is not None:\n",
    "        final_summary['best_performance_method'] = best_performance['method_name']\n",
    "        final_summary['best_performance_recovery'] = float(best_performance['f1_recovery_pct'])\n",
    "        final_summary['average_f1_recovery'] = float(avg_recovery)\n",
    "    \n",
    "    summary_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_phase3_final_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    print(f\"\\nüíæ Final summary saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nüéâ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\")\n",
    "    print(f\"\\nüìã Research Contributions Achieved:\")\n",
    "    print(f\"   ‚úÖ Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\")\n",
    "    print(f\"   ‚úÖ Downscaled sampling methodology for efficient evaluation\")\n",
    "    print(f\"   ‚úÖ Smartphone deployment feasibility assessment\")\n",
    "    print(f\"   ‚úÖ Evidence-based method recommendations for mobile health applications\")\n",
    "    print(f\"   ‚úÖ Performance-efficiency trade-off quantification for worst-case scenarios\")\n",
    "    print(f\"   ‚úÖ Proof-of-concept validation with {SAMPLE_SIZE_PER_CONDITION}-file sampling approach\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Comprehensive analysis skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive results compilation complete (downscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Comprehensive Visualization and Final Results (Downscaled)\n",
    "print(\"üìà COMPREHENSIVE VISUALIZATION AND FINAL RESULTS - DOWNSCALED\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if 'comprehensive_results' in locals() and comprehensive_results:\n",
    "    # Set up plotting\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    comprehensive_df = pd.DataFrame(comprehensive_results)\n",
    "    \n",
    "    # 1. Smartphone Suitability Score Comparison\n",
    "    plt.subplot(3, 4, 1)\n",
    "    if 'smartphone_suitability_score' in comprehensive_df.columns:\n",
    "        method_scores = comprehensive_df.groupby('method_name')['smartphone_suitability_score'].mean().sort_values(ascending=True)\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(method_scores)))\n",
    "        bars = plt.barh(range(len(method_scores)), method_scores.values, color=colors)\n",
    "        plt.yticks(range(len(method_scores)), method_scores.index)\n",
    "        plt.xlabel('Smartphone Suitability Score')\n",
    "        plt.title('Overall Smartphone Suitability Ranking\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, method_scores.values)):\n",
    "            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{value:.3f}', ha='left', va='center', fontsize=9)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Smartphone Suitability\\nScores Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Smartphone Suitability Ranking\\n(Downscaled)')\n",
    "    \n",
    "    # 2. F1 Recovery Performance\n",
    "    plt.subplot(3, 4, 2)\n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns:\n",
    "        method_f1_recovery = comprehensive_df.groupby('method_name')['f1_recovery_pct'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_f1_recovery)), method_f1_recovery.values, \n",
    "                color=['green' if x >= 75 else 'orange' if x >= 50 else 'red' for x in method_f1_recovery.values])\n",
    "        plt.xticks(range(len(method_f1_recovery)), method_f1_recovery.index, rotation=45, ha='right')\n",
    "        plt.ylabel('F1 Recovery (%)')\n",
    "        plt.title('Detection Performance Recovery\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add horizontal lines for recovery targets\n",
    "        plt.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Min Acceptable')\n",
    "        plt.axhline(y=75, color='orange', linestyle='--', alpha=0.7, label='Good')\n",
    "        plt.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='Excellent')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'F1 Recovery\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Detection Performance Recovery\\n(Downscaled)')\n",
    "    \n",
    "    # 3. Computational Efficiency\n",
    "    plt.subplot(3, 4, 3)\n",
    "    valid_efficiency = comprehensive_df[comprehensive_df['real_time_factor'].notna()] if 'real_time_factor' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_efficiency.empty:\n",
    "        method_efficiency = valid_efficiency.groupby('method_name')['real_time_factor'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_efficiency)), method_efficiency.values,\n",
    "                color=['green' if x >= 1.0 else 'orange' if x >= 0.5 else 'red' for x in method_efficiency.values])\n",
    "        plt.xticks(range(len(method_efficiency)), method_efficiency.index, rotation=45, ha='right')\n",
    "        plt.ylabel('Real-Time Factor')\n",
    "        plt.title('Computational Efficiency\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Real-Time Threshold')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Efficiency\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Computational Efficiency\\n(Downscaled)')\n",
    "    \n",
    "    # 4. Signal Quality Improvement\n",
    "    plt.subplot(3, 4, 4)\n",
    "    valid_quality = comprehensive_df[comprehensive_df['snr_improvement_db'].notna()] if 'snr_improvement_db' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_quality.empty:\n",
    "        method_quality = valid_quality.groupby('method_name')['snr_improvement_db'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_quality)), method_quality.values,\n",
    "                color=['green' if x >= 5 else 'orange' if x >= 0 else 'red' for x in method_quality.values])\n",
    "        plt.xticks(range(len(method_quality)), method_quality.index, rotation=45, ha='right')\n",
    "        plt.ylabel('SNR Improvement (dB)')\n",
    "        plt.title('Signal Quality Enhancement\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.7, label='No Improvement')\n",
    "        plt.legend(fontsize=8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Signal Quality\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Signal Quality Enhancement\\n(Downscaled)')\n",
    "    \n",
    "    # 5. Feature Preservation\n",
    "    plt.subplot(3, 4, 5)\n",
    "    valid_preservation = comprehensive_df[comprehensive_df['avg_correlation_recovery'].notna()] if 'avg_correlation_recovery' in comprehensive_df.columns else pd.DataFrame()\n",
    "    if not valid_preservation.empty:\n",
    "        method_preservation = valid_preservation.groupby('method_name')['avg_correlation_recovery'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(method_preservation)), method_preservation.values,\n",
    "                color=['green' if x >= 0.8 else 'orange' if x >= 0.5 else 'red' for x in method_preservation.values])\n",
    "        plt.xticks(range(len(method_preservation)), method_preservation.index, rotation=45, ha='right')\n",
    "        plt.ylabel('Correlation Recovery')\n",
    "        plt.title('Feature Preservation\\n(Downscaled)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature Preservation\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Feature Preservation\\n(Downscaled)')\n",
    "    \n",
    "    # 6. Performance vs Efficiency Scatter Plot\n",
    "    plt.subplot(3, 4, 6)\n",
    "    valid_scatter = comprehensive_df[(comprehensive_df['f1_recovery_pct'].notna()) & \n",
    "                                   (comprehensive_df['real_time_factor'].notna())] if all(col in comprehensive_df.columns for col in ['f1_recovery_pct', 'real_time_factor']) else pd.DataFrame()\n",
    "    if not valid_scatter.empty:\n",
    "        methods = valid_scatter['method_name'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_scatter[valid_scatter['method_name'] == method]\n",
    "            plt.scatter(method_data['real_time_factor'], method_data['f1_recovery_pct'], \n",
    "                       label=method, alpha=0.7, s=60, color=colors[i])\n",
    "        \n",
    "        plt.xlabel('Real-Time Factor')\n",
    "        plt.ylabel('F1 Recovery (%)')\n",
    "        plt.title('Performance vs Efficiency Trade-off\\n(Downscaled)')\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Performance vs Efficiency\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Performance vs Efficiency Trade-off\\n(Downscaled)')\n",
    "    \n",
    "    # 7. Multi-Dimensional Comparison\n",
    "    plt.subplot(3, 4, 7)\n",
    "    radar_metrics = ['f1_recovery_score', 'efficiency_score', 'signal_quality_score', 'feature_preservation_score']\n",
    "    radar_metrics_available = [col for col in radar_metrics if col in comprehensive_df.columns]\n",
    "    radar_labels = ['F1 Recovery', 'Efficiency', 'Signal Quality', 'Feature Preservation'][:len(radar_metrics_available)]\n",
    "    \n",
    "    if radar_metrics_available:\n",
    "        method_radar_scores = comprehensive_df.groupby('method_name')[radar_metrics_available].mean()\n",
    "        \n",
    "        if not method_radar_scores.empty:\n",
    "            method_names = method_radar_scores.index\n",
    "            x_pos = np.arange(len(radar_labels))\n",
    "            width = 0.8 / len(method_names) if len(method_names) > 0 else 0.8\n",
    "            \n",
    "            for i, method in enumerate(method_names):\n",
    "                scores = method_radar_scores.loc[method, radar_metrics_available].values\n",
    "                plt.bar(x_pos + i*width, scores, width, label=method, alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Dimensions')\n",
    "            plt.ylabel('Normalized Scores')\n",
    "            plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "            plt.xticks(x_pos + width*(len(method_names)-1)/2, radar_labels, rotation=45, ha='right')\n",
    "            plt.legend(fontsize=8)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Multi-Dimensional\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Multi-Dimensional\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Multi-Dimensional Comparison\\n(Downscaled)')\n",
    "    \n",
    "    # 8. Recovery vs Original Performance\n",
    "    plt.subplot(3, 4, 8)\n",
    "    valid_recovery = comprehensive_df[(comprehensive_df['original_f1'].notna()) & \n",
    "                                    (comprehensive_df['f1_score'].notna())] if all(col in comprehensive_df.columns for col in ['original_f1', 'f1_score']) else pd.DataFrame()\n",
    "    if not valid_recovery.empty:\n",
    "        methods = valid_recovery['method_name'].unique()\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(methods)))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = valid_recovery[valid_recovery['method_name'] == method]\n",
    "            plt.scatter(method_data['original_f1'], method_data['f1_score'], \n",
    "                       label=method, alpha=0.7, s=60, color=colors[i])\n",
    "        \n",
    "        # Add diagonal line for reference (no improvement)\n",
    "        min_val = min(valid_recovery['original_f1'].min(), valid_recovery['f1_score'].min())\n",
    "        max_val = max(valid_recovery['original_f1'].max(), valid_recovery['f1_score'].max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='No Improvement')\n",
    "        \n",
    "        plt.xlabel('Original Noisy F1-Score')\n",
    "        plt.ylabel('Denoised F1-Score')\n",
    "        plt.title('Recovery Effectiveness\\n(Downscaled)')\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Recovery Effectiveness\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Recovery Effectiveness\\n(Downscaled)')\n",
    "    \n",
    "    # 9-12: Method Performance Heatmaps (simplified for downscaled data)\n",
    "    heatmap_metrics = [\n",
    "        ('f1_recovery_pct', 'F1 Recovery (%) by Method-Condition\\n(Downscaled)'),\n",
    "        ('real_time_factor', 'Real-Time Factor by Method-Condition\\n(Downscaled)'),\n",
    "        ('snr_improvement_db', 'SNR Improvement (dB) by Method-Condition\\n(Downscaled)'),\n",
    "        ('avg_correlation_recovery', 'Feature Preservation by Method-Condition\\n(Downscaled)')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(heatmap_metrics, 9):\n",
    "        plt.subplot(3, 4, idx)\n",
    "        if metric in comprehensive_df.columns:\n",
    "            valid_data = comprehensive_df[comprehensive_df[metric].notna()]\n",
    "            if not valid_data.empty and len(valid_data['method_name'].unique()) > 1:\n",
    "                try:\n",
    "                    pivot_data = valid_data.pivot_table(index='method_name', columns='condition_name', values=metric, aggfunc='mean')\n",
    "                    if not pivot_data.empty:\n",
    "                        # Adjust figure size for readability\n",
    "                        sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "                                  cbar_kws={'label': metric}, square=False)\n",
    "                        plt.title(title, fontsize=10)\n",
    "                        plt.xlabel('Condition', fontsize=9)\n",
    "                        plt.ylabel('Method', fontsize=9)\n",
    "                        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "                        plt.yticks(rotation=0, fontsize=8)\n",
    "                    else:\n",
    "                        plt.text(0.5, 0.5, f'{title}\\nData Not Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                        plt.title(title, fontsize=10)\n",
    "                except Exception as e:\n",
    "                    plt.text(0.5, 0.5, f'{title}\\nError: {str(e)[:30]}...', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                    plt.title(title, fontsize=10)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, f'{title}\\nInsufficient Data', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title(title, fontsize=10)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'{title}\\nColumn Not Found', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title(title, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comprehensive visualization\n",
    "    viz_path = os.path.join(RESULTS_OUTPUT_DIR, \"downscaled_phase3_comprehensive_analysis.png\")\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üíæ Comprehensive visualization saved: {viz_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate final summary report  \n",
    "    print(f\"\\nüìã FINAL DOWNSCALED PHASE 3 SUMMARY REPORT:\")\n",
    "    print(f\"\\nüéØ Research Objective Achievement:\")\n",
    "    print(f\"   ‚úÖ Evaluated {len(DENOISING_METHODS)} denoising methods\")\n",
    "    print(f\"   ‚úÖ Tested on {len(comprehensive_df['condition_name'].unique())} priority noise conditions\")\n",
    "    print(f\"   ‚úÖ Measured across 4 dimensions: Performance, Efficiency, Quality, Preservation\")\n",
    "    print(f\"   ‚úÖ Generated smartphone deployment recommendations\")\n",
    "    print(f\"   ‚úÖ Downscaled approach: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    # Key findings (with safe access to data)\n",
    "    if 'smartphone_suitability_score' in comprehensive_df.columns and not comprehensive_df['smartphone_suitability_score'].isna().all():\n",
    "        best_overall = comprehensive_df.loc[comprehensive_df['smartphone_suitability_score'].idxmax()]\n",
    "        print(f\"   ü•á Best Overall Method: {best_overall['method_name']} (Score: {best_overall['smartphone_suitability_score']:.3f})\")\n",
    "    \n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns and not comprehensive_df['f1_recovery_pct'].isna().all():\n",
    "        best_performance = comprehensive_df.loc[comprehensive_df['f1_recovery_pct'].idxmax()]\n",
    "        print(f\"   üéØ Best Performance Recovery: {best_performance['method_name']} ({best_performance['f1_recovery_pct']:.1f}% F1 recovery)\")\n",
    "    \n",
    "    if 'real_time_factor' in comprehensive_df.columns:\n",
    "        best_efficiency_df = comprehensive_df[comprehensive_df['real_time_factor'].notna()]\n",
    "        if not best_efficiency_df.empty:\n",
    "            best_efficiency = best_efficiency_df.loc[best_efficiency_df['real_time_factor'].idxmax()]\n",
    "            print(f\"   ‚ö° Most Efficient: {best_efficiency['method_name']} ({best_efficiency['real_time_factor']:.2f}x real-time)\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    if 'f1_recovery_pct' in comprehensive_df.columns:\n",
    "        avg_recovery = comprehensive_df['f1_recovery_pct'].mean()\n",
    "        recovery_std = comprehensive_df['f1_recovery_pct'].std()\n",
    "        \n",
    "        print(f\"\\nüìä Downscaled Performance Statistics:\")\n",
    "        print(f\"   üìà Average F1 Recovery: {avg_recovery:.1f}% (¬±{recovery_std:.1f}%)\")\n",
    "        print(f\"   üìà Methods achieving >50% recovery: {len(comprehensive_df[comprehensive_df['f1_recovery_pct'] >= 50])} / {len(comprehensive_df)}\")\n",
    "        print(f\"   üìà Methods achieving >75% recovery: {len(comprehensive_df[comprehensive_df['f1_recovery_pct'] >= 75])} / {len(comprehensive_df)}\")\n",
    "        print(f\"   üìè Sample size: {SAMPLE_SIZE_PER_CONDITION} files per condition\")\n",
    "    \n",
    "    print(f\"\\nüéâ DOWNSCALED PHASE 3 COMPREHENSIVE DENOISING EVALUATION COMPLETE!\")\n",
    "    print(f\"\\nüìã Research Contributions Achieved:\")\n",
    "    print(f\"   ‚úÖ Rapid systematic multi-dimensional evaluation of denoising for sleep apnea detection\")\n",
    "    print(f\"   ‚úÖ Downscaled sampling methodology for efficient method comparison\")\n",
    "    print(f\"   ‚úÖ Smartphone deployment feasibility assessment\")\n",
    "    print(f\"   ‚úÖ Evidence-based method recommendations for mobile health applications\")\n",
    "    print(f\"   ‚úÖ Performance-efficiency trade-off quantification\")\n",
    "    print(f\"   ‚úÖ Feature preservation analysis for breathing biomarkers\")\n",
    "    print(f\"   ‚úÖ Proof-of-concept validation with {SAMPLE_SIZE_PER_CONDITION}-file sampling\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Comprehensive visualization skipped - no results available\")\n",
    "    print(f\"   Please ensure all previous cells have been executed successfully\")\n",
    "    print(f\"   Expected results from:\")\n",
    "    print(f\"     - Cell 5: Denoising application and performance evaluation\")\n",
    "    print(f\"     - Cell 6: Signal quality assessment\")\n",
    "    print(f\"     - Cell 7: Feature preservation analysis\")\n",
    "    print(f\"     - Cell 8: Comprehensive results compilation\")\n",
    "\n",
    "print(f\"\\nüèÅ Downscaled Phase 3 notebook execution complete!\")\n",
    "print(f\"Time finished: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
