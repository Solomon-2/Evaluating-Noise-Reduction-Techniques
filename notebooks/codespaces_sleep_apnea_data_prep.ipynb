{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Apnea Data Preparation - GitHub Codespaces (Batch Processing)\n",
    "\n",
    "**Rolling Download-Process-Delete Workflow for ~100 Patients**\n",
    "\n",
    "This notebook processes PhysioNet sleep apnea data in batches optimized for GitHub Codespaces:\n",
    "1. **Download 25 patients** ‚Üí Process with 16kHz feature extraction ‚Üí Save locally ‚Üí Delete raw data\n",
    "2. **Repeat for 4 batches** to process ~100 patients total\n",
    "3. **Resumable**: Can start from any batch if interrupted\n",
    "4. **Local storage**: Results saved to local filesystem (no cloud integration)\n",
    "\n",
    "---\n",
    "## üìã **CONFIGURATION** - Modify these parameters to resume processing\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 25          # Patients per batch\n",
    "START_BATCH = 1          # üîß CHANGE THIS to resume (1, 2, 3, or 4)\n",
    "END_BATCH = 4            # Target final batch\n",
    "TOTAL_PATIENTS = 100     # Total patients to process\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Fresh start: `START_BATCH = 1` (processes patients 1-25)\n",
    "- Resume after batch 1: `START_BATCH = 2` (processes patients 26-50)\n",
    "- Final batch only: `START_BATCH = 4` (processes patients 76-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "print(\"=== SLEEP APNEA DATA PREPARATION - CODESPACES BATCH PROCESSING ===\")\n",
    "print(\"Rolling Download-Process-Delete workflow for ~100 patients\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# üîß BATCH CONFIGURATION - MODIFY THESE TO RESUME PROCESSING\n",
    "# ============================================================================\n",
    "BATCH_SIZE = 25          # Patients per batch (optimal for Codespaces storage) \n",
    "START_BATCH = 1          # üîß CHANGE THIS: 1, 2, 3, or 4 to resume processing\n",
    "END_BATCH = 4            # Final batch number (4 batches = 100 patients)\n",
    "TOTAL_PATIENTS = 100     # Total patients to process across all batches\n",
    "\n",
    "# ============================================================================\n",
    "# üßµ DYNAMIC THREADING CONFIGURATION - ADJUST ANYTIME!\n",
    "# ============================================================================\n",
    "# üîß CHANGE THESE VALUES ANYTIME TO TEST DIFFERENT THREAD COUNTS:\n",
    "MAX_CONCURRENT_PATIENTS = 4  # üîß START HERE: Try 4, then 6, 8, 10... until you hit limits\n",
    "TIMEOUT_PER_PATIENT = 900     # 15 minutes timeout per patient\n",
    "ENABLE_THREADING = True       # Set to False to use original sequential processing\n",
    "\n",
    "# üí° CODESPACES THREADING OPTIMIZATION TIPS:\n",
    "print(f\"üßµ CODESPACES THREADING OPTIMIZATION GUIDE:\")\n",
    "print(f\"   üí° Codespaces typically has more CPU cores than Colab\")\n",
    "print(f\"   üí° Start with 4 threads, then try 6, 8, 10, 12...\")\n",
    "print(f\"   üí° Monitor: htop command or VS Code performance tab\")\n",
    "print(f\"   üí° Sweet spot: Usually 4-12 threads for Codespaces\")\n",
    "print(f\"   üí° Too many threads ‚Üí resource contention, slower performance\")\n",
    "print(f\"   üí° Signs of overload: High CPU usage, slower per-patient times\")\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATED VALUES - DO NOT MODIFY\n",
    "# ============================================================================\n",
    "patients_start = (START_BATCH - 1) * BATCH_SIZE + 1\n",
    "patients_end = START_BATCH * BATCH_SIZE\n",
    "if patients_end > TOTAL_PATIENTS:\n",
    "    patients_end = TOTAL_PATIENTS\n",
    "\n",
    "print(f\"\\nüìä BATCH CONFIGURATION:\")\n",
    "print(f\"   Current batch: {START_BATCH}/{END_BATCH}\")\n",
    "print(f\"   Patients in this batch: {patients_start}-{patients_end} ({patients_end - patients_start + 1} patients)\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Target total: {TOTAL_PATIENTS} patients\")\n",
    "\n",
    "print(f\"\\nüßµ CURRENT THREADING SETTINGS:\")\n",
    "print(f\"   Threading enabled: {ENABLE_THREADING}\")\n",
    "print(f\"   üîß Concurrent patients: {MAX_CONCURRENT_PATIENTS} (CHANGE THIS TO EXPERIMENT!)\")\n",
    "print(f\"   Timeout per patient: {TIMEOUT_PER_PATIENT/60:.1f} minutes\")\n",
    "print(f\"   Expected theoretical speedup: ~{MAX_CONCURRENT_PATIENTS}x faster\")\n",
    "print(f\"   üí° To test different thread counts: Change MAX_CONCURRENT_PATIENTS and re-run Cell 8\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOCAL STORAGE CONFIGURATION (CODESPACES)\n",
    "# ============================================================================\n",
    "OUTPUT_BASE_PATH = './sleep_apnea_data'  # Local directory for results\n",
    "CURRENT_BATCH_FILE = f'codespaces_dataset_batch{START_BATCH}.csv'\n",
    "LINKS_FILE = './download_links.txt'      # Local path to links file\n",
    "\n",
    "print(f\"\\nüíæ LOCAL STORAGE (CODESPACES):\")\n",
    "print(f\"   Save location: {OUTPUT_BASE_PATH}\")\n",
    "print(f\"   Current batch file: {CURRENT_BATCH_FILE}\")\n",
    "print(f\"   Links file: {LINKS_FILE}\")\n",
    "print(f\"   üí° All results saved locally (no cloud integration)\")\n",
    "\n",
    "# ============================================================================\n",
    "# AUDIO PROCESSING SETTINGS\n",
    "# ============================================================================\n",
    "TARGET_SAMPLE_RATE = 16000  # 16kHz for optimized processing\n",
    "FRAME_DURATION = 30.0       # seconds per frame\n",
    "OVERLAP_RATIO = 0.5         # 50% overlap between frames\n",
    "APNEA_THRESHOLD = 0.1       # 10% apnea overlap threshold for labeling\n",
    "AUDIO_CHANNEL = 'Mic'       # Audio channel to extract\n",
    "\n",
    "print(f\"\\nüéµ AUDIO PROCESSING:\")\n",
    "print(f\"   Sample rate: {TARGET_SAMPLE_RATE} Hz\")\n",
    "print(f\"   Frame duration: {FRAME_DURATION} seconds\")\n",
    "print(f\"   Frame overlap: {OVERLAP_RATIO * 100}%\")\n",
    "print(f\"   Apnea threshold: {APNEA_THRESHOLD * 100}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration complete. Ready to process batch {START_BATCH}.\")\n",
    "print(f\"üîß TO EXPERIMENT: Change MAX_CONCURRENT_PATIENTS (4‚Üí6‚Üí8‚Üí10...) and re-run Cell 8!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup Local Directories (Codespaces)\n",
    "print(\"üìÅ SETTING UP LOCAL DIRECTORIES...\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(OUTPUT_BASE_PATH, exist_ok=True)\n",
    "os.makedirs('./temp_patient_data', exist_ok=True)\n",
    "os.makedirs('./downloads', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Local directories created successfully\")\n",
    "print(f\"üìÇ Created directories:\")\n",
    "print(f\"   - {OUTPUT_BASE_PATH} (local save location)\")\n",
    "print(f\"   - ./temp_patient_data (temporary processing)\")\n",
    "print(f\"   - ./downloads (download staging)\")\n",
    "\n",
    "# Check if batch file already exists\n",
    "batch_file_path = os.path.join(OUTPUT_BASE_PATH, CURRENT_BATCH_FILE)\n",
    "if os.path.exists(batch_file_path):\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {CURRENT_BATCH_FILE} already exists locally!\")\n",
    "    print(f\"   File path: {batch_file_path}\")\n",
    "    print(f\"   Consider changing START_BATCH if this batch is already complete.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ {CURRENT_BATCH_FILE} does not exist. Ready for fresh processing.\")\n",
    "\n",
    "# Check if links file exists\n",
    "if os.path.exists(LINKS_FILE):\n",
    "    print(f\"\\n‚úÖ Links file found: {LINKS_FILE}\")\n",
    "    with open(LINKS_FILE, 'r') as f:\n",
    "        link_count = len([line for line in f if line.strip()])\n",
    "    print(f\"   Contains {link_count} download links\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Links file not found: {LINKS_FILE}\")\n",
    "    print(f\"   Please ensure download_links.txt is in the current directory\")\n",
    "\n",
    "# List existing batch files\n",
    "existing_batches = [f for f in os.listdir(OUTPUT_BASE_PATH) if f.startswith('codespaces_dataset_batch') and f.endswith('.csv')]\n",
    "if existing_batches:\n",
    "    print(f\"\\nüìä EXISTING BATCH FILES:\")\n",
    "    total_size_mb = 0\n",
    "    for batch_file in sorted(existing_batches):\n",
    "        file_path = os.path.join(OUTPUT_BASE_PATH, batch_file)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size_mb += file_size_mb\n",
    "        print(f\"   - {batch_file} ({file_size_mb:.1f} MB)\")\n",
    "    print(f\"   Total: {total_size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"\\nüìä No existing batch files found. This appears to be a fresh start.\")\n",
    "\n",
    "# Show disk space\n",
    "import shutil\n",
    "disk_usage = shutil.disk_usage('.')\n",
    "free_space_gb = disk_usage.free / (1024**3)\n",
    "total_space_gb = disk_usage.total / (1024**3)\n",
    "print(f\"\\nüíæ DISK SPACE:\")\n",
    "print(f\"   Available: {free_space_gb:.1f} GB / {total_space_gb:.1f} GB\")\n",
    "print(f\"   Estimated space per batch: ~2-5 GB (depending on compression)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Install Dependencies and Import Libraries\n",
    "print(\"üì¶ INSTALLING DEPENDENCIES...\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install librosa mne tqdm psutil\n",
    "\n",
    "print(\"\\nüìö IMPORTING LIBRARIES...\")\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import mne\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# XML processing (will need to recreate extract_apnea_events function)\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Threading for parallel processing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# System monitoring\n",
    "import psutil\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üîä librosa version: {librosa.__version__}\")\n",
    "print(f\"üß† mne version: {mne.__version__}\")\n",
    "print(f\"üî¢ numpy version: {np.__version__}\")\n",
    "print(f\"üêº pandas version: {pd.__version__}\")\n",
    "print(f\"üßµ Threading support: {threading.active_count()} active threads\")\n",
    "print(f\"üñ•Ô∏è System info: {psutil.cpu_count()} CPUs, {psutil.virtual_memory().total / (1024**3):.1f} GB RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: XML Annotation Parser (Recreated from working_with_xml.py)\n",
    "print(\"üîç DEFINING XML ANNOTATION PARSER...\")\n",
    "\n",
    "def extract_apnea_events(rml_file_path, output_csv=None):\n",
    "    \"\"\"\n",
    "    Extract apnea events from RML annotation files.\n",
    "    Recreated from working_with_xml.py for Codespaces compatibility.\n",
    "    \n",
    "    Args:\n",
    "        rml_file_path (str): Path to the RML file\n",
    "        output_csv (str, optional): Path to save CSV output\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (event_type, start_time, end_time)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(rml_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        apnea_events = []\n",
    "        \n",
    "        # Find all scored events\n",
    "        for scored_event in root.findall('.//ScoredEvent'):\n",
    "            # Get event name/type\n",
    "            name_elem = scored_event.find('Name')\n",
    "            if name_elem is None:\n",
    "                continue\n",
    "                \n",
    "            event_name = name_elem.text\n",
    "            \n",
    "            # Filter for apnea-related events\n",
    "            apnea_keywords = ['Apnea', 'Hypopnea', 'apnea', 'hypopnea']\n",
    "            if not any(keyword in event_name for keyword in apnea_keywords):\n",
    "                continue\n",
    "            \n",
    "            # Get start time\n",
    "            start_elem = scored_event.find('Start')\n",
    "            if start_elem is None:\n",
    "                continue\n",
    "            start_time = float(start_elem.text)\n",
    "            \n",
    "            # Get duration\n",
    "            duration_elem = scored_event.find('Duration')\n",
    "            if duration_elem is None:\n",
    "                continue\n",
    "            duration = float(duration_elem.text)\n",
    "            \n",
    "            # Calculate end time\n",
    "            end_time = start_time + duration\n",
    "            \n",
    "            apnea_events.append((event_name, start_time, end_time))\n",
    "        \n",
    "        # Save to CSV if requested\n",
    "        if output_csv:\n",
    "            df = pd.DataFrame(apnea_events, columns=['EventType', 'StartTime', 'EndTime'])\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            print(f\"   üíæ Saved {len(apnea_events)} events to {output_csv}\")\n",
    "        \n",
    "        return apnea_events\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error parsing {rml_file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ XML annotation parser defined\")\n",
    "print(\"   - Extracts apnea events from RML files\")\n",
    "print(\"   - Filters for Apnea/Hypopnea events\")\n",
    "print(\"   - Returns (event_type, start_time, end_time) tuples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Download Functions (Enhanced from file download.ipynb)\n",
    "print(\"üì• DEFINING DOWNLOAD FUNCTIONS...\")\n",
    "\n",
    "def group_links_by_patient(links_content):\n",
    "    \"\"\"\n",
    "    Groups download URLs by patient ID.\n",
    "    Modified to work with links content directly (not file path).\n",
    "    \"\"\"\n",
    "    grouped_data = {}\n",
    "    patient_id_regex = re.compile(r'(\\d{8}-\\d{6})')\n",
    "    \n",
    "    for url in links_content.strip().split('\\n'):\n",
    "        url = url.strip()\n",
    "        if not url:\n",
    "            continue\n",
    "            \n",
    "        match = patient_id_regex.search(url)\n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        patient_id = match.group(1)\n",
    "        if patient_id not in grouped_data:\n",
    "            grouped_data[patient_id] = {'rml': None, 'edf': []}\n",
    "            \n",
    "        if url.endswith('.rml'):\n",
    "            grouped_data[patient_id]['rml'] = url\n",
    "        elif url.endswith('.edf'):\n",
    "            grouped_data[patient_id]['edf'].append(url)\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "def download_file_with_retry(url, local_path, max_retries=3, base_delay=2):\n",
    "    \"\"\"\n",
    "    Downloads a file with retry logic and resume capability.\n",
    "    Enhanced from original with better error handling.\n",
    "    \"\"\"\n",
    "    # Check if file already exists and is complete\n",
    "    if os.path.exists(local_path):\n",
    "        try:\n",
    "            local_size = os.path.getsize(local_path)\n",
    "            if local_size > 1000:  # Assume files > 1KB are likely complete\n",
    "                print(f\"      ‚úì File exists: {os.path.basename(local_path)} ({local_size/1024:.1f} KB)\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Download with retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"      üì• Downloading: {os.path.basename(local_path)} (attempt {attempt + 1}/{max_retries})\")\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            \n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                \n",
    "                # Download to temporary file first\n",
    "                temp_path = local_path + '.tmp'\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Move to final location\n",
    "                shutil.move(temp_path, local_path)\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                print(f\"      ‚úÖ Downloaded: {os.path.basename(local_path)} ({file_size/1024:.1f} KB)\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Attempt {attempt + 1} failed: {str(e)[:100]}...\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"      ‚è≥ Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    # Clean up temp file if exists\n",
    "    temp_path = local_path + '.tmp'\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def download_patient_data(patient_original_id, patient_files, patient_folder):\n",
    "    \"\"\"\n",
    "    Downloads all files for a single patient.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"   üìÇ Downloading to: {patient_folder}\")\n",
    "    \n",
    "    success = True\n",
    "    \n",
    "    # Download RML file\n",
    "    if patient_files['rml']:\n",
    "        rml_url = patient_files['rml']\n",
    "        rml_filename_match = re.search(r'fileName=([^&]+)', rml_url)\n",
    "        rml_filename = rml_filename_match.group(1) if rml_filename_match else os.path.basename(rml_url).split('?')[0]\n",
    "        rml_filename = requests.utils.unquote(rml_filename)\n",
    "        rml_path = os.path.join(patient_folder, rml_filename)\n",
    "        \n",
    "        if not download_file_with_retry(rml_url, rml_path):\n",
    "            success = False\n",
    "            print(f\"      ‚ùå Failed to download RML file\")\n",
    "    \n",
    "    # Download EDF files\n",
    "    edf_success_count = 0\n",
    "    for edf_url in patient_files['edf']:\n",
    "        edf_filename_match = re.search(r'fileName=([^&]+)', edf_url)\n",
    "        edf_filename = edf_filename_match.group(1) if edf_filename_match else os.path.basename(edf_url).split('?')[0]\n",
    "        edf_filename = requests.utils.unquote(edf_filename)\n",
    "        edf_path = os.path.join(patient_folder, edf_filename)\n",
    "        \n",
    "        # Also download corresponding .hea file\n",
    "        hea_url = edf_url.replace('.edf', '.hea')\n",
    "        hea_filename = edf_filename.replace('.edf', '.hea')\n",
    "        hea_path = os.path.join(patient_folder, hea_filename)\n",
    "        \n",
    "        edf_ok = download_file_with_retry(edf_url, edf_path)\n",
    "        hea_ok = download_file_with_retry(hea_url, hea_path)\n",
    "        \n",
    "        if edf_ok and hea_ok:\n",
    "            edf_success_count += 1\n",
    "        else:\n",
    "            print(f\"      ‚ùå Failed EDF/HEA pair: {edf_filename}\")\n",
    "    \n",
    "    total_edf_count = len(patient_files['edf'])\n",
    "    print(f\"   üìä Downloaded {edf_success_count}/{total_edf_count} EDF files\")\n",
    "    \n",
    "    return success and edf_success_count > 0\n",
    "\n",
    "print(\"‚úÖ Download functions defined\")\n",
    "print(\"   - group_links_by_patient(): Groups URLs by patient ID\")\n",
    "print(\"   - download_file_with_retry(): Downloads with retry logic\")\n",
    "print(\"   - download_patient_data(): Downloads all files for one patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Feature Extraction Functions (Enhanced from parallel_feature_extraction.ipynb)\n",
    "print(\"üéµ DEFINING FEATURE EXTRACTION FUNCTIONS...\")\n",
    "\n",
    "def extract_comprehensive_features(audio_frame, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features for sleep apnea detection.\n",
    "    Enhanced from parallel_feature_extraction.ipynb with 16kHz optimization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(audio_frame) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Basic acoustic features\n",
    "        rms = float(librosa.feature.rms(y=audio_frame).mean())\n",
    "        zcr = float(librosa.feature.zero_crossing_rate(y=audio_frame).mean())\n",
    "        centroid = float(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n",
    "        bandwidth = float(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n",
    "        rolloff = float(librosa.feature.spectral_rolloff(y=audio_frame, sr=sample_rate).mean())\n",
    "        \n",
    "        # MFCCs (first 8 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=8)\n",
    "        mfcc_means = mfccs.mean(axis=1)\n",
    "        mfcc_stds = mfccs.std(axis=1)\n",
    "        \n",
    "        # Temporal features for breathing patterns (5-second windows)\n",
    "        window_size = int(5 * sample_rate)  # 5 seconds\n",
    "        num_windows = len(audio_frame) // window_size\n",
    "        \n",
    "        if num_windows >= 2:\n",
    "            rms_windows = []\n",
    "            zcr_windows = []\n",
    "            \n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window = audio_frame[start_idx:end_idx]\n",
    "                \n",
    "                rms_windows.append(librosa.feature.rms(y=window).mean())\n",
    "                zcr_windows.append(librosa.feature.zero_crossing_rate(y=window).mean())\n",
    "            \n",
    "            rms_variability = float(np.std(rms_windows))\n",
    "            zcr_variability = float(np.std(zcr_windows))\n",
    "            breathing_regularity = float(1.0 / (1.0 + rms_variability))\n",
    "        else:\n",
    "            rms_variability = 0.0\n",
    "            zcr_variability = 0.0\n",
    "            breathing_regularity = 0.5\n",
    "        \n",
    "        # Silence detection\n",
    "        silence_threshold = np.percentile(np.abs(audio_frame), 20)\n",
    "        silence_mask = np.abs(audio_frame) < silence_threshold\n",
    "        silence_ratio = float(np.mean(silence_mask))\n",
    "        \n",
    "        # Breathing pause detection\n",
    "        silence_changes = np.diff(silence_mask.astype(int))\n",
    "        pause_starts = np.where(silence_changes == 1)[0]\n",
    "        pause_ends = np.where(silence_changes == -1)[0]\n",
    "        \n",
    "        if len(pause_starts) > 0 and len(pause_ends) > 0:\n",
    "            if len(pause_ends) < len(pause_starts):\n",
    "                pause_ends = np.append(pause_ends, len(audio_frame))\n",
    "            pause_durations = (pause_ends[:len(pause_starts)] - pause_starts) / sample_rate\n",
    "            avg_pause_duration = float(np.mean(pause_durations))\n",
    "            max_pause_duration = float(np.max(pause_durations))\n",
    "        else:\n",
    "            avg_pause_duration = 0.0\n",
    "            max_pause_duration = 0.0\n",
    "        \n",
    "        # Combine all features\n",
    "        features = {\n",
    "            'clean_rms': rms,\n",
    "            'clean_zcr': zcr,\n",
    "            'clean_centroid': centroid,\n",
    "            'clean_bandwidth': bandwidth,\n",
    "            'clean_rolloff': rolloff,\n",
    "            'clean_rms_variability': rms_variability,\n",
    "            'clean_zcr_variability': zcr_variability,\n",
    "            'clean_breathing_regularity': breathing_regularity,\n",
    "            'clean_silence_ratio': silence_ratio,\n",
    "            'clean_avg_pause_duration': avg_pause_duration,\n",
    "            'clean_max_pause_duration': max_pause_duration\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs\n",
    "        for i, (mean_val, std_val) in enumerate(zip(mfcc_means, mfcc_stds), 1):\n",
    "            features[f'clean_mfcc_{i}_mean'] = float(mean_val)\n",
    "            features[f'clean_mfcc_{i}_std'] = float(std_val)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Feature extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_apnea_label(timestamp, duration, apnea_events, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculate apnea label based on overlap with annotated events.\n",
    "    Uses proportion-based labeling with configurable threshold.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frame_end = timestamp + duration\n",
    "        apnea_seconds = 0\n",
    "        \n",
    "        for _, start, end in apnea_events:\n",
    "            overlap_start = max(timestamp, start)\n",
    "            overlap_end = min(frame_end, end)\n",
    "            if overlap_start < overlap_end:\n",
    "                apnea_seconds += (overlap_end - overlap_start)\n",
    "        \n",
    "        proportion = apnea_seconds / duration\n",
    "        label = 1 if proportion > threshold else 0\n",
    "        return label, proportion\n",
    "    except:\n",
    "        return 0, 0.0\n",
    "\n",
    "def process_patient_edf_files(patient_folder, patient_id):\n",
    "    \"\"\"\n",
    "    Process all EDF files for a single patient and extract features.\n",
    "    Returns list of feature records for the patient.\n",
    "    \"\"\"\n",
    "    print(f\"   üéµ Processing audio files for {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Find EDF and RML files\n",
    "        edf_files = sorted([f for f in os.listdir(patient_folder) if f.endswith('.edf')])\n",
    "        rml_files = [f for f in os.listdir(patient_folder) if f.endswith('.rml')]\n",
    "        \n",
    "        if not edf_files or not rml_files:\n",
    "            print(f\"      ‚ùå Missing files: {len(edf_files)} EDF, {len(rml_files)} RML\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"      üìÅ Found {len(edf_files)} EDF and {len(rml_files)} RML files\")\n",
    "        \n",
    "        # Load apnea events\n",
    "        rml_path = os.path.join(patient_folder, rml_files[0])\n",
    "        apnea_events = extract_apnea_events(rml_path)\n",
    "        print(f\"      üìã Loaded {len(apnea_events)} apnea events\")\n",
    "        \n",
    "        # Process each EDF file\n",
    "        all_features = []\n",
    "        \n",
    "        for edf_idx, edf_file in enumerate(edf_files, 1):\n",
    "            print(f\"      üéµ Processing EDF {edf_idx}/{len(edf_files)}: {edf_file}\")\n",
    "            \n",
    "            try:\n",
    "                edf_path = os.path.join(patient_folder, edf_file)\n",
    "                raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)\n",
    "                \n",
    "                if AUDIO_CHANNEL not in raw.ch_names:\n",
    "                    print(f\"         ‚ö†Ô∏è No {AUDIO_CHANNEL} channel, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                raw.pick_channels([AUDIO_CHANNEL])\n",
    "                original_sample_rate = int(raw.info['sfreq'])\n",
    "                duration_min = raw.n_times / original_sample_rate / 60\n",
    "                \n",
    "                print(f\"         ‚è±Ô∏è Duration: {duration_min:.1f} min, {original_sample_rate} Hz ‚Üí {TARGET_SAMPLE_RATE} Hz\")\n",
    "                \n",
    "                # Frame parameters\n",
    "                original_frame_samples = int(FRAME_DURATION * original_sample_rate)\n",
    "                original_step_samples = int(original_frame_samples * (1 - OVERLAP_RATIO))\n",
    "                \n",
    "                # Time offset for multi-EDF processing\n",
    "                time_offset = (edf_idx - 1) * 60 * 60  # Each EDF ‚âà 1 hour\n",
    "                \n",
    "                frame_count = 0\n",
    "                for frame_start in range(0, raw.n_times - original_frame_samples + 1, original_step_samples):\n",
    "                    frame_end = frame_start + original_frame_samples\n",
    "                    timestamp = (frame_start / original_sample_rate) + time_offset\n",
    "                    \n",
    "                    # Load and downsample audio frame\n",
    "                    try:\n",
    "                        audio_frame, _ = raw[:, frame_start:frame_end]\n",
    "                        audio_frame = audio_frame.flatten()\n",
    "                        \n",
    "                        # Downsample to target rate\n",
    "                        if original_sample_rate != TARGET_SAMPLE_RATE:\n",
    "                            audio_frame = librosa.resample(\n",
    "                                audio_frame, \n",
    "                                orig_sr=original_sample_rate, \n",
    "                                target_sr=TARGET_SAMPLE_RATE\n",
    "                            )\n",
    "                        \n",
    "                        # Extract features\n",
    "                        features = extract_comprehensive_features(audio_frame, TARGET_SAMPLE_RATE)\n",
    "                        if features is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get apnea label\n",
    "                        apnea_label, apnea_proportion = get_apnea_label(\n",
    "                            timestamp, FRAME_DURATION, apnea_events, APNEA_THRESHOLD\n",
    "                        )\n",
    "                        \n",
    "                        # Create record\n",
    "                        record = {\n",
    "                            'patient_id': patient_id,\n",
    "                            'edf_file': edf_file,\n",
    "                            'timestamp': float(timestamp),\n",
    "                            'frame_duration': FRAME_DURATION,\n",
    "                            'sample_rate': TARGET_SAMPLE_RATE,\n",
    "                            'apnea_label': int(apnea_label),\n",
    "                            'apnea_proportion': float(apnea_proportion),\n",
    "                            **features\n",
    "                        }\n",
    "                        \n",
    "                        all_features.append(record)\n",
    "                        frame_count += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"         ‚ö†Ô∏è Frame {frame_count} failed: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"         ‚úÖ Extracted {frame_count} frames\")\n",
    "                del raw  # Free memory\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"         ‚ùå EDF processing failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        apnea_count = sum(1 for f in all_features if f['apnea_label'] == 1)\n",
    "        print(f\"   ‚úÖ {patient_id}: {len(all_features)} frames, {apnea_count} apnea\")\n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {patient_id}: Processing failed: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")\n",
    "print(\"   - extract_comprehensive_features(): 27 audio features with 16kHz optimization\")\n",
    "print(\"   - get_apnea_label(): Proportion-based labeling with 10% threshold\")\n",
    "print(\"   - process_patient_edf_files(): Full patient processing pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load Download Links File\n",
    "print(\"üìé LOADING DOWNLOAD LINKS FILE\")\n",
    "print(f\"Loading from: {LINKS_FILE}\")\n",
    "\n",
    "# Load and parse the links\n",
    "try:\n",
    "    with open(LINKS_FILE, 'r') as f:\n",
    "        links_content = f.read()\n",
    "    \n",
    "    print(f\"‚úÖ {LINKS_FILE} loaded successfully ({len(links_content)} characters)\")\n",
    "    \n",
    "    # Group links by patient\n",
    "    grouped_links = group_links_by_patient(links_content)\n",
    "    print(f\"üìä Found {len(grouped_links)} unique patients in links file\")\n",
    "    \n",
    "    # Show sample of available patients\n",
    "    valid_patients = [pid for pid, files in grouped_links.items() if files['rml'] and files['edf']]\n",
    "    print(f\"‚úÖ {len(valid_patients)} patients have both RML and EDF files\")\n",
    "    \n",
    "    if len(valid_patients) >= TOTAL_PATIENTS:\n",
    "        print(f\"üéØ Sufficient patients available for target of {TOTAL_PATIENTS}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Only {len(valid_patients)} valid patients found, less than target {TOTAL_PATIENTS}\")\n",
    "        \n",
    "    # Show first few patient IDs as example\n",
    "    print(f\"\\nüìã Sample patient IDs:\")\n",
    "    for i, pid in enumerate(list(valid_patients)[:5]):\n",
    "        files_info = grouped_links[pid]\n",
    "        print(f\"   {i+1}. {pid}: {len(files_info['edf'])} EDF files, {'‚úì' if files_info['rml'] else '‚úó'} RML\")\n",
    "    \n",
    "    if len(valid_patients) > 5:\n",
    "        print(f\"   ... and {len(valid_patients) - 5} more patients\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå {LINKS_FILE} not found\")\n",
    "    print(f\"Please ensure download_links.txt is in the current directory\")\n",
    "    raise Exception(\"Download links file is required to proceed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading links file: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n‚úÖ Links file processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main Batch Processing Loop (Dynamic Threading for Codespaces)\n",
    "print(f\"üöÄ STARTING BATCH {START_BATCH} PROCESSING (CODESPACES)\")\n",
    "print(f\"Processing patients {patients_start}-{patients_end}\")\n",
    "print(f\"Threading: {'ENABLED' if ENABLE_THREADING else 'DISABLED'} ({MAX_CONCURRENT_PATIENTS} concurrent)\" if ENABLE_THREADING else \"Sequential processing\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# üîß DYNAMIC THREADING: Read current configuration\n",
    "current_threads = MAX_CONCURRENT_PATIENTS\n",
    "current_timeout = TIMEOUT_PER_PATIENT\n",
    "\n",
    "print(f\"üßµ DYNAMIC THREADING STATUS (CODESPACES):\")\n",
    "print(f\"   Current thread count: {current_threads}\")\n",
    "print(f\"   üí° TO EXPERIMENT: Change MAX_CONCURRENT_PATIENTS in Cell 1 and re-run this cell\")\n",
    "print(f\"   üí° CODESPACES TRY: 4 ‚Üí 6 ‚Üí 8 ‚Üí 10 ‚Üí 12... until you see diminishing returns\")\n",
    "print(f\"   üí° MONITOR: Use 'htop' command in terminal or VS Code performance tab\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get system information for monitoring resource usage\"\"\"\n",
    "    try:\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        memory = psutil.virtual_memory()\n",
    "        disk = psutil.disk_usage('.')\n",
    "        \n",
    "        return {\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'cpu_count': psutil.cpu_count(),\n",
    "            'memory_used_gb': memory.used / (1024**3),\n",
    "            'memory_total_gb': memory.total / (1024**3),\n",
    "            'memory_percent': memory.percent,\n",
    "            'disk_used_gb': disk.used / (1024**3),\n",
    "            'disk_free_gb': disk.free / (1024**3)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not get system info: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_single_patient_threading(patient_info):\n",
    "    \"\"\"\n",
    "    Process a single patient in a thread-safe manner.\n",
    "    Enhanced with resource monitoring and thread identification.\n",
    "    Returns: (success, patient_features, stats)\n",
    "    \"\"\"\n",
    "    patient_original_id, patient_number, patient_idx = patient_info\n",
    "    patient_id = f\"patient_{patient_number:02d}\"\n",
    "    patient_folder = f\"./temp_patient_data/{patient_id}\"\n",
    "    thread_id = threading.current_thread().name\n",
    "    \n",
    "    print(f\"\\n[{thread_id}] --- Processing {patient_id} (Original: {patient_original_id}) ---\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    stats = {\n",
    "        'patient_id': patient_id,\n",
    "        'original_id': patient_original_id,\n",
    "        'thread_id': thread_id,\n",
    "        'thread_count_used': current_threads,\n",
    "        'success': False,\n",
    "        'frames_extracted': 0,\n",
    "        'apnea_frames': 0,\n",
    "        'processing_time': 0,\n",
    "        'download_time': 0,\n",
    "        'feature_extraction_time': 0,\n",
    "        'cleanup_time': 0,\n",
    "        'error_message': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Download patient data\n",
    "        download_start = time.time()\n",
    "        print(f\"[{thread_id}] üì• Step 1/3: Downloading {patient_id}...\")\n",
    "        patient_files = grouped_links[patient_original_id]\n",
    "        \n",
    "        download_success = download_patient_data(patient_original_id, patient_files, patient_folder)\n",
    "        stats['download_time'] = time.time() - download_start\n",
    "        \n",
    "        if not download_success:\n",
    "            stats['error_message'] = \"Download failed\"\n",
    "            print(f\"[{thread_id}] ‚ùå Download failed for {patient_id} (took {stats['download_time']:.1f}s)\")\n",
    "            return False, [], stats\n",
    "        \n",
    "        print(f\"[{thread_id}] ‚úÖ Download successful for {patient_id} (took {stats['download_time']:.1f}s)\")\n",
    "        \n",
    "        # Step 2: Process and extract features\n",
    "        extraction_start = time.time()\n",
    "        print(f\"[{thread_id}] üéµ Step 2/3: Extracting features for {patient_id}...\")\n",
    "        patient_features = process_patient_edf_files(patient_folder, patient_id)\n",
    "        stats['feature_extraction_time'] = time.time() - extraction_start\n",
    "        \n",
    "        if not patient_features:\n",
    "            stats['error_message'] = \"Feature extraction failed\"\n",
    "            print(f\"[{thread_id}] ‚ùå Feature extraction failed for {patient_id} (took {stats['feature_extraction_time']:.1f}s)\")\n",
    "            return False, [], stats\n",
    "        \n",
    "        # Update stats\n",
    "        stats['frames_extracted'] = len(patient_features)\n",
    "        stats['apnea_frames'] = sum(1 for f in patient_features if f['apnea_label'] == 1)\n",
    "        stats['success'] = True\n",
    "        \n",
    "        print(f\"[{thread_id}] ‚úÖ Features extracted for {patient_id}: {len(patient_features)} frames (took {stats['feature_extraction_time']:.1f}s)\")\n",
    "        \n",
    "        return True, patient_features, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        stats['error_message'] = str(e)\n",
    "        print(f\"[{thread_id}] ‚ùå Critical error processing {patient_id}: {e}\")\n",
    "        return False, [], stats\n",
    "        \n",
    "    finally:\n",
    "        # Step 3: Clean up temporary files (always runs)\n",
    "        cleanup_start = time.time()\n",
    "        stats['processing_time'] = time.time() - start_time\n",
    "        \n",
    "        print(f\"[{thread_id}] üóëÔ∏è Step 3/3: Cleaning up {patient_id}...\")\n",
    "        if os.path.exists(patient_folder):\n",
    "            try:\n",
    "                shutil.rmtree(patient_folder)\n",
    "                stats['cleanup_time'] = time.time() - cleanup_start\n",
    "                print(f\"[{thread_id}] ‚úÖ Cleanup successful for {patient_id} (took {stats['cleanup_time']:.1f}s)\")\n",
    "            except Exception as cleanup_error:\n",
    "                stats['cleanup_time'] = time.time() - cleanup_start\n",
    "                print(f\"[{thread_id}] ‚ö†Ô∏è Cleanup warning for {patient_id}: {cleanup_error}\")\n",
    "\n",
    "# Main processing logic\n",
    "batch_start_time = time.time()\n",
    "batch_features = []  # Thread-safe accumulation\n",
    "successful_patients = 0\n",
    "failed_patients = 0\n",
    "processing_stats = []\n",
    "\n",
    "# Get initial system info\n",
    "initial_system_info = get_system_info()\n",
    "if initial_system_info:\n",
    "    print(f\"\\nüìä INITIAL CODESPACES SYSTEM STATUS:\")\n",
    "    print(f\"   CPUs: {initial_system_info['cpu_count']} cores\")\n",
    "    print(f\"   CPU: {initial_system_info['cpu_percent']:.1f}%\")\n",
    "    print(f\"   RAM: {initial_system_info['memory_used_gb']:.1f}/{initial_system_info['memory_total_gb']:.1f} GB ({initial_system_info['memory_percent']:.1f}%)\")\n",
    "    print(f\"   Disk: {initial_system_info['disk_free_gb']:.1f} GB free\")\n",
    "\n",
    "# Select patients for this batch\n",
    "valid_patients = [pid for pid, files in grouped_links.items() if files['rml'] and files['edf']]\n",
    "batch_patients = valid_patients[patients_start-1:patients_end]  # Convert to 0-based indexing\n",
    "\n",
    "print(f\"\\nüìã Selected {len(batch_patients)} patients for batch {START_BATCH}\")\n",
    "\n",
    "# Prepare patient info tuples\n",
    "patient_info_list = [\n",
    "    (patient_original_id, patients_start + patient_idx, patient_idx)\n",
    "    for patient_idx, patient_original_id in enumerate(batch_patients)\n",
    "]\n",
    "\n",
    "# Threading lock for thread-safe operations\n",
    "results_lock = threading.Lock()\n",
    "\n",
    "if ENABLE_THREADING:\n",
    "    print(f\"\\nüßµ STARTING THREADED PROCESSING (CODESPACES):\")\n",
    "    print(f\"   üîß Concurrent patients: {current_threads} (configurable in Cell 1)\")\n",
    "    print(f\"   Timeout per patient: {current_timeout/60:.1f} minutes\")\n",
    "    print(f\"   Total patients: {len(patient_info_list)}\")\n",
    "    print(f\"   üí° CODESPACES TIP: Try higher thread counts than Colab!\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=current_threads, thread_name_prefix=\"Patient\") as executor:\n",
    "        # Submit all patient processing jobs\n",
    "        future_to_patient = {\n",
    "            executor.submit(process_single_patient_threading, patient_info): patient_info[0]\n",
    "            for patient_info in patient_info_list\n",
    "        }\n",
    "        \n",
    "        print(f\"üì§ Submitted {len(future_to_patient)} patient processing jobs to {current_threads} threads\")\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_patient, timeout=current_timeout * len(patient_info_list)):\n",
    "            patient_original_id = future_to_patient[future]\n",
    "            \n",
    "            try:\n",
    "                # Get result with individual patient timeout\n",
    "                success, patient_features, stats = future.result(timeout=current_timeout)\n",
    "                \n",
    "                # Thread-safe result accumulation\n",
    "                with results_lock:\n",
    "                    processing_stats.append(stats)\n",
    "                    \n",
    "                    if success:\n",
    "                        batch_features.extend(patient_features)\n",
    "                        successful_patients += 1\n",
    "                        \n",
    "                        # Progress update with timing details\n",
    "                        total_frames = len(batch_features)\n",
    "                        apnea_frames = sum(1 for f in batch_features if f['apnea_label'] == 1)\n",
    "                        \n",
    "                        print(f\"üéØ [{stats['thread_id']}] SUCCESS: {stats['patient_id']} - {stats['frames_extracted']} frames\")\n",
    "                        print(f\"   ‚è±Ô∏è Timing: Download {stats['download_time']:.1f}s | Processing {stats['feature_extraction_time']:.1f}s | Cleanup {stats['cleanup_time']:.1f}s\")\n",
    "                        print(f\"üìä Batch progress: {successful_patients + failed_patients}/{len(patient_info_list)} patients, {total_frames} total frames\")\n",
    "                    else:\n",
    "                        failed_patients += 1\n",
    "                        print(f\"‚ùå [{stats['thread_id']}] FAILED: {stats['patient_id']} - {stats['error_message']}\")\n",
    "                        \n",
    "                    # Show system status every 5 patients\n",
    "                    if (successful_patients + failed_patients) % 5 == 0:\n",
    "                        current_system_info = get_system_info()\n",
    "                        if current_system_info:\n",
    "                            print(f\"üìä CODESPACES STATUS: CPU {current_system_info['cpu_percent']:.1f}% | RAM {current_system_info['memory_percent']:.1f}% | Disk {current_system_info['disk_free_gb']:.1f}GB free\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                with results_lock:\n",
    "                    failed_patients += 1\n",
    "                print(f\"‚ùå Exception for {patient_original_id}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nüìú STARTING SEQUENTIAL PROCESSING:\")\n",
    "    \n",
    "    # Sequential processing (original method)\n",
    "    for patient_info in tqdm(patient_info_list, desc=f\"Batch {START_BATCH} Progress\"):\n",
    "        success, patient_features, stats = process_single_patient_threading(patient_info)\n",
    "        \n",
    "        processing_stats.append(stats)\n",
    "        \n",
    "        if success:\n",
    "            batch_features.extend(patient_features)\n",
    "            successful_patients += 1\n",
    "        else:\n",
    "            failed_patients += 1\n",
    "        \n",
    "        # Show progress\n",
    "        total_frames = len(batch_features)\n",
    "        apnea_frames = sum(1 for f in batch_features if f['apnea_label'] == 1)\n",
    "        print(f\"üìä Progress: {successful_patients + failed_patients}/{len(patient_info_list)} patients, {total_frames} frames\")\n",
    "\n",
    "batch_elapsed = time.time() - batch_start_time\n",
    "\n",
    "# Final batch statistics with threading analysis\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÅ BATCH {START_BATCH} PROCESSING COMPLETE (CODESPACES)!\")\n",
    "print(f\"‚è±Ô∏è Total time: {batch_elapsed/60:.1f} minutes\")\n",
    "print(f\"üßµ Threading: {'ENABLED' if ENABLE_THREADING else 'DISABLED'} (used {current_threads} threads)\")\n",
    "print(f\"‚úÖ Successful patients: {successful_patients}\")\n",
    "print(f\"‚ùå Failed patients: {failed_patients}\")\n",
    "print(f\"üìä Total frames extracted: {len(batch_features):,}\")\n",
    "\n",
    "if batch_features:\n",
    "    apnea_count = sum(1 for f in batch_features if f['apnea_label'] == 1)\n",
    "    apnea_rate = (apnea_count / len(batch_features)) * 100\n",
    "    print(f\"üö® Apnea frames: {apnea_count:,} ({apnea_rate:.1f}%)\")\n",
    "    print(f\"üò¥ Normal frames: {len(batch_features) - apnea_count:,} ({100-apnea_rate:.1f}%)\")\n",
    "    \n",
    "    # Enhanced threading performance analysis for Codespaces\n",
    "    if ENABLE_THREADING and processing_stats:\n",
    "        successful_stats = [s for s in processing_stats if s['success']]\n",
    "        if successful_stats:\n",
    "            avg_total_time = sum(s['processing_time'] for s in successful_stats) / len(successful_stats)\n",
    "            avg_download_time = sum(s['download_time'] for s in successful_stats) / len(successful_stats)\n",
    "            avg_processing_time = sum(s['feature_extraction_time'] for s in successful_stats) / len(successful_stats)\n",
    "            avg_cleanup_time = sum(s['cleanup_time'] for s in successful_stats) / len(successful_stats)\n",
    "            \n",
    "            estimated_sequential_time = avg_total_time * len(patient_info_list)\n",
    "            actual_speedup = estimated_sequential_time / batch_elapsed\n",
    "            theoretical_speedup = current_threads\n",
    "            efficiency = (actual_speedup / theoretical_speedup) * 100\n",
    "            \n",
    "            print(f\"\\nüöÄ CODESPACES THREADING PERFORMANCE ANALYSIS:\")\n",
    "            print(f\"   üîß Thread count used: {current_threads}\")\n",
    "            print(f\"   ‚è±Ô∏è Average times per patient:\")\n",
    "            print(f\"      - Download: {avg_download_time/60:.1f} minutes\")\n",
    "            print(f\"      - Processing: {avg_processing_time/60:.1f} minutes\") \n",
    "            print(f\"      - Cleanup: {avg_cleanup_time:.1f} seconds\")\n",
    "            print(f\"      - Total: {avg_total_time/60:.1f} minutes\")\n",
    "            print(f\"   üèÉ Speedup achieved: {actual_speedup:.1f}x (vs sequential)\")\n",
    "            print(f\"   üéØ Theoretical max: {theoretical_speedup}x\")\n",
    "            print(f\"   üìà Threading efficiency: {efficiency:.1f}%\")\n",
    "            \n",
    "            # Codespaces-specific recommendations\n",
    "            print(f\"\\nüí° CODESPACES OPTIMIZATION SUGGESTIONS:\")\n",
    "            if efficiency > 80:\n",
    "                print(f\"   ‚úÖ Excellent efficiency! Try increasing to {current_threads + 2} threads\")\n",
    "                print(f\"   üí° Codespaces can often handle higher thread counts than Colab\")\n",
    "            elif efficiency > 60:\n",
    "                print(f\"   ‚ö†Ô∏è Good efficiency. Try {current_threads + 2} threads but monitor resources\")\n",
    "            elif efficiency > 40:\n",
    "                print(f\"   ‚ö†Ô∏è Moderate efficiency. Consider staying at {current_threads} threads\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Low efficiency. Try reducing to {max(2, current_threads - 2)} threads\")\n",
    "                \n",
    "            print(f\"   üìä Monitor with 'htop' command and adjust MAX_CONCURRENT_PATIENTS accordingly\")\n",
    "\n",
    "# Final system status\n",
    "final_system_info = get_system_info()\n",
    "if final_system_info and initial_system_info:\n",
    "    print(f\"\\nüìä FINAL CODESPACES SYSTEM STATUS:\")\n",
    "    print(f\"   CPU: {final_system_info['cpu_percent']:.1f}% (was {initial_system_info['cpu_percent']:.1f}%)\")\n",
    "    print(f\"   RAM: {final_system_info['memory_used_gb']:.1f}/{final_system_info['memory_total_gb']:.1f} GB ({final_system_info['memory_percent']:.1f}%)\")\n",
    "    print(f\"   RAM change: {final_system_info['memory_used_gb'] - initial_system_info['memory_used_gb']:+.1f} GB\")\n",
    "    print(f\"   Disk: {final_system_info['disk_free_gb']:.1f} GB free\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No features extracted in this batch\")\n",
    "\n",
    "print(f\"\\nüîß TO EXPERIMENT WITH DIFFERENT THREAD COUNTS (CODESPACES):\")\n",
    "print(f\"   1. Change MAX_CONCURRENT_PATIENTS in Cell 1 (try {current_threads + 2})\")\n",
    "print(f\"   2. Re-run this Cell 8 to test the new thread count\")\n",
    "print(f\"   3. Compare the 'Threading efficiency' percentage\")\n",
    "print(f\"   4. Use 'htop' in terminal to monitor system resources\")\n",
    "print(f\"   5. Find the sweet spot for your Codespaces instance!\")\n",
    "\n",
    "# Store processing statistics for analysis\n",
    "globals()['batch_processing_stats'] = processing_stats\n",
    "globals()['threading_experiment_results'] = {\n",
    "    'platform': 'codespaces',\n",
    "    'thread_count': current_threads,\n",
    "    'batch_time_minutes': batch_elapsed/60,\n",
    "    'successful_patients': successful_patients,\n",
    "    'failed_patients': failed_patients,\n",
    "    'frames_extracted': len(batch_features)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Batch Results to Local Storage (Codespaces)\n",
    "print(f\"üíæ SAVING BATCH {START_BATCH} TO LOCAL STORAGE (CODESPACES)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if batch_features:\n",
    "    # Convert to DataFrame\n",
    "    print(f\"üìä Converting {len(batch_features)} records to DataFrame...\")\n",
    "    df = pd.DataFrame(batch_features)\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"‚úÖ DataFrame created: {df.shape}\")\n",
    "    print(f\"üë• Unique patients: {df['patient_id'].nunique()}\")\n",
    "    \n",
    "    # Feature columns analysis\n",
    "    feature_cols = [col for col in df.columns if col.startswith('clean_')]\n",
    "    print(f\"üéØ Feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    # Save to local storage\n",
    "    batch_file_path = os.path.join(OUTPUT_BASE_PATH, CURRENT_BATCH_FILE)\n",
    "    print(f\"üìÅ Saving to: {batch_file_path}\")\n",
    "    \n",
    "    df.to_csv(batch_file_path, index=False)\n",
    "    \n",
    "    # Verify file was saved\n",
    "    if os.path.exists(batch_file_path):\n",
    "        file_size_mb = os.path.getsize(batch_file_path) / (1024 * 1024)\n",
    "        print(f\"‚úÖ Successfully saved: {CURRENT_BATCH_FILE} ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Display sample of the data\n",
    "        print(f\"\\nüìã SAMPLE DATA:\")\n",
    "        print(df[['patient_id', 'timestamp', 'apnea_label', 'clean_rms', 'clean_zcr']].head())\n",
    "        \n",
    "        # Feature correlation analysis\n",
    "        print(f\"\\nüîó TOP 5 FEATURE CORRELATIONS WITH APNEA:\")\n",
    "        correlations = df[feature_cols].corrwith(df['apnea_label']).abs().sort_values(ascending=False)\n",
    "        for feature, corr in correlations.head().items():\n",
    "            print(f\"   {feature}: {corr:.3f}\")\n",
    "        \n",
    "        # Per-patient breakdown\n",
    "        print(f\"\\nüë§ PER-PATIENT BREAKDOWN:\")\n",
    "        patient_stats = df.groupby('patient_id').agg({\n",
    "            'apnea_label': ['count', 'sum', 'mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        for patient in df['patient_id'].unique():\n",
    "            count = patient_stats.loc[patient, ('apnea_label', 'count')]\n",
    "            apnea = patient_stats.loc[patient, ('apnea_label', 'sum')]\n",
    "            rate = patient_stats.loc[patient, ('apnea_label', 'mean')] * 100\n",
    "            print(f\"   {patient}: {count} frames, {apnea} apnea ({rate:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error: File was not saved to {batch_file_path}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No data to save - batch processing failed\")\n",
    "\n",
    "# Show all existing batch files\n",
    "print(f\"\\nüìÇ ALL BATCH FILES IN LOCAL STORAGE:\")\n",
    "existing_batches = [f for f in os.listdir(OUTPUT_BASE_PATH) if f.startswith('codespaces_dataset_batch') and f.endswith('.csv')]\n",
    "\n",
    "if existing_batches:\n",
    "    total_size_mb = 0\n",
    "    for batch_file in sorted(existing_batches):\n",
    "        file_path = os.path.join(OUTPUT_BASE_PATH, batch_file)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size_mb += file_size_mb\n",
    "        print(f\"   ‚úÖ {batch_file} ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìä Total saved data: {total_size_mb:.1f} MB across {len(existing_batches)} batches\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed_batches = len(existing_batches)\n",
    "    remaining_batches = END_BATCH - completed_batches\n",
    "    if remaining_batches > 0:\n",
    "        print(f\"üöÄ Progress: {completed_batches}/{END_BATCH} batches complete\")\n",
    "        print(f\"üìã Next: Set START_BATCH = {completed_batches + 1} to continue\")\n",
    "    else:\n",
    "        print(f\"üéâ All {END_BATCH} batches completed! Dataset ready for analysis.\")\n",
    "        \n",
    "        # Create combined dataset\n",
    "        print(f\"\\nüîÑ Creating combined dataset...\")\n",
    "        all_batch_dfs = []\n",
    "        for batch_file in sorted(existing_batches):\n",
    "            file_path = os.path.join(OUTPUT_BASE_PATH, batch_file)\n",
    "            batch_df = pd.read_csv(file_path)\n",
    "            all_batch_dfs.append(batch_df)\n",
    "        \n",
    "        combined_df = pd.concat(all_batch_dfs, ignore_index=True)\n",
    "        combined_file_path = os.path.join(OUTPUT_BASE_PATH, 'codespaces_complete_dataset.csv')\n",
    "        combined_df.to_csv(combined_file_path, index=False)\n",
    "        \n",
    "        combined_size_mb = os.path.getsize(combined_file_path) / (1024 * 1024)\n",
    "        print(f\"‚úÖ Combined dataset saved: codespaces_complete_dataset.csv ({combined_size_mb:.1f} MB)\")\n",
    "        print(f\"üìä Total records: {len(combined_df):,}\")\n",
    "        print(f\"üë• Total patients: {combined_df['patient_id'].nunique()}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"   No batch files found\")\n",
    "\n",
    "# Show disk usage\n",
    "disk_usage = shutil.disk_usage('.')\n",
    "free_space_gb = disk_usage.free / (1024**3)\n",
    "used_space_gb = disk_usage.used / (1024**3)\n",
    "total_space_gb = disk_usage.total / (1024**3)\n",
    "\n",
    "print(f\"\\nüíæ CODESPACES DISK USAGE:\")\n",
    "print(f\"   Used: {used_space_gb:.1f} GB\")\n",
    "print(f\"   Available: {free_space_gb:.1f} GB\")\n",
    "print(f\"   Total: {total_space_gb:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüéØ BATCH {START_BATCH} PROCESSING COMPLETE (CODESPACES)!\")\n",
    "print(f\"Data safely saved to local storage: {OUTPUT_BASE_PATH}\")\n",
    "print(f\"üìÅ Files are accessible from VS Code file explorer or terminal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}