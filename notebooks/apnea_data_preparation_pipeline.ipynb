{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74322164",
   "metadata": {},
   "source": [
    "# Apnea Audio Data Preparation Pipeline\n",
    "\n",
    "This notebook provides a complete and robust pipeline for preparing audio data for apnea event detection. It performs the following steps for each patient specified in the configuration:\n",
    "\n",
    "1.  **Downloads** EDF (audio) and RML (event annotation) files from provided URLs.\n",
    "2.  **Validates** that all of a patient's audio files have the same sample rate to prevent data corruption.\n",
    "3.  **Extracts** apnea event timestamps (`Obstructive`, `Central`, `Mixed`) from the RML file.\n",
    "4.  **Concatenates** the 'Mic' channel from all of a patient's EDF files into a single, continuous WAV file.\n",
    "5.  **Analyzes** the WAV file second-by-second, extracting a rich set of 23 audio features for each second.\n",
    "6.  **Labels** each second as `1` (apnea) or `0` (normal) based on the extracted event timestamps.\n",
    "7.  **Appends** the features and labels for each patient into a single master CSV file, ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. IMPORTS ===\n",
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# --- Data and Signal Processing ---\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import mne\n",
    "\n",
    "# --- File Downloading ---\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0de2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. CONFIGURATION ===\n",
    "# --- Edit this section for your patients ---\n",
    "\n",
    "# List of patient data. Each dictionary represents one patient.\n",
    "# Add a new dictionary to this list for each new patient you want to process.\n",
    "patient_groups = [\n",
    "    {\n",
    "        \"rml_url\": \"RML_LINK_1\",\n",
    "        \"edf_urls\": [\"EDF_LINK_1A\", \"EDF_LINK_1B\"]\n",
    "    },\n",
    "    {\n",
    "        \"rml_url\": \"RML_LINK_2\",\n",
    "        \"edf_urls\": [\"EDF_LINK_2A\", \"EDF_LINK_2B\", \"EDF_LINK_2C\"]\n",
    "    },\n",
    "    # {\n",
    "    #     \"rml_url\": \"RML_LINK_3\",\n",
    "    #     \"edf_urls\": [\"EDF_LINK_3A\"]\n",
    "    # },\n",
    "]\n",
    "\n",
    "# --- Set the base directory for all downloaded and generated files ---\n",
    "base_dir = \"/content/apnea_data\"  # Using Colab's content directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878822b",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These cells define all the functions used in the pipeline. There is no need to edit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. FUNCTION DEFINITIONS ===\n",
    "\n",
    "def download_file(url, out_path):\n",
    "    \"\"\"Downloads a file from a URL to a specified path using requests.\"\"\"\n",
    "    if not os.path.exists(out_path):\n",
    "        print(f\"Downloading {url}...\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            with open(out_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Successfully downloaded to {out_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Failed to download {url}. Error: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"File already exists: {out_path}\")\n",
    "    return True\n",
    "\n",
    "def extract_apnea_events(xml_file_path):\n",
    "    \"\"\"Parses an RML file and extracts start and end times of apnea events.\"\"\"\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'ns': 'http://www.respironics.com/PatientStudy.xsd'}\n",
    "    apnea_events = []\n",
    "    apnea_types = ['ObstructiveApnea', 'CentralApnea', 'MixedApnea']\n",
    "    for event in root.findall('.//ns:Event', namespace):\n",
    "        if event.get('Family') == 'Respiratory' and event.get('Type') in apnea_types:\n",
    "            start_time = float(event.get('Start'))\n",
    "            duration = float(event.get('Duration'))\n",
    "            end_time = start_time + duration\n",
    "            apnea_events.append({'start': start_time, 'end': end_time})\n",
    "    apnea_events.sort(key=lambda x: x['start'])\n",
    "    print(f\"Extracted {len(apnea_events)} apnea events from {os.path.basename(xml_file_path)}\")\n",
    "    return apnea_events\n",
    "\n",
    "def is_apnea(frame_start, frame_end, events):\n",
    "    \"\"\"Checks if a given time frame overlaps with any apnea event.\"\"\"\n",
    "    for event in events:\n",
    "        if frame_end > event['start'] and frame_start < event['end']:\n",
    "            return 1  # Apnea\n",
    "    return 0  # Normal\n",
    "\n",
    "def extract_features(frame, sr):\n",
    "    \"\"\"Calculates a rich set of audio features from a single audio frame.\"\"\"\n",
    "    if np.all(frame == 0):\n",
    "        return [0.0] * 23 # Return zeros for a silent frame\n",
    "\n",
    "    # Basic features\n",
    "    energy = np.mean(np.abs(frame))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=frame)[0])\n",
    "    rms = np.mean(librosa.feature.rms(y=frame)[0])\n",
    "\n",
    "    # Spectral features\n",
    "    spectrum = np.abs(np.fft.rfft(frame))\n",
    "    freqs = np.fft.rfftfreq(len(frame), 1/sr)\n",
    "    centroid = librosa.feature.spectral_centroid(y=frame, sr=sr)[0, 0]\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=frame, sr=sr)[0, 0]\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=frame, sr=sr)[0, 0]\n",
    "    flatness = librosa.feature.spectral_flatness(y=frame)[0, 0]\n",
    "\n",
    "    # Advanced features\n",
    "    mfccs = librosa.feature.mfcc(y=frame, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = mfccs.mean(axis=1)\n",
    "    skew = scipy.stats.skew(frame)\n",
    "    kurt = scipy.stats.kurtosis(frame)\n",
    "\n",
    "    # Spectrogram entropy\n",
    "    power_spec = spectrum ** 2\n",
    "    ps_norm = power_spec / (np.sum(power_spec) + 1e-10)\n",
    "    entropy = -np.sum(ps_norm * np.log2(ps_norm + 1e-10))\n",
    "\n",
    "    # Aggregate all features into a single list\n",
    "    features = [energy, zcr, centroid, rms, bandwidth, rolloff, flatness, skew, kurt, entropy] + list(mfccs_mean)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. MASTER PROCESSING FUNCTION ===\n",
    "\n",
    "def process_patient(patient_id, patient_group, base_dir, master_csv_path):\n",
    "    \"\"\"Runs the full data preparation pipeline for a single patient.\"\"\"\n",
    "    print(f\"\\n{'='*20} Processing {patient_id} {'='*20}\")\n",
    "    \n",
    "    # Create a dedicated directory for the patient's files\n",
    "    patient_dir = os.path.join(base_dir, patient_id)\n",
    "    os.makedirs(patient_dir, exist_ok=True)\n",
    "\n",
    "    # --- Step 1: Download all files for the patient ---\n",
    "    print(\"\\n--- Step 1: Downloading Files ---\")\n",
    "    rml_path = os.path.join(patient_dir, f\"{patient_id}.rml\")\n",
    "    if not download_file(patient_group[\"rml_url\"], rml_path):\n",
    "        print(f\"Skipping {patient_id} due to RML download failure.\")\n",
    "        return\n",
    "        \n",
    "    edf_paths = []\n",
    "    for i, edf_url in enumerate(patient_group[\"edf_urls\"], 1):\n",
    "        edf_path = os.path.join(patient_dir, f\"{patient_id}_part_{i}.edf\")\n",
    "        if download_file(edf_url, edf_path):\n",
    "            edf_paths.append(edf_path)\n",
    "\n",
    "    # --- Step 2: Concatenate EDF 'Mic' channels into a single WAV file ---\n",
    "    print(\"\\n--- Step 2: Concatenating Audio Files ---\")\n",
    "    output_wav = os.path.join(patient_dir, f\"{patient_id}_full_mic.wav\")\n",
    "    target_sfreq = None\n",
    "    \n",
    "    # CRITICAL: First, validate that all EDFs have a 'Mic' channel and the same sample rate\n",
    "    valid_edf_paths = []\n",
    "    for edf_path in edf_paths:\n",
    "        try:\n",
    "            raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)\n",
    "            if \"Mic\" in raw.ch_names:\n",
    "                current_sfreq = int(raw.info[\"sfreq\"])\n",
    "                if target_sfreq is None:\n",
    "                    target_sfreq = current_sfreq\n",
    "                elif target_sfreq != current_sfreq:\n",
    "                    print(f\"❌ CRITICAL ERROR: Sample rate mismatch in {os.path.basename(edf_path)}. Expected {target_sfreq}, found {current_sfreq}.\")\n",
    "                    print(f\"Skipping {patient_id}.\")\n",
    "                    return\n",
    "                valid_edf_paths.append(edf_path)\n",
    "            else:\n",
    "                print(f\"⚠️ WARNING: 'Mic' channel not found in {os.path.basename(edf_path)}. It will be skipped.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR: Could not read {os.path.basename(edf_path)}. Error: {e}\")\n",
    "\n",
    "    if target_sfreq is None:\n",
    "        print(f\"❌ No valid EDF file with a 'Mic' channel found for {patient_id}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # If validation passes, concatenate the valid files\n",
    "    with sf.SoundFile(output_wav, 'w', samplerate=target_sfreq, channels=1, subtype='PCM_16') as out_f:\n",
    "        for edf_path in valid_edf_paths:\n",
    "            raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
    "            # Ensure data is float32 for consistency with librosa\n",
    "            mic_data = raw.get_data(picks=[\"Mic\"])[0].astype(np.float32)\n",
    "            out_f.write(mic_data)\n",
    "            print(f\"Appended {len(mic_data)} samples from {os.path.basename(edf_path)}\")\n",
    "    print(f\"✅ Successfully created concatenated WAV: {os.path.basename(output_wav)}\")\n",
    "    \n",
    "    # --- Step 3: Extract Features and Labels, Append to Master CSV ---\n",
    "    print(\"\\n--- Step 3: Extracting Features and Labels ---\")\n",
    "    apnea_events = extract_apnea_events(rml_path)\n",
    "    \n",
    "    header = ['patient_id', 'frame_start', 'frame_end', 'energy', 'zcr', 'centroid', 'rms', 'bandwidth', 'rolloff', 'flatness', 'skew', 'kurt', 'entropy'] + [f'mfcc_{i+1}' for i in range(13)] + ['label']\n",
    "    write_header = not os.path.exists(master_csv_path) or os.stat(master_csv_path).st_size == 0\n",
    "\n",
    "    try:\n",
    "        with sf.SoundFile(output_wav, 'r') as f_in:\n",
    "            sr = f_in.samplerate\n",
    "            frame_sec = 1\n",
    "            frame_len = int(sr * frame_sec)\n",
    "            n_frames = int(np.floor(len(f_in) / frame_len))\n",
    "\n",
    "            with open(master_csv_path, 'a', newline='') as f_out:\n",
    "                writer = csv.writer(f_out)\n",
    "                if write_header:\n",
    "                    writer.writerow(header)\n",
    "                \n",
    "                for i in range(n_frames):\n",
    "                    frame = f_in.read(frames=frame_len, dtype='float32')\n",
    "                    if len(frame) < frame_len:\n",
    "                        break\n",
    "                    \n",
    "                    frame_start_sec = i * frame_sec\n",
    "                    frame_end_sec = frame_start_sec + frame_sec\n",
    "                    \n",
    "                    # THE CORE LOGIC: Calculate features on raw audio, then determine label\n",
    "                    # This prevents data leakage.\n",
    "                    features = extract_features(frame, sr)\n",
    "                    label = is_apnea(frame_start_sec, frame_end_sec, apnea_events)\n",
    "                    \n",
    "                    writer.writerow([patient_id, frame_start_sec, frame_end_sec] + features + [label])\n",
    "        print(f\"✅ Appended {n_frames} frames from {patient_id} to master CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR during feature extraction for {patient_id}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. MAIN EXECUTION LOOP ===\n",
    "# This cell runs the entire pipeline for all patients defined in the configuration.\n",
    "\n",
    "master_csv_file = os.path.join(base_dir, \"master_apnea_dataset.csv\")\n",
    "\n",
    "# Ensure the base directory exists\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for i, group in enumerate(patient_groups, 1):\n",
    "    p_id = f\"patient_{i}\"\n",
    "    process_patient(p_id, group, base_dir, master_csv_file)\n",
    "\n",
    "print(f\"\\n{'='*20} PIPELINE COMPLETE {'='*20}\")\n",
    "print(f\"Master dataset saved to: {master_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
