{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Sleep Apnea Data Preparation - Google Colab (Batch Processing)\n",
    "\n",
    "**Rolling Download-Process-Delete Workflow for ~100 Patients**\n",
    "\n",
    "This notebook processes PhysioNet sleep apnea data in batches to manage Colab's 100GB storage limit:\n",
    "1. **Download 25 patients** ‚Üí Process with 16kHz feature extraction ‚Üí Save to Google Drive ‚Üí Delete raw data\n",
    "2. **Repeat for 4 batches** to process ~100 patients total\n",
    "3. **Resumable**: Can start from any batch if interrupted\n",
    "\n",
    "---\n",
    "## üìã **CONFIGURATION** - Modify these parameters to resume processing\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 25          # Patients per batch\n",
    "START_BATCH = 1          # üîß CHANGE THIS to resume (1, 2, 3, or 4)\n",
    "END_BATCH = 4            # Target final batch\n",
    "TOTAL_PATIENTS = 100     # Total patients to process\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Fresh start: `START_BATCH = 1` (processes patients 1-25)\n",
    "- Resume after batch 1: `START_BATCH = 2` (processes patients 26-50)\n",
    "- Final batch only: `START_BATCH = 4` (processes patients 76-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": "# Cell 1: Configuration and Setup\nprint(\"=== SLEEP APNEA DATA PREPARATION - COLAB BATCH PROCESSING ===\")\nprint(\"Rolling Download-Process-Delete workflow for ~100 patients\\n\")\n\n# ============================================================================\n# üîß BATCH CONFIGURATION - MODIFY THESE TO RESUME PROCESSING\n# ============================================================================\nBATCH_SIZE = 25          # Patients per batch (optimal for 100GB Colab limit) \nSTART_BATCH = 1          # üîß CHANGE THIS: 1, 2, 3, or 4 to resume processing\nEND_BATCH = 4            # Final batch number (4 batches = 100 patients)\nTOTAL_PATIENTS = 100     # Total patients to process across all batches\n\n# ============================================================================\n# üßµ DYNAMIC THREADING CONFIGURATION - ADJUST ANYTIME!\n# ============================================================================\n# üîß CHANGE THESE VALUES ANYTIME TO TEST DIFFERENT THREAD COUNTS:\nMAX_CONCURRENT_PATIENTS = 3  # üîß START HERE: Try 3, then 4, 5, 6... until you hit limits\nTIMEOUT_PER_PATIENT = 900     # 15 minutes timeout per patient\nENABLE_THREADING = True       # Set to False to use original sequential processing\n\n# üí° THREADING OPTIMIZATION TIPS:\nprint(f\"üßµ THREADING OPTIMIZATION GUIDE:\")\nprint(f\"   üí° Start with 3 threads, then gradually increase\")\nprint(f\"   üí° Monitor: CPU usage, RAM usage, disk I/O in Colab\")\nprint(f\"   üí° Sweet spot: Usually 3-6 threads for Colab\")\nprint(f\"   üí° Too many threads ‚Üí resource contention, slower performance\")\nprint(f\"   üí° Signs of overload: High RAM usage, slower per-patient times\")\n\n# ============================================================================\n# CALCULATED VALUES - DO NOT MODIFY\n# ============================================================================\npatients_start = (START_BATCH - 1) * BATCH_SIZE + 1\npatients_end = START_BATCH * BATCH_SIZE\nif patients_end > TOTAL_PATIENTS:\n    patients_end = TOTAL_PATIENTS\n\nprint(f\"\\nüìä BATCH CONFIGURATION:\")\nprint(f\"   Current batch: {START_BATCH}/{END_BATCH}\")\nprint(f\"   Patients in this batch: {patients_start}-{patients_end} ({patients_end - patients_start + 1} patients)\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Target total: {TOTAL_PATIENTS} patients\")\n\nprint(f\"\\nüßµ CURRENT THREADING SETTINGS:\")\nprint(f\"   Threading enabled: {ENABLE_THREADING}\")\nprint(f\"   üîß Concurrent patients: {MAX_CONCURRENT_PATIENTS} (CHANGE THIS TO EXPERIMENT!)\")\nprint(f\"   Timeout per patient: {TIMEOUT_PER_PATIENT/60:.1f} minutes\")\nprint(f\"   Expected theoretical speedup: ~{MAX_CONCURRENT_PATIENTS}x faster\")\nprint(f\"   üí° To test different thread counts: Change MAX_CONCURRENT_PATIENTS and re-run Cell 8\")\n\n# ============================================================================\n# GOOGLE DRIVE CONFIGURATION\n# ============================================================================\nDRIVE_BASE_PATH = '/content/drive/MyDrive/sleep_apnea_data'\nCURRENT_BATCH_FILE = f'colab_dataset_batch{START_BATCH}.csv'\nLINKS_FILE = 'download_links.txt'  # Should be uploaded to Colab or Drive\n\nprint(f\"\\nüíæ GOOGLE DRIVE INTEGRATION:\")\nprint(f\"   Save location: {DRIVE_BASE_PATH}\")\nprint(f\"   Current batch file: {CURRENT_BATCH_FILE}\")\n\n# ============================================================================\n# AUDIO PROCESSING SETTINGS\n# ============================================================================\nTARGET_SAMPLE_RATE = 16000  # 16kHz for optimized processing\nFRAME_DURATION = 30.0       # seconds per frame\nOVERLAP_RATIO = 0.5         # 50% overlap between frames\nAPNEA_THRESHOLD = 0.1       # 10% apnea overlap threshold for labeling\nAUDIO_CHANNEL = 'Mic'       # Audio channel to extract\n\nprint(f\"\\nüéµ AUDIO PROCESSING:\")\nprint(f\"   Sample rate: {TARGET_SAMPLE_RATE} Hz\")\nprint(f\"   Frame duration: {FRAME_DURATION} seconds\")\nprint(f\"   Frame overlap: {OVERLAP_RATIO * 100}%\")\nprint(f\"   Apnea threshold: {APNEA_THRESHOLD * 100}%\")\n\nprint(f\"\\n‚úÖ Configuration complete. Ready to process batch {START_BATCH}.\")\nprint(f\"üîß TO EXPERIMENT: Change MAX_CONCURRENT_PATIENTS (3‚Üí4‚Üí5‚Üí6...) and re-run Cell 8!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Mount Google Drive and Setup Directories\n",
    "print(\"üìÅ MOUNTING GOOGLE DRIVE...\")\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
    "os.makedirs('/content/temp_patient_data', exist_ok=True)\n",
    "os.makedirs('/content/downloads', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted successfully\")\n",
    "print(f\"üìÇ Created directories:\")\n",
    "print(f\"   - {DRIVE_BASE_PATH} (Google Drive save location)\")\n",
    "print(f\"   - /content/temp_patient_data (temporary processing)\")\n",
    "print(f\"   - /content/downloads (download staging)\")\n",
    "\n",
    "# Check if batch file already exists\n",
    "batch_file_path = os.path.join(DRIVE_BASE_PATH, CURRENT_BATCH_FILE)\n",
    "if os.path.exists(batch_file_path):\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {CURRENT_BATCH_FILE} already exists in Google Drive!\")\n",
    "    print(f\"   File path: {batch_file_path}\")\n",
    "    print(f\"   Consider changing START_BATCH if this batch is already complete.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ {CURRENT_BATCH_FILE} does not exist. Ready for fresh processing.\")\n",
    "\n",
    "# List existing batch files\n",
    "existing_batches = [f for f in os.listdir(DRIVE_BASE_PATH) if f.startswith('colab_dataset_batch') and f.endswith('.csv')]\n",
    "if existing_batches:\n",
    "    print(f\"\\nüìä EXISTING BATCH FILES IN DRIVE:\")\n",
    "    for batch_file in sorted(existing_batches):\n",
    "        file_path = os.path.join(DRIVE_BASE_PATH, batch_file)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   - {batch_file} ({file_size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"\\nüìä No existing batch files found. This appears to be a fresh start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": "# Cell 3: Install Dependencies and Import Libraries\nprint(\"üì¶ INSTALLING DEPENDENCIES...\")\n\n# Install required packages\n!pip install librosa mne tqdm\n\nprint(\"\\nüìö IMPORTING LIBRARIES...\")\n\n# Core libraries\nimport os\nimport re\nimport time\nimport shutil\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Audio processing\nimport librosa\nimport mne\n\n# Progress tracking\nfrom tqdm.notebook import tqdm\n\n# XML processing (will need to recreate extract_apnea_events function)\nimport xml.etree.ElementTree as ET\n\n# Threading for parallel processing\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport threading\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ All libraries imported successfully\")\nprint(f\"üîä librosa version: {librosa.__version__}\")\nprint(f\"üß† mne version: {mne.__version__}\")\nprint(f\"üî¢ numpy version: {np.__version__}\")\nprint(f\"üêº pandas version: {pd.__version__}\")\nprint(f\"üßµ Threading support: {threading.active_count()} active threads\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xml_parser"
   },
   "outputs": [],
   "source": "# Cell 4: Import Original XML Parser (Upload working_with_xml.py first)\nprint(\"üîç IMPORTING ORIGINAL XML PARSER...\")\n\n# First, upload the working_with_xml.py file to Google Drive\nprint(\"üìé UPLOAD WORKING_WITH_XML.PY FILE\")\nprint(\"Please upload your 'working_with_xml.py' file to Google Drive at:\")\nprint(f\"   {DRIVE_BASE_PATH}/working_with_xml.py\")\nprint(\"This ensures we use the proven XML parser instead of a recreated version.\")\n\nfrom google.colab import files\nimport sys\nimport os\n\n# Check if working_with_xml.py already exists in Drive\nxml_parser_path = os.path.join(DRIVE_BASE_PATH, 'working_with_xml.py')\n\nif os.path.exists(xml_parser_path):\n    print(f\"‚úÖ Found existing working_with_xml.py in Google Drive\")\n    print(f\"   Path: {xml_parser_path}\")\nelse:\n    print(f\"üì§ Upload working_with_xml.py file:\")\n    uploaded = files.upload()\n    \n    if 'working_with_xml.py' in uploaded:\n        # Copy to Google Drive for persistence\n        import shutil\n        shutil.copy('working_with_xml.py', xml_parser_path)\n        print(f\"‚úÖ working_with_xml.py uploaded and saved to Google Drive\")\n        print(f\"   Size: {len(uploaded['working_with_xml.py'])} bytes\")\n    else:\n        print(f\"‚ùå working_with_xml.py not found in uploaded files\")\n        print(\"Available files:\", list(uploaded.keys()) if 'uploaded' in locals() else \"None\")\n        raise Exception(\"working_with_xml.py is required to proceed\")\n\n# Add Google Drive path to Python path so we can import\ndrive_src_path = DRIVE_BASE_PATH\nif drive_src_path not in sys.path:\n    sys.path.insert(0, drive_src_path)\n\n# Import the original, proven XML parser\ntry:\n    from working_with_xml import extract_apnea_events\n    print(\"‚úÖ Original XML parser imported successfully from Google Drive\")\n    print(\"   üéØ This eliminates the apnea labeling issues from recreated parser\")\n    print(\"   üîß Uses proven XML parsing with proper namespace handling\")\n    print(\"   üìã Extracts: ObstructiveApnea, CentralApnea, MixedApnea, Hypopnea\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import working_with_xml: {e}\")\n    print(\"Please ensure working_with_xml.py is properly uploaded to Google Drive\")\n    raise\n\n# Test the function to ensure it works\nprint(f\"\\nüß™ TESTING XML PARSER:\")\nprint(\"Function signature:\", extract_apnea_events.__doc__)\nprint(\"‚úÖ XML parser ready for use in feature extraction pipeline\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_functions"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Download Functions (Enhanced from file download.ipynb)\n",
    "print(\"üì• DEFINING DOWNLOAD FUNCTIONS...\")\n",
    "\n",
    "def group_links_by_patient(links_content):\n",
    "    \"\"\"\n",
    "    Groups download URLs by patient ID.\n",
    "    Modified to work with links content directly (not file path).\n",
    "    \"\"\"\n",
    "    grouped_data = {}\n",
    "    patient_id_regex = re.compile(r'(\\d{8}-\\d{6})')\n",
    "    \n",
    "    for url in links_content.strip().split('\\n'):\n",
    "        url = url.strip()\n",
    "        if not url:\n",
    "            continue\n",
    "            \n",
    "        match = patient_id_regex.search(url)\n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        patient_id = match.group(1)\n",
    "        if patient_id not in grouped_data:\n",
    "            grouped_data[patient_id] = {'rml': None, 'edf': []}\n",
    "            \n",
    "        if url.endswith('.rml'):\n",
    "            grouped_data[patient_id]['rml'] = url\n",
    "        elif url.endswith('.edf'):\n",
    "            grouped_data[patient_id]['edf'].append(url)\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "def download_file_with_retry(url, local_path, max_retries=3, base_delay=2):\n",
    "    \"\"\"\n",
    "    Downloads a file with retry logic and resume capability.\n",
    "    Enhanced from original with better error handling.\n",
    "    \"\"\"\n",
    "    # Check if file already exists and is complete\n",
    "    if os.path.exists(local_path):\n",
    "        try:\n",
    "            local_size = os.path.getsize(local_path)\n",
    "            if local_size > 1000:  # Assume files > 1KB are likely complete\n",
    "                print(f\"      ‚úì File exists: {os.path.basename(local_path)} ({local_size/1024:.1f} KB)\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Download with retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"      üì• Downloading: {os.path.basename(local_path)} (attempt {attempt + 1}/{max_retries})\")\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            \n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                \n",
    "                # Download to temporary file first\n",
    "                temp_path = local_path + '.tmp'\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Move to final location\n",
    "                shutil.move(temp_path, local_path)\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                print(f\"      ‚úÖ Downloaded: {os.path.basename(local_path)} ({file_size/1024:.1f} KB)\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Attempt {attempt + 1} failed: {str(e)[:100]}...\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"      ‚è≥ Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    # Clean up temp file if exists\n",
    "    temp_path = local_path + '.tmp'\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def download_patient_data(patient_original_id, patient_files, patient_folder):\n",
    "    \"\"\"\n",
    "    Downloads all files for a single patient.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"   üìÇ Downloading to: {patient_folder}\")\n",
    "    \n",
    "    success = True\n",
    "    \n",
    "    # Download RML file\n",
    "    if patient_files['rml']:\n",
    "        rml_url = patient_files['rml']\n",
    "        rml_filename_match = re.search(r'fileName=([^&]+)', rml_url)\n",
    "        rml_filename = rml_filename_match.group(1) if rml_filename_match else os.path.basename(rml_url).split('?')[0]\n",
    "        rml_filename = requests.utils.unquote(rml_filename)\n",
    "        rml_path = os.path.join(patient_folder, rml_filename)\n",
    "        \n",
    "        if not download_file_with_retry(rml_url, rml_path):\n",
    "            success = False\n",
    "            print(f\"      ‚ùå Failed to download RML file\")\n",
    "    \n",
    "    # Download EDF files\n",
    "    edf_success_count = 0\n",
    "    for edf_url in patient_files['edf']:\n",
    "        edf_filename_match = re.search(r'fileName=([^&]+)', edf_url)\n",
    "        edf_filename = edf_filename_match.group(1) if edf_filename_match else os.path.basename(edf_url).split('?')[0]\n",
    "        edf_filename = requests.utils.unquote(edf_filename)\n",
    "        edf_path = os.path.join(patient_folder, edf_filename)\n",
    "        \n",
    "        # Also download corresponding .hea file\n",
    "        hea_url = edf_url.replace('.edf', '.hea')\n",
    "        hea_filename = edf_filename.replace('.edf', '.hea')\n",
    "        hea_path = os.path.join(patient_folder, hea_filename)\n",
    "        \n",
    "        edf_ok = download_file_with_retry(edf_url, edf_path)\n",
    "        hea_ok = download_file_with_retry(hea_url, hea_path)\n",
    "        \n",
    "        if edf_ok and hea_ok:\n",
    "            edf_success_count += 1\n",
    "        else:\n",
    "            print(f\"      ‚ùå Failed EDF/HEA pair: {edf_filename}\")\n",
    "    \n",
    "    total_edf_count = len(patient_files['edf'])\n",
    "    print(f\"   üìä Downloaded {edf_success_count}/{total_edf_count} EDF files\")\n",
    "    \n",
    "    return success and edf_success_count > 0\n",
    "\n",
    "print(\"‚úÖ Download functions defined\")\n",
    "print(\"   - group_links_by_patient(): Groups URLs by patient ID\")\n",
    "print(\"   - download_file_with_retry(): Downloads with retry logic\")\n",
    "print(\"   - download_patient_data(): Downloads all files for one patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Feature Extraction Functions (Enhanced from parallel_feature_extraction.ipynb)\n",
    "print(\"üéµ DEFINING FEATURE EXTRACTION FUNCTIONS...\")\n",
    "\n",
    "def extract_comprehensive_features(audio_frame, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features for sleep apnea detection.\n",
    "    Enhanced from parallel_feature_extraction.ipynb with 16kHz optimization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(audio_frame) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Basic acoustic features\n",
    "        rms = float(librosa.feature.rms(y=audio_frame).mean())\n",
    "        zcr = float(librosa.feature.zero_crossing_rate(y=audio_frame).mean())\n",
    "        centroid = float(librosa.feature.spectral_centroid(y=audio_frame, sr=sample_rate).mean())\n",
    "        bandwidth = float(librosa.feature.spectral_bandwidth(y=audio_frame, sr=sample_rate).mean())\n",
    "        rolloff = float(librosa.feature.spectral_rolloff(y=audio_frame, sr=sample_rate).mean())\n",
    "        \n",
    "        # MFCCs (first 8 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_frame, sr=sample_rate, n_mfcc=8)\n",
    "        mfcc_means = mfccs.mean(axis=1)\n",
    "        mfcc_stds = mfccs.std(axis=1)\n",
    "        \n",
    "        # Temporal features for breathing patterns (5-second windows)\n",
    "        window_size = int(5 * sample_rate)  # 5 seconds\n",
    "        num_windows = len(audio_frame) // window_size\n",
    "        \n",
    "        if num_windows >= 2:\n",
    "            rms_windows = []\n",
    "            zcr_windows = []\n",
    "            \n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window = audio_frame[start_idx:end_idx]\n",
    "                \n",
    "                rms_windows.append(librosa.feature.rms(y=window).mean())\n",
    "                zcr_windows.append(librosa.feature.zero_crossing_rate(y=window).mean())\n",
    "            \n",
    "            rms_variability = float(np.std(rms_windows))\n",
    "            zcr_variability = float(np.std(zcr_windows))\n",
    "            breathing_regularity = float(1.0 / (1.0 + rms_variability))\n",
    "        else:\n",
    "            rms_variability = 0.0\n",
    "            zcr_variability = 0.0\n",
    "            breathing_regularity = 0.5\n",
    "        \n",
    "        # Silence detection\n",
    "        silence_threshold = np.percentile(np.abs(audio_frame), 20)\n",
    "        silence_mask = np.abs(audio_frame) < silence_threshold\n",
    "        silence_ratio = float(np.mean(silence_mask))\n",
    "        \n",
    "        # Breathing pause detection\n",
    "        silence_changes = np.diff(silence_mask.astype(int))\n",
    "        pause_starts = np.where(silence_changes == 1)[0]\n",
    "        pause_ends = np.where(silence_changes == -1)[0]\n",
    "        \n",
    "        if len(pause_starts) > 0 and len(pause_ends) > 0:\n",
    "            if len(pause_ends) < len(pause_starts):\n",
    "                pause_ends = np.append(pause_ends, len(audio_frame))\n",
    "            pause_durations = (pause_ends[:len(pause_starts)] - pause_starts) / sample_rate\n",
    "            avg_pause_duration = float(np.mean(pause_durations))\n",
    "            max_pause_duration = float(np.max(pause_durations))\n",
    "        else:\n",
    "            avg_pause_duration = 0.0\n",
    "            max_pause_duration = 0.0\n",
    "        \n",
    "        # Combine all features\n",
    "        features = {\n",
    "            'clean_rms': rms,\n",
    "            'clean_zcr': zcr,\n",
    "            'clean_centroid': centroid,\n",
    "            'clean_bandwidth': bandwidth,\n",
    "            'clean_rolloff': rolloff,\n",
    "            'clean_rms_variability': rms_variability,\n",
    "            'clean_zcr_variability': zcr_variability,\n",
    "            'clean_breathing_regularity': breathing_regularity,\n",
    "            'clean_silence_ratio': silence_ratio,\n",
    "            'clean_avg_pause_duration': avg_pause_duration,\n",
    "            'clean_max_pause_duration': max_pause_duration\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs\n",
    "        for i, (mean_val, std_val) in enumerate(zip(mfcc_means, mfcc_stds), 1):\n",
    "            features[f'clean_mfcc_{i}_mean'] = float(mean_val)\n",
    "            features[f'clean_mfcc_{i}_std'] = float(std_val)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Feature extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_apnea_label(timestamp, duration, apnea_events, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculate apnea label based on overlap with annotated events.\n",
    "    Uses proportion-based labeling with configurable threshold.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frame_end = timestamp + duration\n",
    "        apnea_seconds = 0\n",
    "        \n",
    "        for _, start, end in apnea_events:\n",
    "            overlap_start = max(timestamp, start)\n",
    "            overlap_end = min(frame_end, end)\n",
    "            if overlap_start < overlap_end:\n",
    "                apnea_seconds += (overlap_end - overlap_start)\n",
    "        \n",
    "        proportion = apnea_seconds / duration\n",
    "        label = 1 if proportion > threshold else 0\n",
    "        return label, proportion\n",
    "    except:\n",
    "        return 0, 0.0\n",
    "\n",
    "def process_patient_edf_files(patient_folder, patient_id):\n",
    "    \"\"\"\n",
    "    Process all EDF files for a single patient and extract features.\n",
    "    Returns list of feature records for the patient.\n",
    "    \"\"\"\n",
    "    print(f\"   üéµ Processing audio files for {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Find EDF and RML files\n",
    "        edf_files = sorted([f for f in os.listdir(patient_folder) if f.endswith('.edf')])\n",
    "        rml_files = [f for f in os.listdir(patient_folder) if f.endswith('.rml')]\n",
    "        \n",
    "        if not edf_files or not rml_files:\n",
    "            print(f\"      ‚ùå Missing files: {len(edf_files)} EDF, {len(rml_files)} RML\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"      üìÅ Found {len(edf_files)} EDF and {len(rml_files)} RML files\")\n",
    "        \n",
    "        # Load apnea events\n",
    "        rml_path = os.path.join(patient_folder, rml_files[0])\n",
    "        apnea_events = extract_apnea_events(rml_path)\n",
    "        print(f\"      üìã Loaded {len(apnea_events)} apnea events\")\n",
    "        \n",
    "        # Process each EDF file\n",
    "        all_features = []\n",
    "        \n",
    "        for edf_idx, edf_file in enumerate(edf_files, 1):\n",
    "            print(f\"      üéµ Processing EDF {edf_idx}/{len(edf_files)}: {edf_file}\")\n",
    "            \n",
    "            try:\n",
    "                edf_path = os.path.join(patient_folder, edf_file)\n",
    "                raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)\n",
    "                \n",
    "                if AUDIO_CHANNEL not in raw.ch_names:\n",
    "                    print(f\"         ‚ö†Ô∏è No {AUDIO_CHANNEL} channel, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                raw.pick_channels([AUDIO_CHANNEL])\n",
    "                original_sample_rate = int(raw.info['sfreq'])\n",
    "                duration_min = raw.n_times / original_sample_rate / 60\n",
    "                \n",
    "                print(f\"         ‚è±Ô∏è Duration: {duration_min:.1f} min, {original_sample_rate} Hz ‚Üí {TARGET_SAMPLE_RATE} Hz\")\n",
    "                \n",
    "                # Frame parameters\n",
    "                original_frame_samples = int(FRAME_DURATION * original_sample_rate)\n",
    "                original_step_samples = int(original_frame_samples * (1 - OVERLAP_RATIO))\n",
    "                \n",
    "                # Time offset for multi-EDF processing\n",
    "                time_offset = (edf_idx - 1) * 60 * 60  # Each EDF ‚âà 1 hour\n",
    "                \n",
    "                frame_count = 0\n",
    "                for frame_start in range(0, raw.n_times - original_frame_samples + 1, original_step_samples):\n",
    "                    frame_end = frame_start + original_frame_samples\n",
    "                    timestamp = (frame_start / original_sample_rate) + time_offset\n",
    "                    \n",
    "                    # Load and downsample audio frame\n",
    "                    try:\n",
    "                        audio_frame, _ = raw[:, frame_start:frame_end]\n",
    "                        audio_frame = audio_frame.flatten()\n",
    "                        \n",
    "                        # Downsample to target rate\n",
    "                        if original_sample_rate != TARGET_SAMPLE_RATE:\n",
    "                            audio_frame = librosa.resample(\n",
    "                                audio_frame, \n",
    "                                orig_sr=original_sample_rate, \n",
    "                                target_sr=TARGET_SAMPLE_RATE\n",
    "                            )\n",
    "                        \n",
    "                        # Extract features\n",
    "                        features = extract_comprehensive_features(audio_frame, TARGET_SAMPLE_RATE)\n",
    "                        if features is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get apnea label\n",
    "                        apnea_label, apnea_proportion = get_apnea_label(\n",
    "                            timestamp, FRAME_DURATION, apnea_events, APNEA_THRESHOLD\n",
    "                        )\n",
    "                        \n",
    "                        # Create record\n",
    "                        record = {\n",
    "                            'patient_id': patient_id,\n",
    "                            'edf_file': edf_file,\n",
    "                            'timestamp': float(timestamp),\n",
    "                            'frame_duration': FRAME_DURATION,\n",
    "                            'sample_rate': TARGET_SAMPLE_RATE,\n",
    "                            'apnea_label': int(apnea_label),\n",
    "                            'apnea_proportion': float(apnea_proportion),\n",
    "                            **features\n",
    "                        }\n",
    "                        \n",
    "                        all_features.append(record)\n",
    "                        frame_count += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"         ‚ö†Ô∏è Frame {frame_count} failed: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"         ‚úÖ Extracted {frame_count} frames\")\n",
    "                del raw  # Free memory\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"         ‚ùå EDF processing failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        apnea_count = sum(1 for f in all_features if f['apnea_label'] == 1)\n",
    "        print(f\"   ‚úÖ {patient_id}: {len(all_features)} frames, {apnea_count} apnea\")\n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {patient_id}: Processing failed: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")\n",
    "print(\"   - extract_comprehensive_features(): 27 audio features with 16kHz optimization\")\n",
    "print(\"   - get_apnea_label(): Proportion-based labeling with 10% threshold\")\n",
    "print(\"   - process_patient_edf_files(): Full patient processing pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_links"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Upload Download Links File\n",
    "print(\"üìé UPLOAD DOWNLOAD LINKS FILE\")\n",
    "print(\"Please upload your 'download_links.txt' file containing PhysioNet URLs.\")\n",
    "print(\"This file should contain one URL per line.\")\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the links file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify the file was uploaded\n",
    "if LINKS_FILE in uploaded:\n",
    "    print(f\"‚úÖ {LINKS_FILE} uploaded successfully ({len(uploaded[LINKS_FILE])} bytes)\")\n",
    "    \n",
    "    # Read and parse the links\n",
    "    with open(LINKS_FILE, 'r') as f:\n",
    "        links_content = f.read()\n",
    "    \n",
    "    # Group links by patient\n",
    "    grouped_links = group_links_by_patient(links_content)\n",
    "    print(f\"üìä Found {len(grouped_links)} unique patients in links file\")\n",
    "    \n",
    "    # Show sample of available patients\n",
    "    valid_patients = [pid for pid, files in grouped_links.items() if files['rml'] and files['edf']]\n",
    "    print(f\"‚úÖ {len(valid_patients)} patients have both RML and EDF files\")\n",
    "    \n",
    "    if len(valid_patients) >= TOTAL_PATIENTS:\n",
    "        print(f\"üéØ Sufficient patients available for target of {TOTAL_PATIENTS}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Only {len(valid_patients)} valid patients found, less than target {TOTAL_PATIENTS}\")\n",
    "        \n",
    "    # Show first few patient IDs as example\n",
    "    print(f\"\\nüìã Sample patient IDs:\")\n",
    "    for i, pid in enumerate(list(valid_patients)[:5]):\n",
    "        files_info = grouped_links[pid]\n",
    "        print(f\"   {i+1}. {pid}: {len(files_info['edf'])} EDF files, {'‚úì' if files_info['rml'] else '‚úó'} RML\")\n",
    "    \n",
    "    if len(valid_patients) > 5:\n",
    "        print(f\"   ... and {len(valid_patients) - 5} more patients\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå {LINKS_FILE} not found in uploaded files\")\n",
    "    print(\"Available files:\", list(uploaded.keys()))\n",
    "    raise Exception(\"Download links file is required to proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_processing"
   },
   "outputs": [],
   "source": "# Cell 8: Main Batch Processing Loop (Dynamic Threading)\nprint(f\"üöÄ STARTING BATCH {START_BATCH} PROCESSING\")\nprint(f\"Processing patients {patients_start}-{patients_end}\")\nprint(f\"Threading: {'ENABLED' if ENABLE_THREADING else 'DISABLED'} ({MAX_CONCURRENT_PATIENTS} concurrent)\" if ENABLE_THREADING else \"Sequential processing\")\nprint(f\"{'='*60}\")\n\n# üîß DYNAMIC THREADING: Read current configuration\ncurrent_threads = MAX_CONCURRENT_PATIENTS\ncurrent_timeout = TIMEOUT_PER_PATIENT\n\nprint(f\"üßµ DYNAMIC THREADING STATUS:\")\nprint(f\"   Current thread count: {current_threads}\")\nprint(f\"   üí° TO EXPERIMENT: Change MAX_CONCURRENT_PATIENTS in Cell 1 and re-run this cell\")\nprint(f\"   üí° TRY: 3 ‚Üí 4 ‚Üí 5 ‚Üí 6 ‚Üí 7 ‚Üí 8... until you see diminishing returns\")\nprint(f\"   üí° MONITOR: Check Colab's Resource tab for RAM/CPU usage\")\n\ndef get_system_info():\n    \"\"\"Get basic system information for monitoring resource usage\"\"\"\n    try:\n        import psutil\n        cpu_percent = psutil.cpu_percent()\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage('/content')\n        \n        return {\n            'cpu_percent': cpu_percent,\n            'memory_used_gb': memory.used / (1024**3),\n            'memory_total_gb': memory.total / (1024**3),\n            'memory_percent': memory.percent,\n            'disk_used_gb': disk.used / (1024**3),\n            'disk_free_gb': disk.free / (1024**3)\n        }\n    except ImportError:\n        return None\n\ndef process_single_patient_threading(patient_info):\n    \"\"\"\n    Process a single patient in a thread-safe manner.\n    Enhanced with resource monitoring and thread identification.\n    Returns: (success, patient_features, stats)\n    \"\"\"\n    patient_original_id, patient_number, patient_idx = patient_info\n    patient_id = f\"patient_{patient_number:02d}\"\n    patient_folder = f\"/content/temp_patient_data/{patient_id}\"\n    thread_id = threading.current_thread().name\n    \n    print(f\"\\n[{thread_id}] --- Processing {patient_id} (Original: {patient_original_id}) ---\")\n    \n    start_time = time.time()\n    stats = {\n        'patient_id': patient_id,\n        'original_id': patient_original_id,\n        'thread_id': thread_id,\n        'thread_count_used': current_threads,\n        'success': False,\n        'frames_extracted': 0,\n        'apnea_frames': 0,\n        'processing_time': 0,\n        'download_time': 0,\n        'feature_extraction_time': 0,\n        'cleanup_time': 0,\n        'error_message': None\n    }\n    \n    try:\n        # Step 1: Download patient data\n        download_start = time.time()\n        print(f\"[{thread_id}] üì• Step 1/3: Downloading {patient_id}...\")\n        patient_files = grouped_links[patient_original_id]\n        \n        download_success = download_patient_data(patient_original_id, patient_files, patient_folder)\n        stats['download_time'] = time.time() - download_start\n        \n        if not download_success:\n            stats['error_message'] = \"Download failed\"\n            print(f\"[{thread_id}] ‚ùå Download failed for {patient_id} (took {stats['download_time']:.1f}s)\")\n            return False, [], stats\n        \n        print(f\"[{thread_id}] ‚úÖ Download successful for {patient_id} (took {stats['download_time']:.1f}s)\")\n        \n        # Step 2: Process and extract features\n        extraction_start = time.time()\n        print(f\"[{thread_id}] üéµ Step 2/3: Extracting features for {patient_id}...\")\n        patient_features = process_patient_edf_files(patient_folder, patient_id)\n        stats['feature_extraction_time'] = time.time() - extraction_start\n        \n        if not patient_features:\n            stats['error_message'] = \"Feature extraction failed\"\n            print(f\"[{thread_id}] ‚ùå Feature extraction failed for {patient_id} (took {stats['feature_extraction_time']:.1f}s)\")\n            return False, [], stats\n        \n        # Update stats\n        stats['frames_extracted'] = len(patient_features)\n        stats['apnea_frames'] = sum(1 for f in patient_features if f['apnea_label'] == 1)\n        stats['success'] = True\n        \n        print(f\"[{thread_id}] ‚úÖ Features extracted for {patient_id}: {len(patient_features)} frames (took {stats['feature_extraction_time']:.1f}s)\")\n        \n        return True, patient_features, stats\n        \n    except Exception as e:\n        stats['error_message'] = str(e)\n        print(f\"[{thread_id}] ‚ùå Critical error processing {patient_id}: {e}\")\n        return False, [], stats\n        \n    finally:\n        # Step 3: Clean up temporary files (always runs)\n        cleanup_start = time.time()\n        stats['processing_time'] = time.time() - start_time\n        \n        print(f\"[{thread_id}] üóëÔ∏è Step 3/3: Cleaning up {patient_id}...\")\n        if os.path.exists(patient_folder):\n            try:\n                shutil.rmtree(patient_folder)\n                stats['cleanup_time'] = time.time() - cleanup_start\n                print(f\"[{thread_id}] ‚úÖ Cleanup successful for {patient_id} (took {stats['cleanup_time']:.1f}s)\")\n            except Exception as cleanup_error:\n                stats['cleanup_time'] = time.time() - cleanup_start\n                print(f\"[{thread_id}] ‚ö†Ô∏è Cleanup warning for {patient_id}: {cleanup_error}\")\n\n# Main processing logic\nbatch_start_time = time.time()\nbatch_features = []  # Thread-safe accumulation\nsuccessful_patients = 0\nfailed_patients = 0\nprocessing_stats = []\n\n# Get initial system info\ninitial_system_info = get_system_info()\nif initial_system_info:\n    print(f\"\\nüìä INITIAL SYSTEM STATUS:\")\n    print(f\"   CPU: {initial_system_info['cpu_percent']:.1f}%\")\n    print(f\"   RAM: {initial_system_info['memory_used_gb']:.1f}/{initial_system_info['memory_total_gb']:.1f} GB ({initial_system_info['memory_percent']:.1f}%)\")\n    print(f\"   Disk: {initial_system_info['disk_free_gb']:.1f} GB free\")\n\n# Select patients for this batch\nvalid_patients = [pid for pid, files in grouped_links.items() if files['rml'] and files['edf']]\nbatch_patients = valid_patients[patients_start-1:patients_end]  # Convert to 0-based indexing\n\nprint(f\"\\nüìã Selected {len(batch_patients)} patients for batch {START_BATCH}\")\n\n# Prepare patient info tuples\npatient_info_list = [\n    (patient_original_id, patients_start + patient_idx, patient_idx)\n    for patient_idx, patient_original_id in enumerate(batch_patients)\n]\n\n# Threading lock for thread-safe operations\nresults_lock = threading.Lock()\n\nif ENABLE_THREADING:\n    print(f\"\\nüßµ STARTING THREADED PROCESSING:\")\n    print(f\"   üîß Concurrent patients: {current_threads} (configurable in Cell 1)\")\n    print(f\"   Timeout per patient: {current_timeout/60:.1f} minutes\")\n    print(f\"   Total patients: {len(patient_info_list)}\")\n    print(f\"   üí° OPTIMIZATION TIP: Monitor resource usage and try different thread counts!\")\n    \n    # Use ThreadPoolExecutor for parallel processing\n    with ThreadPoolExecutor(max_workers=current_threads, thread_name_prefix=\"Patient\") as executor:\n        # Submit all patient processing jobs\n        future_to_patient = {\n            executor.submit(process_single_patient_threading, patient_info): patient_info[0]\n            for patient_info in patient_info_list\n        }\n        \n        print(f\"üì§ Submitted {len(future_to_patient)} patient processing jobs to {current_threads} threads\")\n        \n        # Collect results as they complete\n        for future in as_completed(future_to_patient, timeout=current_timeout * len(patient_info_list)):\n            patient_original_id = future_to_patient[future]\n            \n            try:\n                # Get result with individual patient timeout\n                success, patient_features, stats = future.result(timeout=current_timeout)\n                \n                # Thread-safe result accumulation\n                with results_lock:\n                    processing_stats.append(stats)\n                    \n                    if success:\n                        batch_features.extend(patient_features)\n                        successful_patients += 1\n                        \n                        # Progress update with timing details\n                        total_frames = len(batch_features)\n                        apnea_frames = sum(1 for f in batch_features if f['apnea_label'] == 1)\n                        \n                        print(f\"üéØ [{stats['thread_id']}] SUCCESS: {stats['patient_id']} - {stats['frames_extracted']} frames\")\n                        print(f\"   ‚è±Ô∏è Timing: Download {stats['download_time']:.1f}s | Processing {stats['feature_extraction_time']:.1f}s | Cleanup {stats['cleanup_time']:.1f}s\")\n                        print(f\"üìä Batch progress: {successful_patients + failed_patients}/{len(patient_info_list)} patients, {total_frames} total frames\")\n                    else:\n                        failed_patients += 1\n                        print(f\"‚ùå [{stats['thread_id']}] FAILED: {stats['patient_id']} - {stats['error_message']}\")\n                        \n                    # Show system status every 5 patients\n                    if (successful_patients + failed_patients) % 5 == 0:\n                        current_system_info = get_system_info()\n                        if current_system_info:\n                            print(f\"üìä SYSTEM STATUS: CPU {current_system_info['cpu_percent']:.1f}% | RAM {current_system_info['memory_percent']:.1f}% | Disk {current_system_info['disk_free_gb']:.1f}GB free\")\n                        \n            except Exception as e:\n                with results_lock:\n                    failed_patients += 1\n                print(f\"‚ùå Exception for {patient_original_id}: {e}\")\n\nelse:\n    print(f\"\\nüìú STARTING SEQUENTIAL PROCESSING:\")\n    \n    # Sequential processing (original method)\n    for patient_info in tqdm(patient_info_list, desc=f\"Batch {START_BATCH} Progress\"):\n        success, patient_features, stats = process_single_patient_threading(patient_info)\n        \n        processing_stats.append(stats)\n        \n        if success:\n            batch_features.extend(patient_features)\n            successful_patients += 1\n        else:\n            failed_patients += 1\n        \n        # Show progress\n        total_frames = len(batch_features)\n        apnea_frames = sum(1 for f in batch_features if f['apnea_label'] == 1)\n        print(f\"üìä Progress: {successful_patients + failed_patients}/{len(patient_info_list)} patients, {total_frames} frames\")\n\nbatch_elapsed = time.time() - batch_start_time\n\n# Final batch statistics with threading analysis\nprint(f\"\\n{'='*60}\")\nprint(f\"üèÅ BATCH {START_BATCH} PROCESSING COMPLETE!\")\nprint(f\"‚è±Ô∏è Total time: {batch_elapsed/60:.1f} minutes\")\nprint(f\"üßµ Threading: {'ENABLED' if ENABLE_THREADING else 'DISABLED'} (used {current_threads} threads)\")\nprint(f\"‚úÖ Successful patients: {successful_patients}\")\nprint(f\"‚ùå Failed patients: {failed_patients}\")\nprint(f\"üìä Total frames extracted: {len(batch_features):,}\")\n\nif batch_features:\n    apnea_count = sum(1 for f in batch_features if f['apnea_label'] == 1)\n    apnea_rate = (apnea_count / len(batch_features)) * 100\n    print(f\"üö® Apnea frames: {apnea_count:,} ({apnea_rate:.1f}%)\")\n    print(f\"üò¥ Normal frames: {len(batch_features) - apnea_count:,} ({100-apnea_rate:.1f}%)\")\n    \n    # Enhanced threading performance analysis\n    if ENABLE_THREADING and processing_stats:\n        successful_stats = [s for s in processing_stats if s['success']]\n        if successful_stats:\n            avg_total_time = sum(s['processing_time'] for s in successful_stats) / len(successful_stats)\n            avg_download_time = sum(s['download_time'] for s in successful_stats) / len(successful_stats)\n            avg_processing_time = sum(s['feature_extraction_time'] for s in successful_stats) / len(successful_stats)\n            avg_cleanup_time = sum(s['cleanup_time'] for s in successful_stats) / len(successful_stats)\n            \n            estimated_sequential_time = avg_total_time * len(patient_info_list)\n            actual_speedup = estimated_sequential_time / batch_elapsed\n            theoretical_speedup = current_threads\n            efficiency = (actual_speedup / theoretical_speedup) * 100\n            \n            print(f\"\\nüöÄ THREADING PERFORMANCE ANALYSIS:\")\n            print(f\"   üîß Thread count used: {current_threads}\")\n            print(f\"   ‚è±Ô∏è Average times per patient:\")\n            print(f\"      - Download: {avg_download_time/60:.1f} minutes\")\n            print(f\"      - Processing: {avg_processing_time/60:.1f} minutes\") \n            print(f\"      - Cleanup: {avg_cleanup_time:.1f} seconds\")\n            print(f\"      - Total: {avg_total_time/60:.1f} minutes\")\n            print(f\"   üèÉ Speedup achieved: {actual_speedup:.1f}x (vs sequential)\")\n            print(f\"   üéØ Theoretical max: {theoretical_speedup}x\")\n            print(f\"   üìà Threading efficiency: {efficiency:.1f}%\")\n            \n            # Recommendations for next run\n            print(f\"\\nüí° OPTIMIZATION SUGGESTIONS:\")\n            if efficiency > 80:\n                print(f\"   ‚úÖ Excellent efficiency! Try increasing to {current_threads + 1} threads\")\n            elif efficiency > 60:\n                print(f\"   ‚ö†Ô∏è Good efficiency. Try {current_threads + 1} threads but monitor resources\")\n            elif efficiency > 40:\n                print(f\"   ‚ö†Ô∏è Moderate efficiency. Consider staying at {current_threads} threads\")\n            else:\n                print(f\"   ‚ùå Low efficiency. Try reducing to {max(1, current_threads - 1)} threads\")\n                \n            print(f\"   üìä Monitor Colab resources and adjust MAX_CONCURRENT_PATIENTS accordingly\")\n\n# Final system status\nfinal_system_info = get_system_info()\nif final_system_info and initial_system_info:\n    print(f\"\\nüìä FINAL SYSTEM STATUS:\")\n    print(f\"   CPU: {final_system_info['cpu_percent']:.1f}% (was {initial_system_info['cpu_percent']:.1f}%)\")\n    print(f\"   RAM: {final_system_info['memory_used_gb']:.1f}/{final_system_info['memory_total_gb']:.1f} GB ({final_system_info['memory_percent']:.1f}%)\")\n    print(f\"   RAM change: {final_system_info['memory_used_gb'] - initial_system_info['memory_used_gb']:+.1f} GB\")\n    print(f\"   Disk: {final_system_info['disk_free_gb']:.1f} GB free\")\n\nelse:\n    print(f\"‚ö†Ô∏è No features extracted in this batch\")\n\nprint(f\"\\nüîß TO EXPERIMENT WITH DIFFERENT THREAD COUNTS:\")\nprint(f\"   1. Change MAX_CONCURRENT_PATIENTS in Cell 1 (try {current_threads + 1})\")\nprint(f\"   2. Re-run this Cell 8 to test the new thread count\")\nprint(f\"   3. Compare the 'Threading efficiency' percentage\")\nprint(f\"   4. Find the sweet spot for your Colab instance!\")\n\n# Store processing statistics for analysis\nglobals()['batch_processing_stats'] = processing_stats\nglobals()['threading_experiment_results'] = {\n    'thread_count': current_threads,\n    'batch_time_minutes': batch_elapsed/60,\n    'successful_patients': successful_patients,\n    'failed_patients': failed_patients,\n    'frames_extracted': len(batch_features)\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Save Batch Results to Google Drive\n",
    "print(f\"üíæ SAVING BATCH {START_BATCH} TO GOOGLE DRIVE\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if batch_features:\n",
    "    # Convert to DataFrame\n",
    "    print(f\"üìä Converting {len(batch_features)} records to DataFrame...\")\n",
    "    df = pd.DataFrame(batch_features)\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"‚úÖ DataFrame created: {df.shape}\")\n",
    "    print(f\"üë• Unique patients: {df['patient_id'].nunique()}\")\n",
    "    \n",
    "    # Feature columns analysis\n",
    "    feature_cols = [col for col in df.columns if col.startswith('clean_')]\n",
    "    print(f\"üéØ Feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    # Save to Google Drive\n",
    "    batch_file_path = os.path.join(DRIVE_BASE_PATH, CURRENT_BATCH_FILE)\n",
    "    print(f\"üìÅ Saving to: {batch_file_path}\")\n",
    "    \n",
    "    df.to_csv(batch_file_path, index=False)\n",
    "    \n",
    "    # Verify file was saved\n",
    "    if os.path.exists(batch_file_path):\n",
    "        file_size_mb = os.path.getsize(batch_file_path) / (1024 * 1024)\n",
    "        print(f\"‚úÖ Successfully saved: {CURRENT_BATCH_FILE} ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Display sample of the data\n",
    "        print(f\"\\nüìã SAMPLE DATA:\")\n",
    "        print(df[['patient_id', 'timestamp', 'apnea_label', 'clean_rms', 'clean_zcr']].head())\n",
    "        \n",
    "        # Feature correlation analysis\n",
    "        print(f\"\\nüîó TOP 5 FEATURE CORRELATIONS WITH APNEA:\")\n",
    "        correlations = df[feature_cols].corrwith(df['apnea_label']).abs().sort_values(ascending=False)\n",
    "        for feature, corr in correlations.head().items():\n",
    "            print(f\"   {feature}: {corr:.3f}\")\n",
    "        \n",
    "        # Per-patient breakdown\n",
    "        print(f\"\\nüë§ PER-PATIENT BREAKDOWN:\")\n",
    "        patient_stats = df.groupby('patient_id').agg({\n",
    "            'apnea_label': ['count', 'sum', 'mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        for patient in df['patient_id'].unique():\n",
    "            count = patient_stats.loc[patient, ('apnea_label', 'count')]\n",
    "            apnea = patient_stats.loc[patient, ('apnea_label', 'sum')]\n",
    "            rate = patient_stats.loc[patient, ('apnea_label', 'mean')] * 100\n",
    "            print(f\"   {patient}: {count} frames, {apnea} apnea ({rate:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error: File was not saved to {batch_file_path}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No data to save - batch processing failed\")\n",
    "\n",
    "# Show all existing batch files\n",
    "print(f\"\\nüìÇ ALL BATCH FILES IN GOOGLE DRIVE:\")\n",
    "existing_batches = [f for f in os.listdir(DRIVE_BASE_PATH) if f.startswith('colab_dataset_batch') and f.endswith('.csv')]\n",
    "\n",
    "if existing_batches:\n",
    "    total_size_mb = 0\n",
    "    for batch_file in sorted(existing_batches):\n",
    "        file_path = os.path.join(DRIVE_BASE_PATH, batch_file)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size_mb += file_size_mb\n",
    "        print(f\"   ‚úÖ {batch_file} ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìä Total saved data: {total_size_mb:.1f} MB across {len(existing_batches)} batches\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed_batches = len(existing_batches)\n",
    "    remaining_batches = END_BATCH - completed_batches\n",
    "    if remaining_batches > 0:\n",
    "        print(f\"üöÄ Progress: {completed_batches}/{END_BATCH} batches complete\")\n",
    "        print(f\"üìã Next: Set START_BATCH = {completed_batches + 1} to continue\")\n",
    "    else:\n",
    "        print(f\"üéâ All {END_BATCH} batches completed! Dataset ready for analysis.\")\n",
    "else:\n",
    "    print(f\"   No batch files found\")\n",
    "\n",
    "print(f\"\\nüéØ BATCH {START_BATCH} PROCESSING COMPLETE!\")\n",
    "print(f\"Data safely saved to Google Drive and accessible from any device.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}