{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KOkkzgPLvW5B"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib # For saving/loading models\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier # Or import xgb.XGBClassifier if you prefer XGBoost\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE # For handling class imbalance\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Use imblearn's pipeline for SMOTE\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8SZrvZZ4vdvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31485928-f406-445c-e4c0-d8bc823b037d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data Information ---\n",
            "  patient_id  frame_start  frame_end    energy       zcr   centroid       rms  \\\n",
            "0  patient_1            0          1  0.019129  0.028809  37.717148  0.007989   \n",
            "1  patient_1            1          2  0.020015  0.026367  38.545484  0.008520   \n",
            "2  patient_1            2          3  0.021853  0.024902  36.254767  0.009332   \n",
            "3  patient_1            3          4  0.019863  0.034180  35.207018  0.008710   \n",
            "4  patient_1            4          5  0.020706  0.031250  37.350507  0.008874   \n",
            "\n",
            "   bandwidth  rolloff  flatness  ...    mfcc_5     mfcc_6     mfcc_7  \\\n",
            "0  29.413288   68.250  0.064825  ...  2.394709  11.713122 -20.107655   \n",
            "1  29.682406   74.375  0.060353  ...  1.224366  10.202329 -12.479948   \n",
            "2  28.744256   63.625  0.047224  ...  7.373545   9.634865 -13.173597   \n",
            "3  28.605743   61.875  0.061508  ...  5.338278  26.358368 -18.778702   \n",
            "4  28.894456   66.875  0.068465  ...  5.666430  25.209194 -12.862845   \n",
            "\n",
            "      mfcc_8     mfcc_9    mfcc_10   mfcc_11   mfcc_12    mfcc_13  label  \n",
            "0 -30.824373 -25.580160   4.932411 -6.033697 -7.714644 -10.305305      0  \n",
            "1 -33.959343 -33.336666   4.573047  0.097128 -6.020862  -7.547226      0  \n",
            "2 -38.454270 -33.303450   8.875240 -7.100756 -8.893260  -6.350083      0  \n",
            "3 -30.124352 -36.650150  11.536938 -1.522682 -2.361908 -10.071195      0  \n",
            "4 -24.396889 -33.623055   9.673685 -5.645969 -5.865208  -8.497774      0  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 311028 entries, 0 to 311027\n",
            "Data columns (total 27 columns):\n",
            " #   Column       Non-Null Count   Dtype  \n",
            "---  ------       --------------   -----  \n",
            " 0   patient_id   311028 non-null  object \n",
            " 1   frame_start  311028 non-null  int64  \n",
            " 2   frame_end    311028 non-null  int64  \n",
            " 3   energy       311028 non-null  float64\n",
            " 4   zcr          311028 non-null  float64\n",
            " 5   centroid     311028 non-null  float64\n",
            " 6   rms          311028 non-null  float64\n",
            " 7   bandwidth    311028 non-null  float64\n",
            " 8   rolloff      311028 non-null  float64\n",
            " 9   flatness     311028 non-null  float64\n",
            " 10  skew         311028 non-null  float64\n",
            " 11  kurt         311028 non-null  float64\n",
            " 12  entropy      311028 non-null  float64\n",
            " 13  mfcc_1       311028 non-null  float64\n",
            " 14  mfcc_2       311028 non-null  float64\n",
            " 15  mfcc_3       311028 non-null  float64\n",
            " 16  mfcc_4       311028 non-null  float64\n",
            " 17  mfcc_5       311028 non-null  float64\n",
            " 18  mfcc_6       311028 non-null  float64\n",
            " 19  mfcc_7       311028 non-null  float64\n",
            " 20  mfcc_8       311028 non-null  float64\n",
            " 21  mfcc_9       311028 non-null  float64\n",
            " 22  mfcc_10      311028 non-null  float64\n",
            " 23  mfcc_11      311028 non-null  float64\n",
            " 24  mfcc_12      311028 non-null  float64\n",
            " 25  mfcc_13      311028 non-null  float64\n",
            " 26  label        311028 non-null  int64  \n",
            "dtypes: float64(23), int64(3), object(1)\n",
            "memory usage: 64.1+ MB\n",
            "\n",
            "Initial Label Distribution:\n",
            "label\n",
            "0    258502\n",
            "1     52526\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Initial Label Distribution (Normalized):\n",
            "label\n",
            "0    0.831121\n",
            "1    0.168879\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load Data & Initial Inspection\n",
        "\n",
        "# --- IMPORTANT: Replace 'your_apnea_dataset.csv' with the actual path to your data ---\n",
        "data_path = 'master_apnea_dataset (1).csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Display initial data info\n",
        "print(\"--- Initial Data Information ---\")\n",
        "print(df.head())\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "print(\"\\nInitial Label Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nInitial Label Distribution (Normalized):\")\n",
        "print(df['label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MiCCRevGv1eI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9627713c-7e80-4c9c-9836-84aa46ca914b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Preprocessing & Cleaning ---\n",
            "Missing values before cleaning:\n",
            " patient_id     0\n",
            "frame_start    0\n",
            "frame_end      0\n",
            "energy         0\n",
            "zcr            0\n",
            "centroid       0\n",
            "rms            0\n",
            "bandwidth      0\n",
            "rolloff        0\n",
            "flatness       0\n",
            "skew           0\n",
            "kurt           0\n",
            "entropy        0\n",
            "mfcc_1         0\n",
            "mfcc_2         0\n",
            "mfcc_3         0\n",
            "mfcc_4         0\n",
            "mfcc_5         0\n",
            "mfcc_6         0\n",
            "mfcc_7         0\n",
            "mfcc_8         0\n",
            "mfcc_9         0\n",
            "mfcc_10        0\n",
            "mfcc_11        0\n",
            "mfcc_12        0\n",
            "mfcc_13        0\n",
            "label          0\n",
            "dtype: int64\n",
            "\n",
            "Identified 4888 rows where all key audio features are near zero.\n",
            "These rows might represent true silence or corrupted data. Deciding to remove them.\n",
            "Removed 4888 rows. New data shape: (306140, 23)\n",
            "\n",
            "Checking for infinite values...\n",
            "Data shape after all cleaning steps: (306140, 23)\n",
            "Label distribution after cleaning:\n",
            " label\n",
            "0    0.828621\n",
            "1    0.171379\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Data Preprocessing & Cleaning\n",
        "\n",
        "print(\"\\n--- Data Preprocessing & Cleaning ---\")\n",
        "\n",
        "# Check for explicit missing values (NaNs)\n",
        "print(\"Missing values before cleaning:\\n\", df.isnull().sum())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Exclude 'patient_id', 'frame_start', 'frame_end' and 'label' from features\n",
        "features = [col for col in df.columns if col not in ['patient_id', 'frame_start', 'frame_end', 'label']]\n",
        "X = df[features]\n",
        "y = df['label']\n",
        "patient_ids = df['patient_id'] # Keep patient_ids separate for splitting\n",
        "\n",
        "# Identify potential 'zero-filled' rows that might indicate problematic silent segments\n",
        "# Using a small epsilon for float comparison\n",
        "epsilon = 1e-6\n",
        "# Assuming 'energy', 'zcr', 'rms', 'bandwidth', 'rolloff' are typically non-zero for meaningful audio\n",
        "# MFCCs can also be problematic if all zeros (or a very large negative number indicating silence)\n",
        "key_audio_features_for_zero_check = ['energy', 'zcr', 'rms', 'bandwidth', 'rolloff'] + [f'mfcc_{i}' for i in range(1, 14)]\n",
        "\n",
        "# Find rows where all key audio features are near zero (or an extremely small value)\n",
        "problematic_zero_rows_indices = df[\n",
        "    (df[key_audio_features_for_zero_check].abs() < epsilon).all(axis=1)\n",
        "].index\n",
        "\n",
        "if not problematic_zero_rows_indices.empty:\n",
        "    print(f\"\\nIdentified {len(problematic_zero_rows_indices)} rows where all key audio features are near zero.\")\n",
        "    print(\"These rows might represent true silence or corrupted data. Deciding to remove them.\")\n",
        "    df_cleaned = df.drop(problematic_zero_rows_indices).reset_index(drop=True)\n",
        "\n",
        "    # Update X, y, and patient_ids with the cleaned data\n",
        "    X = df_cleaned[features]\n",
        "    y = df_cleaned['label']\n",
        "    patient_ids = df_cleaned['patient_id']\n",
        "    print(f\"Removed {len(problematic_zero_rows_indices)} rows. New data shape: {X.shape}\")\n",
        "else:\n",
        "    df_cleaned = df.copy()\n",
        "    print(\"\\nNo rows found with all key audio features near zero. Proceeding with original data.\")\n",
        "\n",
        "\n",
        "# Check for infinite values (e.g., from division by zero or log of zero)\n",
        "# Replace infinities with NaN, then drop rows with NaN. This is a robust way to handle them.\n",
        "# Apply this to the numerical columns only.\n",
        "print(\"\\nChecking for infinite values...\")\n",
        "initial_shape = df_cleaned.shape\n",
        "for col in X.select_dtypes(include=np.number).columns:\n",
        "    if np.isinf(X[col]).any():\n",
        "        print(f\"  Found infinite values in column: {col}\")\n",
        "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Drop rows where NaNs were introduced by cleaning (or were already present)\n",
        "initial_rows = X.shape[0]\n",
        "X, y, patient_ids = X.dropna(), y[X.dropna().index], patient_ids[X.dropna().index] # Ensure y and patient_ids are also aligned\n",
        "\n",
        "rows_after_nan_drop = X.shape[0]\n",
        "if initial_rows != rows_after_nan_drop:\n",
        "    print(f\"Dropped {initial_rows - rows_after_nan_drop} rows due to NaN values (including those from infinite replacement).\")\n",
        "\n",
        "print(\"Data shape after all cleaning steps:\", X.shape)\n",
        "print(\"Label distribution after cleaning:\\n\", y.value_counts(normalize=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BLaWrhnPv6bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12ee841-a83a-47fc-f868-0c2128b1d4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Splitting (Patient-Wise) ---\n",
            "Total unique patients: 20\n",
            "Patients in training set: 16\n",
            "Patients in test set: 4\n",
            "Train set shape: (244717, 23), Test set shape: (61423, 23)\n",
            "Train label distribution:\n",
            "label\n",
            "0    0.813454\n",
            "1    0.186546\n",
            "Name: proportion, dtype: float64\n",
            "Test label distribution:\n",
            "label\n",
            "0    0.889048\n",
            "1    0.110952\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Data Splitting (Patient-Wise)\n",
        "\n",
        "print(\"\\n--- Data Splitting (Patient-Wise) ---\")\n",
        "\n",
        "# Get unique patient IDs\n",
        "unique_patient_ids = patient_ids.unique()\n",
        "\n",
        "# Split patient IDs into training and testing sets\n",
        "# Stratify by patient's overall label prevalence if possible, otherwise just random.\n",
        "# For simplicity, we'll stratify based on the label for the *entire* patient_id group.\n",
        "# A more robust stratification might consider the proportion of apnea events per patient.\n",
        "# Here, we'll assign patients to train/test based on a random split.\n",
        "train_patient_ids, test_patient_ids = train_test_split(\n",
        "    unique_patient_ids,\n",
        "    test_size=0.2, # 20% of patients for testing\n",
        "    random_state=42\n",
        "    # stratify= # Can't directly stratify patient IDs by their *frame* labels easily.\n",
        "    # The current approach is to ensure a patient's data is either fully in train or fully in test.\n",
        ")\n",
        "\n",
        "# Filter the main DataFrame based on patient IDs\n",
        "X_train = X[patient_ids.isin(train_patient_ids)].copy()\n",
        "y_train = y[patient_ids.isin(train_patient_ids)].copy()\n",
        "X_test = X[patient_ids.isin(test_patient_ids)].copy()\n",
        "y_test = y[patient_ids.isin(test_patient_ids)].copy()\n",
        "\n",
        "# Keep patient IDs aligned for GroupKFold in hyperparameter tuning\n",
        "patient_ids_train = patient_ids[patient_ids.isin(train_patient_ids)].copy()\n",
        "\n",
        "print(f\"Total unique patients: {len(unique_patient_ids)}\")\n",
        "print(f\"Patients in training set: {len(train_patient_ids)}\")\n",
        "print(f\"Patients in test set: {len(test_patient_ids)}\")\n",
        "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "print(f\"Train label distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test label distribution:\\n{y_test.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JDbjUdRGv-f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "bde9d5f8-efe6-4f20-bfd4-cfa2bf6fe7e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cuda.bindings.cyruntime'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2180258269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Cell 5: Model Definition and Pipeline Construction (GPU VERSION)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcuRF\u001b[0m \u001b[0;31m# Import the GPU Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cuml/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mlibcuml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpylibraft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pylibraft/common/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mai_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mai_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcai_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcai_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice_ndarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceResources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceResourcesSNMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcuda.pyx\u001b[0m in \u001b[0;36minit pylibraft.common.cuda\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuda.bindings.cyruntime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# # Cell 5: Model Definition and Pipeline Construction\n",
        "\n",
        "# Cell 5: Model Definition and Pipeline Construction (GPU VERSION)\n",
        "from cuml.ensemble import RandomForestClassifier as cuRF # Import the GPU Random Forest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"\\n--- Model Definition and Pipeline Construction (GPU Accelerated) ---\")\n",
        "\n",
        "# Define the GPU classifier. The parameters are very similar to scikit-learn's.\n",
        "# NOTE: cuML's Random Forest does not have a 'class_weight' parameter.\n",
        "# We will rely on SMOTE (the slow but powerful method) or handle imbalance later if needed.\n",
        "# For now, let's build the fast pipeline without SMOTE to see the speed.\n",
        "classifier_gpu = cuRF(\n",
        "    n_estimators=100,\n",
        "    max_depth=16, # cuML requires a max_depth to be set\n",
        "    random_state=42,\n",
        "    n_streams=1 # Important for performance on a single GPU\n",
        ")\n",
        "\n",
        "\n",
        "# Create a standard pipeline. The GPU model will work within it.\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', classifier_gpu)\n",
        "])\n",
        "\n",
        "print(\"GPU-Accelerated ML Pipeline created:\")\n",
        "print(pipeline)\n",
        "\n",
        "\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# print(\"\\n--- Model Definition and Pipeline Construction ---\")\n",
        "\n",
        "# # Define your classifier. RandomForestClassifier is a good choice for a start.\n",
        "# # For imbalanced datasets, 'class_weight=\"balanced\"' can be an alternative to SMOTE,\n",
        "# # but SMOTE often works well by generating synthetic samples.\n",
        "# # classifier = RandomForestClassifier(random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "# classifier = RandomForestClassifier(\n",
        "#     random_state=42,\n",
        "#     class_weight='balanced', # <-- FASTER ALTERNATIVE TO SMOTE\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Alternatively, if you want to use XGBoost (uncomment the lines below):\n",
        "# # import xgboost as xgb\n",
        "# # classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "# # If using XGBoost without SMOTE, consider 'scale_pos_weight' for imbalance:\n",
        "# # classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss',\n",
        "# #                                scale_pos_weight=sum(y_train == 0) / sum(y_train == 1))\n",
        "\n",
        "\n",
        "# # Create an ImbPipeline to chain preprocessing (scaling, SMOTE) with the classifier.\n",
        "# # The StandardScaler ensures data is scaled BEFORE SMOTE creates synthetic samples.\n",
        "# # SMOTE is only applied to the training data *within* each cross-validation fold during fitting.\n",
        "# # pipeline = ImbPipeline([\n",
        "# #     ('scaler', StandardScaler()),        # Step 1: Standardize features\n",
        "# #     ('smote', SMOTE(random_state=42)),   # Step 2: Handle imbalance via oversampling\n",
        "# #     ('classifier', classifier)           # Step 3: The machine learning model\n",
        "# # ])\n",
        "\n",
        "# pipeline = Pipeline([\n",
        "#     ('scaler', StandardScaler()),        # Step 1: Standardize features\n",
        "#     ('classifier', classifier)           # Step 2: The machine learning model\n",
        "# ])\n",
        "# print(\"ML Pipeline created:\")\n",
        "# print(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH_iYTqKwC6J"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Hyperparameter Tuning (with CORRECTED TQDM Progress Bar)\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning (GridSearchCV with GroupKFold) ---\")\n",
        "\n",
        "# Define parameter grid (this remains the same)\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# param_grid = {\n",
        "#     'classifier__n_estimators': [100],          # Test only one, reasonable number of trees\n",
        "#     'classifier__max_depth': [10, 15],           # IMPORTANT: Use specific, shallower depths\n",
        "#     'classifier__min_samples_split': [2, 5]\n",
        "# }\n",
        "\n",
        "# Use GroupKFold for cross-validation (this remains the same)\n",
        "cv = GroupKFold(n_splits=3)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# --- THIS IS THE CORRECTED PART ---\n",
        "# Calculate the total number of fits correctly\n",
        "num_fits = len(ParameterGrid(param_grid)) * cv.get_n_splits(X_train, y_train, groups=patient_ids_train)\n",
        "print(f\"Fitting {cv.get_n_splits(X_train, y_train, groups=patient_ids_train)} folds for each of {len(ParameterGrid(param_grid))} candidates, totalling {num_fits} fits...\")\n",
        "# --- END CORRECTION ---\n",
        "\n",
        "# Use joblib's context manager to show a tqdm progress bar with the correct total\n",
        "with tqdm(total=num_fits) as pbar:\n",
        "    with joblib.parallel_backend('threading'):\n",
        "        grid_search.fit(X_train, y_train, groups=patient_ids_train)\n",
        "\n",
        "# Get the best model (this remains the same)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\n--- Grid Search Results ---\")\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation F1-score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uEJwMduwEBU"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Model Evaluation\n",
        "\n",
        "print(\"\\n--- Model Evaluation on Test Set ---\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1] # Probability of the positive class (apnea)\n",
        "\n",
        "# Print classification metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cm_display = ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "print(\"\\nROC Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "roc_display = RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve (often more informative for imbalanced data)\n",
        "print(\"\\nPrecision-Recall Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "pr_display = PrecisionRecallDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nagWnswI0_"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Model Saving (Optional)\n",
        "\n",
        "model_filename = 'apnea_detection_model.pkl'\n",
        "try:\n",
        "    joblib.dump(best_model, model_filename)\n",
        "    print(f\"\\nModel saved successfully as '{model_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Example of how to load the model later:\n",
        "# loaded_model = joblib.load(model_filename)\n",
        "# print(f\"Model loaded successfully from '{model_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to install RAPIDS cuML for GPU acceleration\n",
        "!pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z75p6677-Hql",
        "outputId": "188f63db-ed63-408f-9cb6-90380a306553"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting cuml-cu11\n",
            "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-25.6.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cuda-python<12.0a0,>=11.8.5 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (11.8.7)\n",
            "Requirement already satisfied: cudf-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: cupy-cuda11x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (13.5.1)\n",
            "Requirement already satisfied: cuvs-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.1)\n",
            "Requirement already satisfied: dask-cuda==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: dask-cudf-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (1.5.1)\n",
            "Requirement already satisfied: libcuml-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: numba<0.62.0a0,>=0.59.1 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (2.0.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (11.7.5.86)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.0)\n",
            "Requirement already satisfied: pylibraft-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: raft-dask-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: rapids-dask-dependency==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: rmm-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: scikit-learn>=1.5 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (1.15.3)\n",
            "Requirement already satisfied: treelite==4.4.1 in /usr/local/lib/python3.11/dist-packages (from cuml-cu11) (4.4.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (5.5.2)\n",
            "Requirement already satisfied: cubinlinker-cu11 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (0.3.0.post3)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (2025.3.2)\n",
            "Requirement already satisfied: libcudf-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: numba-cuda<0.12.0a0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (0.11.0)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (0.2.12)\n",
            "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (2.2.2)\n",
            "Requirement already satisfied: ptxcompiler-cu11 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (0.8.1.post3)\n",
            "Requirement already satisfied: pyarrow<20.0.0a0,>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (18.1.0)\n",
            "Requirement already satisfied: pylibcudf-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu11==25.6.*->cuml-cu11) (4.14.1)\n",
            "Requirement already satisfied: libcuvs-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from cuvs-cu11==25.6.*->cuml-cu11) (25.6.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.6.*->cuml-cu11) (8.2.1)\n",
            "Requirement already satisfied: pynvml<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.6.*->cuml-cu11) (12.0.0)\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.6.*->cuml-cu11) (3.0.0)\n",
            "Requirement already satisfied: libraft-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from libcuml-cu11==25.6.*->cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: rapids-logger==0.1.* in /usr/local/lib/python3.11/dist-packages (from libcuml-cu11==25.6.*->cuml-cu11) (0.1.1)\n",
            "Requirement already satisfied: distributed-ucxx-cu11==0.44.* in /usr/local/lib/python3.11/dist-packages (from raft-dask-cu11==25.6.*->cuml-cu11) (0.44.0)\n",
            "Requirement already satisfied: ucx-py-cu11==0.44.* in /usr/local/lib/python3.11/dist-packages (from raft-dask-cu11==25.6.*->cuml-cu11) (0.44.0)\n",
            "Requirement already satisfied: dask==2025.5.0 in /usr/local/lib/python3.11/dist-packages (from rapids-dask-dependency==25.6.*->cuml-cu11) (2025.5.0)\n",
            "Requirement already satisfied: distributed==2025.5.0 in /usr/local/lib/python3.11/dist-packages (from rapids-dask-dependency==25.6.*->cuml-cu11) (2025.5.0)\n",
            "Requirement already satisfied: librmm-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from rmm-cu11==25.6.*->cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (3.1.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (8.7.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (3.1.6)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (1.1.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (3.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (2.4.0)\n",
            "Requirement already satisfied: ucxx-cu11==0.44.* in /usr/local/lib/python3.11/dist-packages (from distributed-ucxx-cu11==0.44.*->raft-dask-cu11==25.6.*->cuml-cu11) (0.44.0)\n",
            "Requirement already satisfied: libkvikio-cu11==25.6.* in /usr/local/lib/python3.11/dist-packages (from libcudf-cu11==25.6.*->cudf-cu11==25.6.*->cuml-cu11) (25.6.0)\n",
            "Requirement already satisfied: libucx-cu11<1.19,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from ucx-py-cu11==0.44.*->raft-dask-cu11==25.6.*->cuml-cu11) (1.18.1)\n",
            "Requirement already satisfied: libucxx-cu11==0.44.* in /usr/local/lib/python3.11/dist-packages (from ucxx-cu11==0.44.*->distributed-ucxx-cu11==0.44.*->raft-dask-cu11==25.6.*->cuml-cu11) (0.44.0)\n",
            "Requirement already satisfied: cuda-bindings~=11.8.7 in /usr/local/lib/python3.11/dist-packages (from cuda-python<12.0a0,>=11.8.5->cuml-cu11) (11.8.7)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda11x>=12.0.0->cuml-cu11) (0.8.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.62.0a0,>=0.59.1->cuml-cu11) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5->cuml-cu11) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu11==25.6.*->cuml-cu11) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu11==25.6.*->cuml-cu11) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu11==25.6.*->cuml-cu11) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml<13.0.0a0,>=12.0.0->dask-cuda==25.6.*->cuml-cu11) (12.575.51)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu11==25.6.*->cuml-cu11) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu11==25.6.*->cuml-cu11) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10.3->distributed==2025.5.0->rapids-dask-dependency==25.6.*->cuml-cu11) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu11==25.6.*->cuml-cu11) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu11==25.6.*->cuml-cu11) (1.17.0)\n",
            "Installing collected packages: cuml-cu11\n",
            "Successfully installed cuml-cu11-25.6.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}