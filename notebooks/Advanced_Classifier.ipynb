{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KOkkzgPLvW5B"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib # For saving/loading models\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier # Or import xgb.XGBClassifier if you prefer XGBoost\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE # For handling class imbalance\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Use imblearn's pipeline for SMOTE\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8SZrvZZ4vdvO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
            "C:\\Users\\solom\\AppData\\Local\\Temp\\ipykernel_22436\\1999110054.py:4: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  data_path = '..\\master_apnea_dataset (1).csv'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Initial Data Information ---\n",
            "  patient_id  frame_start  frame_end    energy       zcr   centroid       rms  \\\n",
            "0  patient_1            0          1  0.019129  0.028809  37.717148  0.007989   \n",
            "1  patient_1            1          2  0.020015  0.026367  38.545484  0.008520   \n",
            "2  patient_1            2          3  0.021853  0.024902  36.254767  0.009332   \n",
            "3  patient_1            3          4  0.019863  0.034180  35.207018  0.008710   \n",
            "4  patient_1            4          5  0.020706  0.031250  37.350507  0.008874   \n",
            "\n",
            "   bandwidth  rolloff  flatness  ...    mfcc_5     mfcc_6     mfcc_7  \\\n",
            "0  29.413288   68.250  0.064825  ...  2.394709  11.713122 -20.107655   \n",
            "1  29.682406   74.375  0.060353  ...  1.224366  10.202329 -12.479948   \n",
            "2  28.744256   63.625  0.047224  ...  7.373545   9.634865 -13.173597   \n",
            "3  28.605743   61.875  0.061508  ...  5.338278  26.358368 -18.778702   \n",
            "4  28.894456   66.875  0.068465  ...  5.666430  25.209194 -12.862845   \n",
            "\n",
            "      mfcc_8     mfcc_9    mfcc_10   mfcc_11   mfcc_12    mfcc_13  label  \n",
            "0 -30.824373 -25.580160   4.932411 -6.033697 -7.714644 -10.305305      0  \n",
            "1 -33.959343 -33.336666   4.573047  0.097128 -6.020862  -7.547226      0  \n",
            "2 -38.454270 -33.303450   8.875240 -7.100756 -8.893260  -6.350083      0  \n",
            "3 -30.124352 -36.650150  11.536938 -1.522682 -2.361908 -10.071195      0  \n",
            "4 -24.396889 -33.623055   9.673685 -5.645969 -5.865208  -8.497774      0  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 311028 entries, 0 to 311027\n",
            "Data columns (total 27 columns):\n",
            " #   Column       Non-Null Count   Dtype  \n",
            "---  ------       --------------   -----  \n",
            " 0   patient_id   311028 non-null  object \n",
            " 1   frame_start  311028 non-null  int64  \n",
            " 2   frame_end    311028 non-null  int64  \n",
            " 3   energy       311028 non-null  float64\n",
            " 4   zcr          311028 non-null  float64\n",
            " 5   centroid     311028 non-null  float64\n",
            " 6   rms          311028 non-null  float64\n",
            " 7   bandwidth    311028 non-null  float64\n",
            " 8   rolloff      311028 non-null  float64\n",
            " 9   flatness     311028 non-null  float64\n",
            " 10  skew         311028 non-null  float64\n",
            " 11  kurt         311028 non-null  float64\n",
            " 12  entropy      311028 non-null  float64\n",
            " 13  mfcc_1       311028 non-null  float64\n",
            " 14  mfcc_2       311028 non-null  float64\n",
            " 15  mfcc_3       311028 non-null  float64\n",
            " 16  mfcc_4       311028 non-null  float64\n",
            " 17  mfcc_5       311028 non-null  float64\n",
            " 18  mfcc_6       311028 non-null  float64\n",
            " 19  mfcc_7       311028 non-null  float64\n",
            " 20  mfcc_8       311028 non-null  float64\n",
            " 21  mfcc_9       311028 non-null  float64\n",
            " 22  mfcc_10      311028 non-null  float64\n",
            " 23  mfcc_11      311028 non-null  float64\n",
            " 24  mfcc_12      311028 non-null  float64\n",
            " 25  mfcc_13      311028 non-null  float64\n",
            " 26  label        311028 non-null  int64  \n",
            "dtypes: float64(23), int64(3), object(1)\n",
            "memory usage: 64.1+ MB\n",
            "\n",
            "Initial Label Distribution:\n",
            "label\n",
            "0    258502\n",
            "1     52526\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Initial Label Distribution (Normalized):\n",
            "label\n",
            "0    0.831121\n",
            "1    0.168879\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load Data & Initial Inspection\n",
        "\n",
        "# --- IMPORTANT: Replace 'your_apnea_dataset.csv' with the actual path to your data ---\n",
        "data_path = '..\\master_apnea_dataset (1).csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Display initial data info\n",
        "print(\"--- Initial Data Information ---\")\n",
        "print(df.head())\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "print(\"\\nInitial Label Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nInitial Label Distribution (Normalized):\")\n",
        "print(df['label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MiCCRevGv1eI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Preprocessing & Cleaning ---\n",
            "Missing values before cleaning:\n",
            " patient_id     0\n",
            "frame_start    0\n",
            "frame_end      0\n",
            "energy         0\n",
            "zcr            0\n",
            "centroid       0\n",
            "rms            0\n",
            "bandwidth      0\n",
            "rolloff        0\n",
            "flatness       0\n",
            "skew           0\n",
            "kurt           0\n",
            "entropy        0\n",
            "mfcc_1         0\n",
            "mfcc_2         0\n",
            "mfcc_3         0\n",
            "mfcc_4         0\n",
            "mfcc_5         0\n",
            "mfcc_6         0\n",
            "mfcc_7         0\n",
            "mfcc_8         0\n",
            "mfcc_9         0\n",
            "mfcc_10        0\n",
            "mfcc_11        0\n",
            "mfcc_12        0\n",
            "mfcc_13        0\n",
            "label          0\n",
            "dtype: int64\n",
            "\n",
            "Identified 4888 rows where all key audio features are near zero.\n",
            "These rows might represent true silence or corrupted data. Deciding to remove them.\n",
            "Removed 4888 rows. New data shape: (306140, 23)\n",
            "\n",
            "Checking for infinite values...\n",
            "Data shape after all cleaning steps: (306140, 23)\n",
            "Label distribution after cleaning:\n",
            " label\n",
            "0    0.828621\n",
            "1    0.171379\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Data Preprocessing & Cleaning\n",
        "\n",
        "print(\"\\n--- Data Preprocessing & Cleaning ---\")\n",
        "\n",
        "# Check for explicit missing values (NaNs)\n",
        "print(\"Missing values before cleaning:\\n\", df.isnull().sum())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Exclude 'patient_id', 'frame_start', 'frame_end' and 'label' from features\n",
        "features = [col for col in df.columns if col not in ['patient_id', 'frame_start', 'frame_end', 'label']]\n",
        "X = df[features]\n",
        "y = df['label']\n",
        "patient_ids = df['patient_id'] # Keep patient_ids separate for splitting\n",
        "\n",
        "# Identify potential 'zero-filled' rows that might indicate problematic silent segments\n",
        "# Using a small epsilon for float comparison\n",
        "epsilon = 1e-6\n",
        "# Assuming 'energy', 'zcr', 'rms', 'bandwidth', 'rolloff' are typically non-zero for meaningful audio\n",
        "# MFCCs can also be problematic if all zeros (or a very large negative number indicating silence)\n",
        "key_audio_features_for_zero_check = ['energy', 'zcr', 'rms', 'bandwidth', 'rolloff'] + [f'mfcc_{i}' for i in range(1, 14)]\n",
        "\n",
        "# Find rows where all key audio features are near zero (or an extremely small value)\n",
        "problematic_zero_rows_indices = df[\n",
        "    (df[key_audio_features_for_zero_check].abs() < epsilon).all(axis=1)\n",
        "].index\n",
        "\n",
        "if not problematic_zero_rows_indices.empty:\n",
        "    print(f\"\\nIdentified {len(problematic_zero_rows_indices)} rows where all key audio features are near zero.\")\n",
        "    print(\"These rows might represent true silence or corrupted data. Deciding to remove them.\")\n",
        "    df_cleaned = df.drop(problematic_zero_rows_indices).reset_index(drop=True)\n",
        "\n",
        "    # Update X, y, and patient_ids with the cleaned data\n",
        "    X = df_cleaned[features]\n",
        "    y = df_cleaned['label']\n",
        "    patient_ids = df_cleaned['patient_id']\n",
        "    print(f\"Removed {len(problematic_zero_rows_indices)} rows. New data shape: {X.shape}\")\n",
        "else:\n",
        "    df_cleaned = df.copy()\n",
        "    print(\"\\nNo rows found with all key audio features near zero. Proceeding with original data.\")\n",
        "\n",
        "\n",
        "# Check for infinite values (e.g., from division by zero or log of zero)\n",
        "# Replace infinities with NaN, then drop rows with NaN. This is a robust way to handle them.\n",
        "# Apply this to the numerical columns only.\n",
        "print(\"\\nChecking for infinite values...\")\n",
        "initial_shape = df_cleaned.shape\n",
        "for col in X.select_dtypes(include=np.number).columns:\n",
        "    if np.isinf(X[col]).any():\n",
        "        print(f\"  Found infinite values in column: {col}\")\n",
        "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Drop rows where NaNs were introduced by cleaning (or were already present)\n",
        "initial_rows = X.shape[0]\n",
        "X, y, patient_ids = X.dropna(), y[X.dropna().index], patient_ids[X.dropna().index] # Ensure y and patient_ids are also aligned\n",
        "\n",
        "rows_after_nan_drop = X.shape[0]\n",
        "if initial_rows != rows_after_nan_drop:\n",
        "    print(f\"Dropped {initial_rows - rows_after_nan_drop} rows due to NaN values (including those from infinite replacement).\")\n",
        "\n",
        "print(\"Data shape after all cleaning steps:\", X.shape)\n",
        "print(\"Label distribution after cleaning:\\n\", y.value_counts(normalize=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BLaWrhnPv6bl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Splitting (Patient-Wise) ---\n",
            "Total unique patients: 20\n",
            "Patients in training set: 16\n",
            "Patients in test set: 4\n",
            "Train set shape: (244717, 23), Test set shape: (61423, 23)\n",
            "Train label distribution:\n",
            "label\n",
            "0    0.813454\n",
            "1    0.186546\n",
            "Name: proportion, dtype: float64\n",
            "Test label distribution:\n",
            "label\n",
            "0    0.889048\n",
            "1    0.110952\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Data Splitting (Patient-Wise)\n",
        "\n",
        "print(\"\\n--- Data Splitting (Patient-Wise) ---\")\n",
        "\n",
        "# Get unique patient IDs\n",
        "unique_patient_ids = patient_ids.unique()\n",
        "\n",
        "# Split patient IDs into training and testing sets\n",
        "# Stratify by patient's overall label prevalence if possible, otherwise just random.\n",
        "# For simplicity, we'll stratify based on the label for the *entire* patient_id group.\n",
        "# A more robust stratification might consider the proportion of apnea events per patient.\n",
        "# Here, we'll assign patients to train/test based on a random split.\n",
        "train_patient_ids, test_patient_ids = train_test_split(\n",
        "    unique_patient_ids,\n",
        "    test_size=0.2, # 20% of patients for testing\n",
        "    random_state=42\n",
        "    # stratify= # Can't directly stratify patient IDs by their *frame* labels easily.\n",
        "    # The current approach is to ensure a patient's data is either fully in train or fully in test.\n",
        ")\n",
        "\n",
        "# Filter the main DataFrame based on patient IDs\n",
        "X_train = X[patient_ids.isin(train_patient_ids)].copy()\n",
        "y_train = y[patient_ids.isin(train_patient_ids)].copy()\n",
        "X_test = X[patient_ids.isin(test_patient_ids)].copy()\n",
        "y_test = y[patient_ids.isin(test_patient_ids)].copy()\n",
        "\n",
        "# Keep patient IDs aligned for GroupKFold in hyperparameter tuning\n",
        "patient_ids_train = patient_ids[patient_ids.isin(train_patient_ids)].copy()\n",
        "\n",
        "print(f\"Total unique patients: {len(unique_patient_ids)}\")\n",
        "print(f\"Patients in training set: {len(train_patient_ids)}\")\n",
        "print(f\"Patients in test set: {len(test_patient_ids)}\")\n",
        "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "print(f\"Train label distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test label distribution:\\n{y_test.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JDbjUdRGv-f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model Definition and Pipeline Construction ---\n",
            "ML Pipeline created:\n",
            "Pipeline(steps=[('scaler', StandardScaler()), ('smote', SMOTE(random_state=42)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(n_jobs=-1, random_state=42))])\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Model Definition and Pipeline Construction\n",
        "\n",
        "print(\"\\n--- Model Definition and Pipeline Construction ---\")\n",
        "\n",
        "# Define your classifier. RandomForestClassifier is a good choice for a start.\n",
        "# For imbalanced datasets, 'class_weight=\"balanced\"' can be an alternative to SMOTE,\n",
        "# but SMOTE often works well by generating synthetic samples.\n",
        "classifier = RandomForestClassifier(random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "# Alternatively, if you want to use XGBoost (uncomment the lines below):\n",
        "# import xgboost as xgb\n",
        "# classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "# If using XGBoost without SMOTE, consider 'scale_pos_weight' for imbalance:\n",
        "# classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss',\n",
        "#                                scale_pos_weight=sum(y_train == 0) / sum(y_train == 1))\n",
        "\n",
        "\n",
        "# Create an ImbPipeline to chain preprocessing (scaling, SMOTE) with the classifier.\n",
        "# The StandardScaler ensures data is scaled BEFORE SMOTE creates synthetic samples.\n",
        "# SMOTE is only applied to the training data *within* each cross-validation fold during fitting.\n",
        "pipeline = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),        # Step 1: Standardize features\n",
        "    ('smote', SMOTE(random_state=42)),   # Step 2: Handle imbalance via oversampling\n",
        "    ('classifier', classifier)           # Step 3: The machine learning model\n",
        "])\n",
        "\n",
        "print(\"ML Pipeline created:\")\n",
        "print(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_iYTqKwC6J",
        "outputId": "3c7567d3-81d5-41bc-f8e1-e9c23c57cbb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Hyperparameter Tuning (GridSearchCV with GroupKFold) ---\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv.get_n_splits(X_train,\u001b[38;5;250m \u001b[39my_train,\u001b[38;5;250m \u001b[39mgroups=patient_ids_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ParameterGrid(param_grid))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidates, totalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# --- END CORRECTION ---\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Use joblib's context manager to show a tqdm progress bar with the correct total\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_fits\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m joblib.parallel_backend(\u001b[33m'\u001b[39m\u001b[33mthreading\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     43\u001b[39m         grid_search.fit(X_train, y_train, groups=patient_ids_train)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
            "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
          ]
        }
      ],
      "source": [
        "# Cell 6: Hyperparameter Tuning (with CORRECTED TQDM Progress Bar)\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning (GridSearchCV with GroupKFold) ---\")\n",
        "\n",
        "# Define parameter grid (this remains the same)\n",
        "# param_grid = {\n",
        "#     'classifier__n_estimators': [100, 200],\n",
        "#     'classifier__max_depth': [10, 20, None],\n",
        "#     'classifier__min_samples_split': [2, 5]\n",
        "# }\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100],          # Test only one, reasonable number of trees\n",
        "    'classifier__max_depth': [10, 15],           # IMPORTANT: Use specific, shallower depths\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Use GroupKFold for cross-validation (this remains the same)\n",
        "cv = GroupKFold(n_splits=3)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# --- THIS IS THE CORRECTED PART ---\n",
        "# Calculate the total number of fits correctly\n",
        "num_fits = len(ParameterGrid(param_grid)) * cv.get_n_splits(X_train, y_train, groups=patient_ids_train)\n",
        "print(f\"Fitting {cv.get_n_splits(X_train, y_train, groups=patient_ids_train)} folds for each of {len(ParameterGrid(param_grid))} candidates, totalling {num_fits} fits...\")\n",
        "# --- END CORRECTION ---\n",
        "\n",
        "# Use joblib's context manager to show a tqdm progress bar with the correct total\n",
        "with tqdm(total=num_fits) as pbar:\n",
        "    with joblib.parallel_backend('threading'):\n",
        "        grid_search.fit(X_train, y_train, groups=patient_ids_train)\n",
        "\n",
        "# Get the best model (this remains the same)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\n--- Grid Search Results ---\")\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation F1-score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uEJwMduwEBU"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Model Evaluation\n",
        "\n",
        "print(\"\\n--- Model Evaluation on Test Set ---\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1] # Probability of the positive class (apnea)\n",
        "\n",
        "# Print classification metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cm_display = ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "print(\"\\nROC Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "roc_display = RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve (often more informative for imbalanced data)\n",
        "print(\"\\nPrecision-Recall Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "pr_display = PrecisionRecallDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nagWnswI0_"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Model Saving (Optional)\n",
        "\n",
        "model_filename = 'apnea_detection_model.pkl'\n",
        "try:\n",
        "    joblib.dump(best_model, model_filename)\n",
        "    print(f\"\\nModel saved successfully as '{model_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Example of how to load the model later:\n",
        "# loaded_model = joblib.load(model_filename)\n",
        "# print(f\"Model loaded successfully from '{model_filename}'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
