{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KOkkzgPLvW5B"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib # For saving/loading models\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier # Or import xgb.XGBClassifier if you prefer XGBoost\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE # For handling class imbalance\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Use imblearn's pipeline for SMOTE\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZrvZZ4vdvO"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '..master_apnea_dataset (1).csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 2: Load Data & Initial Inspection\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --- IMPORTANT: Replace 'your_apnea_dataset.csv' with the actual path to your data ---\u001b[39;00m\n\u001b[32m      4\u001b[39m data_path = \u001b[33m'\u001b[39m\u001b[33m..master_apnea_dataset (1).csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display initial data info\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Initial Data Information ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solom\\Documents\\Evaluating-Noise-Reduction-Techniques\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..master_apnea_dataset (1).csv'"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load Data & Initial Inspection\n",
        "\n",
        "# --- IMPORTANT: Replace 'your_apnea_dataset.csv' with the actual path to your data ---\n",
        "data_path = '..\\master_apnea_dataset (1).csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Display initial data info\n",
        "print(\"--- Initial Data Information ---\")\n",
        "print(df.head())\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "print(\"\\nInitial Label Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nInitial Label Distribution (Normalized):\")\n",
        "print(df['label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiCCRevGv1eI"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Data Preprocessing & Cleaning\n",
        "\n",
        "print(\"\\n--- Data Preprocessing & Cleaning ---\")\n",
        "\n",
        "# Check for explicit missing values (NaNs)\n",
        "print(\"Missing values before cleaning:\\n\", df.isnull().sum())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Exclude 'patient_id', 'frame_start', 'frame_end' and 'label' from features\n",
        "features = [col for col in df.columns if col not in ['patient_id', 'frame_start', 'frame_end', 'label']]\n",
        "X = df[features]\n",
        "y = df['label']\n",
        "patient_ids = df['patient_id'] # Keep patient_ids separate for splitting\n",
        "\n",
        "# Identify potential 'zero-filled' rows that might indicate problematic silent segments\n",
        "# Using a small epsilon for float comparison\n",
        "epsilon = 1e-6\n",
        "# Assuming 'energy', 'zcr', 'rms', 'bandwidth', 'rolloff' are typically non-zero for meaningful audio\n",
        "# MFCCs can also be problematic if all zeros (or a very large negative number indicating silence)\n",
        "key_audio_features_for_zero_check = ['energy', 'zcr', 'rms', 'bandwidth', 'rolloff'] + [f'mfcc_{i}' for i in range(1, 14)]\n",
        "\n",
        "# Find rows where all key audio features are near zero (or an extremely small value)\n",
        "problematic_zero_rows_indices = df[\n",
        "    (df[key_audio_features_for_zero_check].abs() < epsilon).all(axis=1)\n",
        "].index\n",
        "\n",
        "if not problematic_zero_rows_indices.empty:\n",
        "    print(f\"\\nIdentified {len(problematic_zero_rows_indices)} rows where all key audio features are near zero.\")\n",
        "    print(\"These rows might represent true silence or corrupted data. Deciding to remove them.\")\n",
        "    df_cleaned = df.drop(problematic_zero_rows_indices).reset_index(drop=True)\n",
        "\n",
        "    # Update X, y, and patient_ids with the cleaned data\n",
        "    X = df_cleaned[features]\n",
        "    y = df_cleaned['label']\n",
        "    patient_ids = df_cleaned['patient_id']\n",
        "    print(f\"Removed {len(problematic_zero_rows_indices)} rows. New data shape: {X.shape}\")\n",
        "else:\n",
        "    df_cleaned = df.copy()\n",
        "    print(\"\\nNo rows found with all key audio features near zero. Proceeding with original data.\")\n",
        "\n",
        "\n",
        "# Check for infinite values (e.g., from division by zero or log of zero)\n",
        "# Replace infinities with NaN, then drop rows with NaN. This is a robust way to handle them.\n",
        "# Apply this to the numerical columns only.\n",
        "print(\"\\nChecking for infinite values...\")\n",
        "initial_shape = df_cleaned.shape\n",
        "for col in X.select_dtypes(include=np.number).columns:\n",
        "    if np.isinf(X[col]).any():\n",
        "        print(f\"  Found infinite values in column: {col}\")\n",
        "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Drop rows where NaNs were introduced by cleaning (or were already present)\n",
        "initial_rows = X.shape[0]\n",
        "X, y, patient_ids = X.dropna(), y[X.dropna().index], patient_ids[X.dropna().index] # Ensure y and patient_ids are also aligned\n",
        "\n",
        "rows_after_nan_drop = X.shape[0]\n",
        "if initial_rows != rows_after_nan_drop:\n",
        "    print(f\"Dropped {initial_rows - rows_after_nan_drop} rows due to NaN values (including those from infinite replacement).\")\n",
        "\n",
        "print(\"Data shape after all cleaning steps:\", X.shape)\n",
        "print(\"Label distribution after cleaning:\\n\", y.value_counts(normalize=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLaWrhnPv6bl"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Data Splitting (Patient-Wise)\n",
        "\n",
        "print(\"\\n--- Data Splitting (Patient-Wise) ---\")\n",
        "\n",
        "# Get unique patient IDs\n",
        "unique_patient_ids = patient_ids.unique()\n",
        "\n",
        "# Split patient IDs into training and testing sets\n",
        "# Stratify by patient's overall label prevalence if possible, otherwise just random.\n",
        "# For simplicity, we'll stratify based on the label for the *entire* patient_id group.\n",
        "# A more robust stratification might consider the proportion of apnea events per patient.\n",
        "# Here, we'll assign patients to train/test based on a random split.\n",
        "train_patient_ids, test_patient_ids = train_test_split(\n",
        "    unique_patient_ids,\n",
        "    test_size=0.2, # 20% of patients for testing\n",
        "    random_state=42\n",
        "    # stratify= # Can't directly stratify patient IDs by their *frame* labels easily.\n",
        "    # The current approach is to ensure a patient's data is either fully in train or fully in test.\n",
        ")\n",
        "\n",
        "# Filter the main DataFrame based on patient IDs\n",
        "X_train = X[patient_ids.isin(train_patient_ids)].copy()\n",
        "y_train = y[patient_ids.isin(train_patient_ids)].copy()\n",
        "X_test = X[patient_ids.isin(test_patient_ids)].copy()\n",
        "y_test = y[patient_ids.isin(test_patient_ids)].copy()\n",
        "\n",
        "# Keep patient IDs aligned for GroupKFold in hyperparameter tuning\n",
        "patient_ids_train = patient_ids[patient_ids.isin(train_patient_ids)].copy()\n",
        "\n",
        "print(f\"Total unique patients: {len(unique_patient_ids)}\")\n",
        "print(f\"Patients in training set: {len(train_patient_ids)}\")\n",
        "print(f\"Patients in test set: {len(test_patient_ids)}\")\n",
        "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "print(f\"Train label distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test label distribution:\\n{y_test.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDbjUdRGv-f4"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Model Definition and Pipeline Construction\n",
        "\n",
        "print(\"\\n--- Model Definition and Pipeline Construction ---\")\n",
        "\n",
        "# Define your classifier. RandomForestClassifier is a good choice for a start.\n",
        "# For imbalanced datasets, 'class_weight=\"balanced\"' can be an alternative to SMOTE,\n",
        "# but SMOTE often works well by generating synthetic samples.\n",
        "classifier = RandomForestClassifier(random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "# Alternatively, if you want to use XGBoost (uncomment the lines below):\n",
        "# import xgboost as xgb\n",
        "# classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "# If using XGBoost without SMOTE, consider 'scale_pos_weight' for imbalance:\n",
        "# classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss',\n",
        "#                                scale_pos_weight=sum(y_train == 0) / sum(y_train == 1))\n",
        "\n",
        "\n",
        "# Create an ImbPipeline to chain preprocessing (scaling, SMOTE) with the classifier.\n",
        "# The StandardScaler ensures data is scaled BEFORE SMOTE creates synthetic samples.\n",
        "# SMOTE is only applied to the training data *within* each cross-validation fold during fitting.\n",
        "pipeline = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),        # Step 1: Standardize features\n",
        "    ('smote', SMOTE(random_state=42)),   # Step 2: Handle imbalance via oversampling\n",
        "    ('classifier', classifier)           # Step 3: The machine learning model\n",
        "])\n",
        "\n",
        "print(\"ML Pipeline created:\")\n",
        "print(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_iYTqKwC6J",
        "outputId": "3c7567d3-81d5-41bc-f8e1-e9c23c57cbb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Hyperparameter Tuning (GridSearchCV with GroupKFold) ---\n",
            "Starting Grid Search with 3 folds...\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Hyperparameter Tuning\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning (GridSearchCV with GroupKFold) ---\")\n",
        "\n",
        "# Define parameter grid for the Random Forest Classifier\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200], # Number of trees in the forest\n",
        "    'classifier__max_depth': [10, 20, None], # Max depth of trees (None means unlimited)\n",
        "    'classifier__min_samples_split': [2, 5], # Min number of samples required to split an internal node\n",
        "    'smote__sampling_strategy': ['auto', 0.5, 0.75] # SMOTE's sampling strategy\n",
        "}\n",
        "\n",
        "# For XGBoost, an example param_grid might look like:\n",
        "# param_grid = {\n",
        "#     'classifier__n_estimators': [100, 200],\n",
        "#     'classifier__max_depth': [3, 5],\n",
        "#     'classifier__learning_rate': [0.05, 0.1],\n",
        "#     'smote__sampling_strategy': ['auto', 0.5, 0.75]\n",
        "# }\n",
        "\n",
        "\n",
        "# Use GroupKFold for cross-validation to ensure patient data stays together\n",
        "# We use patient_ids_train to define groups within the training set.\n",
        "cv = GroupKFold(n_splits=3) # Use 3 or 5 splits for quicker initial tuning\n",
        "\n",
        "# Perform Grid Search\n",
        "# 'scoring' is crucial for imbalanced datasets; 'f1' or 'roc_auc' are good choices for binary classification.\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='f1', # Optimize for F1-score, which balances precision and recall\n",
        "    n_jobs=-1,    # Use all available cores\n",
        "    verbose=2,    # Show progress\n",
        "    error_score='raise' # Raise an error if any parameter combination fails\n",
        ")\n",
        "\n",
        "print(f\"Starting Grid Search with {cv.n_splits} folds...\")\n",
        "grid_search.fit(X_train, y_train, groups=patient_ids_train) # Pass groups for GroupKFold\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\n--- Grid Search Results ---\")\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation F1-score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uEJwMduwEBU"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Model Evaluation\n",
        "\n",
        "print(\"\\n--- Model Evaluation on Test Set ---\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1] # Probability of the positive class (apnea)\n",
        "\n",
        "# Print classification metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cm_display = ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "print(\"\\nROC Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "roc_display = RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve (often more informative for imbalanced data)\n",
        "print(\"\\nPrecision-Recall Curve:\")\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "pr_display = PrecisionRecallDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nagWnswI0_"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Model Saving (Optional)\n",
        "\n",
        "model_filename = 'apnea_detection_model.pkl'\n",
        "try:\n",
        "    joblib.dump(best_model, model_filename)\n",
        "    print(f\"\\nModel saved successfully as '{model_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Example of how to load the model later:\n",
        "# loaded_model = joblib.load(model_filename)\n",
        "# print(f\"Model loaded successfully from '{model_filename}'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
